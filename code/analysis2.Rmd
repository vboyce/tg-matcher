---
title: "tg-matcher 2: Preliminary analysis"
output:
  html_document:
    toc: true
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = F, message = F)
knitr::opts_chunk$set(dev = "png", dev.args = list(type = "cairo-png"))
options(knitr.table.format = "html")
knitr::opts_chunk$set(echo = F)
library(tidyverse)
library(viridis)
library(Replicate)
library(metafor)
library(esc)
library(here)
library(brms)
library(rstan)
library(googledrive)
library(tidybayes)
library(ggstance)
library("lattice")
library(reshape2)
library(ggrepel)
library(ggthemes)
library(knitr)
library(cowplot)
library(ggtext)
library(ggimage)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

theme_set(theme_bw())

dat_loc <- "data/expt2_full_data.csv"
model_location <- "code/models"

images <- "experiments/expt1/assets/images"
```

# Experiment

[try the experiment](vboyce.github.io/tg-matcher)

# Boring stuff
## Read in data

```{r}
raw <- read_csv(here(dat_loc)) |>
  select(-proliferate.condition) |>
  filter(!is.na(response))

free_response <- raw |>
  filter(is.na(correct_tangram)) |>
  select(workerid, stimulus, response, rt)

good_stuff <- raw |>
  filter(!is.na(correct_tangram)) |>
  select(
    workerid, button_rt, condition, correct, correct_tangram,
    gameId, selected, text, trial_index
  ) |>
  mutate(workerid = as.factor(workerid)) |>
  mutate(rt_sec = button_rt / 1000) |>
  separate(condition, c("group_size", "thickness", "round")) |>
  mutate(
    condition = str_c(group_size, "_", thickness),
    group_size = str_c(group_size, "_player"),
    round = str_c("round_", round),
    correct = as.numeric(correct)
  ) |>
  group_by(workerid) |>
  mutate(trial_order = row_number()) |>
  ungroup()
```


```{r}
good_stuff  |> unique()
```

##  Bonus
```{r, eval=F}
worker <- read_csv(here("data/tg_matcher_expt_2-workerids.csv")) |> mutate(workerid = as.factor(workerid))

bonuses <- good_stuff |>
  group_by(workerid) |>
  summarize(bonus = round(sum(correct) * .05, 2)) |>
  left_join(worker) |>
  select(prolific_participant_id, bonus) |>
  write_csv(here("bonus.csv"))

cost <- bonuses |>
  mutate(cost = bonus * 4 / 3) |>
  summarize(s = sum(cost))
```

## Timing

This is clock time over the whole experiment (paying attention or not)

```{r}
total_rt <- raw |>
  group_by(workerid) |>
  summarize(m = max(time_elapsed) / 1000 / 60)
# want to know about outliers... since some participants might have paused at some point

ggplot(total_rt, aes(x = m)) +
  geom_histogram() +
  labs(x = "Total elapsed time in minutes")
```

How long are individual trials taking? 
```{r}
# good_stuff |> group_by(workerid) |> mutate(winsor_time=ifelse(rt_sec>60, 60, rt_sec)) |> summarize(total_trial = sum(rt_sec)/60) |>
# ggplot(aes(x=total_trial))+geom_histogram()
good_stuff |>
  ggplot(aes(x = trial_order, y = rt_sec)) +
  labs(y = "Time in seconds", x = "Trial") +
  geom_point(alpha = .1) +
  facet_grid(condition ~ round)
```

if we exclude the > than 1 minute ones (as plausible got distracted doing other things), mean rts: 


```{r}
good_stuff |>
  group_by(workerid) |>
  filter(rt_sec < 60) |>
  summarize(mean_time = mean(rt_sec)) |>
  ggplot(aes(x = mean_time)) +
  geom_histogram() +
  labs(x = "Mean per-participant response time in seconds")
```

So, 10ish seconds per trial generally. 

# Accuracy

```{r}
acc_by_participant <- good_stuff |>
  group_by(workerid) |>
  summarize(acc = sum(correct) / n())



acc_by_type <- good_stuff |>
  group_by(condition, group_size, thickness, round) |>
  summarize(
    acc = sum(correct) / n(),
    lower = Hmisc::smean.cl.boot(correct)[2],
    higher = Hmisc::smean.cl.boot(correct)[3]
  ) |>
  arrange(acc)

acc_by_type_part <- good_stuff |>
  group_by(workerid, group_size, condition, thickness, round) |>
  summarize(acc = sum(correct) / n())

acc_by_target <- good_stuff |>
  group_by(correct_tangram) |>
  summarize(acc = sum(correct) / n()) |>
  arrange(acc)

acc_by_type_target <- good_stuff |>
  group_by(correct_tangram, group_size, condition, thickness, round) |>
  summarize(acc = sum(correct) / n()) |>
  arrange(acc)
```

Average accuracy is lower than in experiment 1 with moderate person-to-person variability. 

And two people who guessed randomly sigh. 

```{r}
ggplot(acc_by_participant, aes(x = "By participant", y = acc)) +
  geom_jitter(height = 0, width = .4) +
  coord_cartesian(ylim = c(0, 1), xlim = c(.5, 1.5), expand = F) +
  geom_hline(yintercept = 1 / 12, linetype = "dashed", color = "blue") +
  stat_summary(data.fun = "mean_cl_boot", color = "blue") +
  labs(x = "", y = "Percent correct")
```

Key question is whether there are differences between the conditions. Here's mean accuracy and bootstrapped 95% CIs in each condition. 

```{r}
acc_by_type
```

And plot with per-participant small dots. 

```{r}
ggplot(acc_by_type_part, aes(x = round, y = acc, color = str_c(group_size, "_", round))) +
  geom_jitter(height = 0, width = .2, alpha = .3, color = "black") +
  coord_cartesian(ylim = c(0, 1), xlim = c(.5, 2.5), expand = F) +
  geom_hline(yintercept = 1 / 12, linetype = "dashed", color = "blue") +
  stat_summary(data.fun = "mean_cl_boot", color = "blue") +
  facet_grid(~condition) +
  theme(legend.position = "none")
```

Tangrams vary a lot in guessability. 

```{r}
library(ggtext)
correct_tangram <- c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L")
labels <- c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L") |> map(~ str_c("<img src=", here(images, str_c("tangram_", ., ".png")), " width='20'/>"))

foo <- tibble(correct_tangram, labels) |>
  left_join(acc_by_target) |>
  arrange(acc)

ggplot(acc_by_type_target, aes(x = reorder(correct_tangram, acc), y = acc, color = condition, shape = round, group = round)) +
  geom_point(position = position_dodge(width = .4)) +
  coord_cartesian(ylim = c(0, 1), xlim = c(.5, 12.5), expand = F) +
  geom_hline(yintercept = 1 / 12, linetype = "dashed") +
  scale_x_discrete(name = NULL, labels = foo$labels) +
  theme(axis.text.x = element_markdown(color = "black", size = 11))


acc_by_target
```


# Models!

predict accuracy as a function of:

 * group size x round
 * nuisance variable of trial (might get better over time)
 * size x round |  tangram
 * size x round + trial | participant

```{r}
acc_priors <- c(
  set_prior("normal(0,1)", class = "b"),
  set_prior("normal(0,1)", class = "sd"),
  set_prior("lkj(1)", class = "cor")
)


acc_mod <- brm(
  correct ~ group_size * thickness * round + trial_order +
    (group_size * thickness * round | correct_tangram) +
    (group_size * thickness * round + trial_order | workerid),
  data = good_stuff,
  family = bernoulli(),
  file = here(model_location, "acc_2"),
  prior = acc_priors,
  control = list(adapt_delta = .95)
)


summary(acc_mod)
```

So basically this confirms what we see above: the percentages aren't that different across conditions, but there is an interaction -- 2-player games end up *less* transparent than 6-player games, even if they start in (roughly) the same place and don't change that much from start to end. 

Logistic models are annoying to think about, so we can look at predictions: 

Dotted lines are 95% posterior predictions from the model (no random effects), solid lines are mean and 95% CI from bootstrapping that category. 

```{r}
do_preds <- function(model) {
  mod <- here(model_location, model) |> read_rds()
  preds <- expand_grid(
    trial_order = 1:60, group_size = c("6_player", "2_player"),
    thickness = c("thick", "thin"),
    round = c("round_1", "round_6")
  ) |>
    add_linpred_draws(mod, value = "predicted", re_formula = NA) |>
    group_by(group_size, thickness, round) |>
    summarize(
      mean = mean(predicted),
      low = quantile(predicted, .025),
      high = quantile(predicted, .975)
    )
  return(preds)
}

predicted <- do_preds("acc_2.rds") |> mutate(across(mean:high, inv_logit_scaled))
```

## check what's up with predictions via simpler models

seems a bit odd that model preds are all larger than the bootstrap, so we check out what's causing it. Seems to be that adding mixed effects leads to pulling up the lower outliers more (b/c there's further out low ones??), although maybe this is partly due to that x logit stuff 

```{r}
do_preds_tangram <- function(model) {
  mod <- here(model_location, model) |> read_rds()
  preds <- expand_grid(
    trial_order = 1:60, group_size = c("6_player", "2_player"),
    thickness = c("thick", "thin"),
    round = c("round_1", "round_6"),
    correct_tangram = c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L")
  ) |>
    add_linpred_draws(mod, value = "predicted", re_formula = ~ (group_size * thickness * round | correct_tangram)) |>
    group_by(group_size, thickness, round, trial_order, .draw) |>
    summarize(predicted = mean(predicted)) |>
    group_by(group_size, thickness, round) |>
    summarize(
      mean = mean(predicted),
      low = quantile(predicted, .025),
      high = quantile(predicted, .975)
    )
  return(preds)
}

predict_average_out_tangram <- do_preds_tangram("acc_2.rds") |> mutate(across(mean:high, inv_logit_scaled))

ggplot(acc_by_type, aes(x = round, y = acc)) +
  coord_cartesian(ylim = c(0, 1), xlim = c(.5, 2.6), expand = F) +
  geom_hline(yintercept = 1 / 12, linetype = "dashed", color = "blue") +
  geom_pointrange(aes(ymin = lower, ymax = higher, color = "data bootstrap"), linetype = "solid", position = position_nudge(x = -.1)) +
  geom_pointrange(data = predict_average_out_tangram, aes(y = mean, ymin = low, ymax = high, color = "full model avg tangram"), linetype = "dotted", position = position_nudge(x = .5)) +
  geom_pointrange(data = predicted, aes(y = mean, ymin = low, ymax = high, color = "full model", ), linetype = "dotted", position = position_nudge(x = .2)) +
  facet_grid(. ~ str_c(group_size, "_", thickness))
```

```{r}
do_preds_per_tangram <- function(model) {
  mod <- here(model_location, model) |> read_rds()
  preds <- expand_grid(
    trial_order = 1:60, group_size = c("6_player", "2_player"),
    thickness = c("thick", "thin"),
    round = c("round_1", "round_6"),
    correct_tangram = c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L")
  ) |>
    add_linpred_draws(mod, value = "predicted", re_formula = ~ (group_size * thickness * round | correct_tangram)) |>
    group_by(group_size, thickness, round, correct_tangram) |>
    summarize(
      mean = mean(predicted),
      low = quantile(predicted, .025),
      high = quantile(predicted, .975)
    )
  return(preds)
}
predict_individual_tangram <- do_preds_per_tangram("acc_2.rds") |>
  mutate(across(mean:high, inv_logit_scaled)) |>
  left_join(acc_by_target)


ggplot(acc_by_type_target, aes(x = reorder(correct_tangram, acc), y = acc, color = str_c(group_size, "_", thickness), shape = round)) +
  geom_point(position = position_nudge(x = -.2)) +
  coord_cartesian(ylim = c(0, 1), xlim = c(.5, 12.5), expand = F) +
  geom_hline(yintercept = 1 / 12, linetype = "dashed") +
  scale_x_discrete(name = NULL, labels = foo$labels) +
  theme(axis.text.x = element_markdown(color = "black", size = 11)) +
  geom_pointrange(data = predict_individual_tangram |> left_join(acc_by_target), aes(y = mean, ymin = low, ymax = high), size = .2, linetype = "dotted", position = position_nudge(x = .2)) +
  labs(color = "") +
  facet_wrap(~round) +
  theme(legend.position = "bottom")
```

# compare accuracy as function of original accuracy & length

how does RT vary based on transcript length?

```{r}
orig <- read_csv(here("expt_prep_code/combined_chat.csv")) |>
  filter(repNum == 5 | repNum == 0) |>
  filter(!is.na(text)) |>
  filter(condition %in% c("2_thin", "6_thin", "2_thick", "6_thick")) |>
  mutate(
    group_size = ifelse(str_detect(condition, "2"), "2_player", "6_player"),
    thickness = ifelse(str_detect(condition, "thick"), "thick", "thin"),
    round = ifelse(repNum == 0, "round_1", "round_6")
  ) |>
  group_by(group_size, round, thickness, condition, gameId, tangram, realCorrect, numPlayers) |>
  mutate(utt_length_words = str_count(text, "\\W+") + 1) %>%
  summarize(
    text = paste0(text, collapse = ", "),
    total_num_words = sum(utt_length_words, na.rm = T) %>% as.numeric()
  ) |>
  select(group_size, thickness, numPlayers, condition, round, gameId, correct_tangram = tangram, realCorrect, text, total_num_words)

augment <- good_stuff |>
  select(-text) |>
  left_join(orig, by = c("group_size", "round", "thickness", "condition", "gameId", "correct_tangram"))
```

```{r}
ggplot(augment, aes(x = total_num_words, y = rt_sec, color = as.factor(correct))) +
  geom_point(alpha = .1) +
  coord_trans(x = "identity", ylim = c(0, 100)) +
  geom_smooth(method = "lm") +
  facet_grid(round ~ condition)
```

how does accuracy vary based on transcript length?

```{r}
ggplot(augment, aes(x = total_num_words, y = correct)) +
  geom_point(alpha = .1) +
  geom_smooth(method = "lm") +
  facet_grid(round ~ condition)
```

Note there's some join or exclusion issue happening where we don't have round results for some of these -- I thought it might be an NA issue, but it looks like not -- looks like results v chat mismatch thing which I will deal with later. Grrr. 

What's the relationship between original listener accuracy and accuracy?

Definitely positive correlation here, which makes sense. 

```{r}
correctness <- augment |>
  filter(!is.na(realCorrect)) |>
  mutate(
    possible = numPlayers - 1,
    pct_correct = realCorrect / possible,
    all_correct = realCorrect == possible
  )

ggplot(correctness, aes(x = all_correct, y = correct)) +
  stat_summary(fun.data = "mean_cl_boot") +
  facet_grid(round ~ condition)

ggplot(correctness, aes(x = pct_correct, y = correct)) +
  stat_summary(fun.data = "mean_cl_boot") +
  geom_smooth(method = "lm") +
  facet_grid(round ~ condition)
```

just to check correlation issues, original accuracy v length. some correlation, but not huge.

```{r}
ggplot(correctness, aes(x = all_correct, y = total_num_words)) +
  stat_summary(fun.data = "mean_cl_boot") +
  facet_grid(round ~ condition)

ggplot(correctness, aes(x = pct_correct, y = total_num_words)) +
  stat_summary(fun.data = "mean_cl_boot") +
  facet_grid(round ~ condition)
```
# by tangram

```{r}
per_utt <- correctness |>
  group_by(correct_tangram, condition, gameId, pct_correct, group_size, thickness, round) |>
  summarize(match_pct = mean(correct))

per_game <- per_utt |>
  group_by(gameId, group_size, thickness, round) |>
  summarize(
    orig_pct = mean(pct_correct),
    match_pct = mean(match_pct)
  )

per_image <- per_utt |>
  group_by(correct_tangram, condition, group_size, thickness, round) |>
  summarize(
    orig_pct = mean(pct_correct),
    match_pct = mean(match_pct)
  )
```

```{r}
per_game |>
  group_by(thickness, group_size, round) |>
  ggplot(aes(x = round, y = match_pct, group = gameId)) +
  geom_point() +
  geom_smooth(method = "lm") +
  facet_grid(thickness ~ group_size)

for_plot <- per_image |>
  mutate(image = str_c("tangram_", correct_tangram, ".png")) |>
  group_by(condition, round)

for_plot |> ggplot(aes(x = round, y = match_pct, group = correct_tangram)) +
  geom_point() +
  geom_smooth(method = "lm") +
  facet_grid(~condition) +
  geom_image(data = for_plot |> filter(round == "round_1"), aes(x = .8, image = here(images, image)))
```
# confusion matrix might be real interesting!

```{r}
correct_tangram <- c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L")
labels <- c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L") |> map(~ str_c("<img src=", here(images, str_c("tangram_", ., ".png")), " width='20'/>"))

confusion <- good_stuff |>
  group_by(correct_tangram, selected) |>
  tally() |>
  group_by(correct_tangram) |>
  mutate(pct = n / sum(n))

self <- confusion |>
  filter(correct_tangram == selected) |>
  select(correct_tangram, self = pct)

corr_order <- tibble(correct_tangram, labels) |>
  left_join(self) |>
  arrange(self)

self_2 <- confusion |>
  ungroup() |>
  filter(correct_tangram == selected) |>
  select(selected, self_2 = pct)


confusion |>
  left_join(self) |>
  left_join(self_2) |>
  mutate(correct_tangram = str_c("<img src=", here(images, "tangram_", correct_tangram, ".png"), "width='100' />")) |>
  ggplot(aes(x = reorder(correct_tangram, self, FUN = mean), y = reorder(selected, self_2, FUN = mean), fill = pct)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "blue") +
  scale_x_discrete(name = "Correct", labels = corr_order$labels) +
  scale_y_discrete(name = "Selected", labels = corr_order$labels) +
  theme(axis.text = element_markdown(color = "black", size = 11))
```

#Joint with expt 1

```{r}
dat_loc_1 <- "data/expt1_full_data.csv"

rotate <- read_csv(here(dat_loc_1)) |>
  select(-proliferate.condition) |>
  filter(!is.na(response)) |>
  filter(!is.na(correct_tangram)) |>
  select(
    workerid, button_rt, condition, correct, correct_tangram,
    gameId, selected, text, trial_index
  ) |>
  mutate(workerid = as.factor(workerid)) |>
  mutate(rt_sec = button_rt / 1000) |>
  separate(condition, c("group_size", "thickness", "round")) |>
  mutate(
    condition = str_c(group_size, "_", thickness),
    group_size = str_c(group_size, "_player"),
    round = str_c("round_", round),
    correct = as.numeric(correct)
  ) |>
  group_by(workerid) |>
  mutate(trial_order = row_number()) |>
  ungroup()


together <- good_stuff |>
  rbind(rotate) |>
  mutate(thickness = factor(thickness, levels = c("thin", "rotate", "thick")))
```

```{r}
together |>
  group_by(workerid, group_size, thickness, round) |>
  summarize(correct = mean(correct)) |>
  ggplot(aes(x = round, y = correct, color = thickness)) +
  stat_summary(fun.data = "mean_cl_boot") +
  stat_summary(aes(group = thickness), fun.data = "mean_cl_boot", geom = "line") +
  facet_wrap(~group_size) +
  coord_cartesian(ylim = c(0, 1))
```

```{r}
together |>
  group_by(correct_tangram, group_size, thickness, round) |>
  summarize(acc = mean(correct)) |>
  pivot_wider(names_from = round, values_from = acc) |>
  mutate(diff = round_6 - round_1) |>
  mutate(image = str_c("tangram_", correct_tangram, ".png")) |>
  ggplot(aes(x = group_size, y = diff)) +
  geom_image(aes(image = here(images, image)), position = position_dodge(width = .3)) +
  geom_hline(yintercept = 0, linetype = "dotted") +
  facet_wrap(~thickness) +
  labs("Increase in naive legibility from round 1 to 6")
```
```{r}
together |> write_csv(here("human_data.csv"))
```
# Versus mpt data

```{r}
tg_matcher <- read_csv(here("human_data.csv"))


# based on https://github.com/vboyce/multiparty-tangrams/blob/main/code/prep_ms.R

url <- "https://raw.githubusercontent.com/vboyce/multiparty-tangrams/main/"

one_round_results <- read_rds(str_c(url, "data/study1/round_results.rds")) %>% mutate(rotate = str_c(as.character(numPlayers), "_rotate"))
two_a_round_results <- read_rds(str_c(url, "data/study2a/round_results.rds")) %>% mutate(rotate = "no_rotate")
two_b_round_results <- read_rds(str_c(url, "data/study2b/round_results.rds")) %>% mutate(rotate = "full_feedback")
two_c_round_results <- read_rds(str_c(url, "data/study2c/round_results.rds")) |> mutate(rotate = "emoji")
three_round_results <- read_rds(str_c(url, "data/study3/round_results.rds")) |> rename(`_id` = "X_id", condition = name)

combined_results <- one_round_results |>
  rbind(two_a_round_results) |>
  rbind(two_b_round_results) |>
  rbind(two_c_round_results) |>
  mutate(activePlayerCount = NA) |>
  rename(condition = rotate) |>
  rbind(three_round_results) |>
  select(realCorrect, gameId, targetNum, repNum, trialNum, condition, numPlayers, activePlayerCount, tangram) |>
  unique() |>
  filter(repNum %in% c(0, 5)) |>
  mutate(round = str_c("round_", repNum + 1)) |> 
  mutate(activePlayerCount=ifelse(is.na(activePlayerCount), numPlayers, activePlayerCount)) |> 
  mutate(orig_correct=realCorrect/(activePlayerCount-1)) |> 
  select(orig_correct, tangram, condition, round, gameId)

compare <- tg_matcher |>
  rename(tangram = correct_tangram, new_correct=correct) |>
  left_join(combined_results) |>
  select(group_size, thickness, round, orig_correct, new_correct, tangram) |> 
  filter(!is.na(orig_correct))
```

going to have sparsity issues, so we may need a model eventually

but want to know where the biggest differences in accuracy are -- type / round / tangram

```{r}
summ <- compare |> group_by(group_size,thickness, round, tangram) |> summarize(new_correct=mean(new_correct), orig_correct=mean(orig_correct)) |> mutate(diff=orig_correct-new_correct) |> 
    mutate(image = str_c("tangram_", tangram, ".png"))


ggplot(summ, aes(x=str_c(group_size,"\n",thickness), y=diff, color=round, group=round))+geom_point(position=position_dodge(width=.2))+geom_hline(aes(yintercept=0))+stat_summary(fun.data="mean_cl_boot", color="black", position=position_dodge(width=.2))

```
```{r}
summ |> ggplot(aes(x=orig_correct, y=new_correct, color=round))+geom_point()+facet_grid(group_size~thickness)+coord_equal()+geom_abline()
```