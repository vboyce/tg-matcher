---
title: "tg-matcher 1: Preliminary analysis"
output:
  html_document:
    toc: true
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = F, message = F)
knitr::opts_chunk$set(dev = "png", dev.args = list(type = "cairo-png"))
options(knitr.table.format = "html")
knitr::opts_chunk$set(echo = F)
library(tidyverse)
library(viridis)
library(Replicate)
library(metafor)
library(esc)
library(here)
library(brms)
library(rstan)
library(googledrive)
library(glmnet)
library(tidybayes)
library(ggstance)
library("lattice")
library(reshape2)
library(ggrepel)
library(ggthemes)
library(knitr)
library(cowplot)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

theme_set(theme_bw())

dat_loc <- "data/expt1_full_data.csv"
model_location <- "code/models"

images <- "experiments/expt1/assets/images"
```

# Experiment

[try the experiment](vboyce.github.io/tg-matcher)

# Boring stuff
## Read in data

```{r}
raw <- read_csv(here(dat_loc)) |>
  select(-proliferate.condition) |>
  filter(!is.na(response))

free_response <- raw |>
  filter(is.na(correct_tangram)) |>
  select(workerid, stimulus, response, rt)

good_stuff <- raw |>
  filter(!is.na(correct_tangram)) |>
  select(
    workerid, button_rt, condition, correct, correct_tangram,
    gameId, selected, text, trial_index
  ) |>
  mutate(workerid = as.factor(workerid)) |>
  mutate(rt_sec = button_rt / 1000) |>
  separate(condition, c("group_size", NA, "round")) |>
  mutate(
    group_size = str_c(group_size, "_player"),
    round = str_c("round_", round),
    correct = as.numeric(correct)
  ) |>
  group_by(workerid) |>
  mutate(trial_order = row_number()) |>
  ungroup()
```

##  Bonus
```{r, eval=F}
worker <- read_csv(here("data/tg_matcher_expt_1_full_data-workerids.csv")) |> mutate(workerid = as.factor(workerid))

bonuses <- good_stuff |>
  group_by(workerid) |>
  summarize(bonus = round(sum(correct) * .05, 2)) |>
  left_join(worker) |>
  select(prolific_participant_id, bonus) |>
  write_csv(here("bonus.csv"))

cost <- bonuses |>
  mutate(cost = bonus * 4 / 3) |>
  summarize(s = sum(cost))
```

## Timing

This is clock time over the whole experiment (paying attention or not)

```{r}
total_rt <- raw |>
  group_by(workerid) |>
  summarize(m = max(time_elapsed) / 1000 / 60)
# want to know about outliers... since some participants might have paused at some point

ggplot(total_rt, aes(x = m)) +
  geom_histogram() +
  labs(x = "Total elapsed time in minutes")
```

How long are individual trials taking? 
```{r}
# good_stuff |> group_by(workerid) |> mutate(winsor_time=ifelse(rt_sec>60, 60, rt_sec)) |> summarize(total_trial = sum(rt_sec)/60) |>
# ggplot(aes(x=total_trial))+geom_histogram()
good_stuff |>
  ggplot(aes(x = trial_order, y = rt_sec)) +
  labs(y = "Time in seconds", x = "Trial") +
  geom_point(alpha = .1) +
  facet_grid(group_size ~ round)
```

if we exclude the > than 1 minute ones (as plausible got distracted doing other things), mean rts: 


```{r}
good_stuff |>
  group_by(workerid) |>
  filter(rt_sec < 60) |>
  summarize(mean_time = mean(rt_sec)) |>
  ggplot(aes(x = mean_time)) +
  geom_histogram() +
  labs(x = "Mean per-participant response time in seconds")
```

So, 10-20 seconds per trial generally. 

# Accuracy

```{r}
acc_by_participant <- good_stuff |>
  group_by(workerid) |>
  summarize(acc = sum(correct) / n())



acc_by_type <- good_stuff |>
  group_by(group_size, round) |>
  summarize(
    acc = sum(correct) / n(),
    lower = Hmisc::smean.cl.boot(correct)[2],
    higher = Hmisc::smean.cl.boot(correct)[3]
  ) |>
  arrange(acc)

acc_by_type_part <- good_stuff |>
  group_by(workerid, group_size, round) |>
  summarize(acc = sum(correct) / n())

acc_by_target <- good_stuff |>
  group_by(correct_tangram) |>
  summarize(acc = sum(correct) / n()) |>
  arrange(acc)

acc_by_type_target <- good_stuff |>
  group_by(correct_tangram, group_size, round) |>
  summarize(acc = sum(correct) / n()) |>
  arrange(acc)
```

Average accuracy is around 62%, with moderate person-to-person variability. 

```{r}
ggplot(acc_by_participant, aes(x = "By participant", y = acc)) +
  geom_jitter(height = 0, width = .4) +
  coord_cartesian(ylim = c(0, 1), xlim = c(.5, 1.5), expand = F) +
  geom_hline(yintercept = 1 / 12, linetype = "dashed", color = "blue") +
  stat_summary(data.fun = "mean_cl_boot", color = "blue") +
  labs(x = "", y = "Percent correct")
```

Key question is whether there are differences between the conditions: not huge ones. Here's mean accuracy and bootstrapped 95% CIs in each condition. There's a 10 percentage point difference between 2-player round 6 and 6-player round 6 (round 1's in the middle). 

```{r}
acc_by_type
```

And plot with per-participant small dots. 

```{r}
ggplot(acc_by_type_part, aes(x = round, y = acc, color = str_c(group_size, "_", round))) +
  geom_jitter(height = 0, width = .2, alpha = .5, color = "black") +
  coord_cartesian(ylim = c(0, 1), xlim = c(.5, 2.5), expand = F) +
  geom_hline(yintercept = 1 / 12, linetype = "dashed", color = "blue") +
  stat_summary(data.fun = "mean_cl_boot") +
  facet_wrap(~group_size) +
  theme(legend.position = "none")
```

Tangrams vary a lot in guessability. 

```{r}
library(ggtext)
correct_tangram <- c("A","B","C", "D", "E", "F", "G", "H", "I", "J", "K", "L") 
labels <- c("A","B","C", "D", "E", "F", "G", "H", "I", "J", "K", "L") |> map(~str_c("<img src=",here(images,str_c("tangram_",.,".png"))," width='20'/>"))

foo <- tibble(correct_tangram, labels) |> left_join(acc_by_target) |> arrange(acc)

ggplot(acc_by_type_target, aes(x = reorder(correct_tangram, acc), y = acc, color = str_c(group_size, "_", round))) +
  geom_point() +
  coord_cartesian(ylim = c(0, 1), xlim = c(.5, 12.5), expand = F) +
  geom_hline(yintercept = 1 / 12, linetype = "dashed") +
  scale_x_discrete(name=NULL, labels=foo$labels)+
    theme(axis.text.x = element_markdown(color = "black", size = 11))


acc_by_target
```


# Models!

predict accuracy as a function of:

 * group size x round
 * nuisance variable of trial (might get better over time)
 * size x round |  tangram
 * size x round + trial | participant

```{r}
acc_priors <- c(
  set_prior("normal(0,1)", class = "b"),
  set_prior("normal(0,1)", class = "sd"),
  set_prior("lkj(1)", class = "cor")
)


acc_mod <- brm(
  correct ~ group_size * round + trial_order +
    (group_size * round | correct_tangram) +
    (group_size * round + trial_order | workerid),
  data = good_stuff,
  family = bernoulli(),
  file = here(model_location, "acc_1"),
  prior = acc_priors,
  control = list(adapt_delta = .95)
)

summary(acc_mod)
```

So basically this confirms what we see above: the percentages aren't that different across conditions, but there is an interaction -- 2-player games end up *less* transparent than 6-player games, even if they start in (roughly) the same place and don't change that much from start to end. 

Logistic models are annoying to think about, so we can look at predictions: 

Dotted lines are 95% posterior predictions from the model (no random effects), solid lines are mean and 95% CI from bootstrapping that category. 

```{r}
do_preds <- function(model) {
  mod <- here(model_location, model) |> read_rds()
  preds <- expand_grid(
    trial_order = 1:60, group_size = c("6_player", "2_player"),
    round = c("round_1", "round_6")
  ) |>
    add_linpred_draws(mod, value = "predicted", re_formula = NA) |>
    group_by(group_size, round) |>
    summarize(
      mean = mean(predicted),
      low = quantile(predicted, .025),
      high = quantile(predicted, .975)
    )
  return(preds)
}

predicted <- do_preds("acc_1.rds") |> mutate(across(mean:high, inv_logit_scaled))


```

## check what's up with predictions via simpler models

seems a bit odd that model preds are all larger than the bootstrap, so we check out what's causing it. Seems to be that adding mixed effects leads to pulling up the lower outliers more (b/c there's further out low ones??), although maybe this is partly due to that x logit stuff 

```{r}
acc_mod_2 <- brm(
  correct ~ 1,
  data = good_stuff,
  family = bernoulli(),
  file = here(model_location, "acc_1_test_1"),
  control = list(adapt_delta = .95)
)
acc_priors_2 <- c(
  set_prior("normal(0,1)", class = "b")
)

acc_mod <- brm(
  correct ~ group_size * round + trial_order,
  data = good_stuff,
  family = bernoulli(),
  file = here(model_location, "acc_1_test_2"),
  prior = acc_priors_2,
  control = list(adapt_delta = .95)
)

acc_priors <- c(
  set_prior("normal(0,1)", class = "b"),
  set_prior("normal(0,1)", class = "sd"),
  set_prior("lkj(1)", class = "cor")
)
acc_mod <- brm(
  correct ~ group_size * round + trial_order +
    (group_size * round | correct_tangram),
  data = good_stuff,
  family = bernoulli(),
  file = here(model_location, "acc_1_test_3"),
  prior = acc_priors,
  control = list(adapt_delta = .95)
)

acc_mod <- brm(
  correct ~ group_size * round + trial_order +
    (group_size * round + trial_order | workerid),
  data = good_stuff,
  family = bernoulli(),
  file = here(model_location, "acc_1_test_4"),
  prior = acc_priors,
  control = list(adapt_delta = .95)
)

predicted_person_level <- do_preds("acc_1_test_4.rds") |> mutate(across(mean:high, inv_logit_scaled))

predicted_tangram_level <- do_preds("acc_1_test_3.rds") |> mutate(across(mean:high, inv_logit_scaled))

predicted_no_me <- do_preds("acc_1_test_2.rds") |> mutate(across(mean:high, inv_logit_scaled))

do_preds_tangram <- function(model) {
  mod <- here(model_location, model) |> read_rds()
  preds <- expand_grid(
    trial_order = 1:60, group_size = c("6_player", "2_player"),
    round = c("round_1", "round_6"),
    correct_tangram = c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L")
  ) |>
    add_linpred_draws(mod, value = "predicted", re_formula = ~ (group_size * round | correct_tangram)) |>
    group_by(group_size, round, trial_order, .draw) |>
    summarize(predicted = mean(predicted)) |>
    group_by(group_size, round) |>
    summarize(
      mean = mean(predicted),
      low = quantile(predicted, .025),
      high = quantile(predicted, .975)
    )
  return(preds)
}

predict_average_out_tangram <- do_preds_tangram("acc_1.rds") |> mutate(across(mean:high, inv_logit_scaled))

ggplot(acc_by_type, aes(x = round, y = acc)) +
  coord_cartesian(ylim = c(0, 1), xlim = c(.5, 2.6), expand = F) +
  geom_hline(yintercept = 1 / 12, linetype = "dashed", color = "blue") +
  geom_pointrange(aes(ymin = lower, ymax = higher, color = "data bootstrap"), linetype = "solid", position = position_nudge(x = -.1)) +
  geom_pointrange(data = predicted_no_me, aes(y = mean, ymin = low, ymax = high, color = "fixed only"), linetype = "dotted", position = position_nudge(x = .1)) +
  geom_pointrange(data = predicted_person_level, aes(y = mean, ymin = low, ymax = high, color = "person-level ranef"), linetype = "dotted", position = position_nudge(x = .2)) +
  geom_pointrange(data = predicted_tangram_level, aes(y = mean, ymin = low, ymax = high, color = "tangram-level ranef"), linetype = "dotted", position = position_nudge(x = .3)) +
  geom_pointrange(data = predict_average_out_tangram, aes(y = mean, ymin = low, ymax = high, color = "full model avg tangram"), linetype = "dotted", position = position_nudge(x = .5)) +
  geom_pointrange(data = predicted, aes(y = mean, ymin = low, ymax = high, color = "full model", ), linetype = "dotted", position = position_nudge(x = .4)) +
  facet_wrap(~group_size)
```

```{r}
do_preds_per_tangram <- function(model) {
  mod <- here(model_location, model) |> read_rds()
  preds <- expand_grid(
    trial_order = 1:60, group_size = c("6_player", "2_player"),
    round = c("round_1", "round_6"),
    correct_tangram = c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L")
  ) |>
    add_linpred_draws(mod, value = "predicted", re_formula = ~ (group_size * round | correct_tangram)) |>
    group_by(group_size, round, correct_tangram) |>
    summarize(
      mean = mean(predicted),
      low = quantile(predicted, .025),
      high = quantile(predicted, .975)
    )
  return(preds)
}
predict_individual_tangram <- do_preds_per_tangram("acc_1.rds") |> mutate(across(mean:high, inv_logit_scaled)) |> left_join(acc_by_target)


ggplot(acc_by_type_target, aes(x = reorder(correct_tangram, acc), y = acc, color = str_c(group_size, "_", round))) +
  geom_point(position=position_nudge(x=-.2)) +
  coord_cartesian(ylim = c(0, 1), xlim = c(.5, 12.5), expand = F) +
  geom_hline(yintercept = 1 / 12, linetype = "dashed") +
  scale_x_discrete(name=NULL, labels=foo$labels)+
    theme(axis.text.x = element_markdown(color = "black", size = 11))+
  geom_pointrange(data = predict_individual_tangram |> left_join(acc_by_target), aes(y = mean, ymin = low, ymax = high), size=.2, linetype = "dotted",position = position_nudge(x=.2))+
  labs(color="")+
  theme(legend.position = "bottom")
  

```

# compare accuracy as function of original accuracy & length

how does RT vary based on transcript length?

```{r}
orig <- read_csv(here("expt_prep_code/combined_chat.csv")) |>
  filter(repNum == 5 | repNum == 0) |>
  filter(!is.na(text)) |>
  filter(condition %in% c("2_rotate", "6_rotate")) |>
  mutate(
    group_size = ifelse(condition == "2_rotate", "2_player", "6_player"),
    round = ifelse(repNum == 0, "round_1", "round_6")
  ) |>
  group_by(group_size, round, gameId, tangram, realCorrect) |>
  mutate(utt_length_words = str_count(text, "\\W+") + 1) %>%
  summarize(
    text = paste0(text, collapse = ", "),
    total_num_words = sum(utt_length_words, na.rm = T) %>% as.numeric()
  ) |>
  select(group_size, round, gameId, correct_tangram = tangram, realCorrect, text, total_num_words)

augment <- good_stuff |>
  select(-text) |>
  left_join(orig, by = c("group_size", "round", "gameId", "correct_tangram"))
```

```{r}
ggplot(augment, aes(x = total_num_words, y = rt_sec, color = as.factor(correct))) +
  geom_point(alpha = .1) +
  coord_trans(x = "log", ylim = c(0, 100)) +
  geom_smooth(method = "lm") +
  facet_grid(round ~ group_size)
```

how does accuracy vary based on transcript length?

```{r}
ggplot(augment, aes(x = total_num_words, y = correct)) +
  geom_point(alpha = .1) +
  coord_trans(x = "log") +
  geom_smooth(method = "lm") +
  facet_grid(round ~ group_size)
```

Note there's some join or exclusion issue happening where we don't have round results for some of these -- I thought it might be an NA issue, but it looks like not -- looks like results v chat mismatch thing which I will deal with later. Grrr. 

What's the relationship between original listener accuracy and accuracy?

Definitely positive correlation here, which makes sense. 

```{r}
correctness <- augment |>
  filter(!is.na(realCorrect)) |>
  mutate(
    possible = ifelse(group_size == "2_player", 1, 5),
    pct_correct = realCorrect / possible,
    all_correct = realCorrect == possible
  )

ggplot(correctness, aes(x = all_correct, y = correct)) +
  stat_summary(fun.data = "mean_cl_boot") +
  facet_grid(round ~ group_size)

ggplot(correctness, aes(x = pct_correct, y = correct)) +
  stat_summary(fun.data = "mean_cl_boot") +
  facet_grid(round ~ group_size)
```

just to check correlation issues, original accuracy v length. some correlation, but not huge.

```{r}
ggplot(correctness, aes(x = all_correct, y = total_num_words)) +
  stat_summary(fun.data = "mean_cl_boot") +
  facet_grid(round ~ group_size)

ggplot(correctness, aes(x = pct_correct, y = total_num_words)) +
  stat_summary(fun.data = "mean_cl_boot") +
  facet_grid(round ~ group_size)
```
# by tangram

```{r}
per_utt <- correctness |> group_by(correct_tangram, gameId, pct_correct, group_size, round) |> 
  summarize(match_pct=mean(correct))

per_game <- per_utt |> group_by(gameId, group_size, round) |> 
  summarize(orig_pct=mean(pct_correct),
            match_pct=mean(match_pct))

per_image <- per_utt |> group_by(correct_tangram, group_size, round) |> 
  summarize(orig_pct=mean(pct_correct),
            match_pct=mean(match_pct))

```

```{r}
library(ggimage)
per_game |> group_by(group_size, round) |> ggplot(aes(x=round, y=match_pct, group=gameId))+geom_point()+geom_smooth(method="lm")+
  facet_grid(~ group_size)

per_image |> mutate(image=str_c("tangram_",correct_tangram,".png")) |> group_by(group_size, round) |> ggplot(aes(x=round, y=match_pct, group=correct_tangram))+geom_point()+geom_smooth(method="lm")+
  facet_grid(~ group_size)+ geom_image(aes(image=here(images,image)))
```
# confusion matrix might be real interesting!

```{r}
library(ggtext)
correct_tangram <- c("A","B","C", "D", "E", "F", "G", "H", "I", "J", "K", "L") 
labels <- c("A","B","C", "D", "E", "F", "G", "H", "I", "J", "K", "L") |> map(~str_c("<img src=",here(images,str_c("tangram_",.,".png"))," width='20'/>"))

confusion <- good_stuff |>
  group_by(correct_tangram, selected) |>
  tally() |>
  group_by(correct_tangram) |>
  mutate(pct = n / sum(n)) 

self <- confusion |> filter(correct_tangram==selected) |> select(correct_tangram, self=pct)

corr_order <- tibble(correct_tangram, labels) |> left_join(self) |> arrange(self)

self_2 <- confusion |>ungroup() |>  filter(correct_tangram==selected) |> select(selected, self_2=pct)


confusion |> left_join(self) |> left_join(self_2) |> mutate(correct_tangram=str_c("<img src=",here(images,"tangram_",correct_tangram,".png"),"width='100' />")) |> 
  ggplot(aes(x=reorder(correct_tangram, self, FUN=mean), y=reorder(selected, self_2, FUN=mean), fill=pct))+geom_tile()+scale_fill_gradient(low="white", high="blue")+
  scale_x_discrete(name="Correct", labels = corr_order$labels) +
    scale_y_discrete(name="Selected", labels = corr_order$labels) +
  
  theme(axis.text= element_markdown(color = "black", size = 11))

```


# Thinking about what the heck the "reduction process" theory space is

So, given these 4 data points we have some things to explain

* what does it mean that accuracy (overall) here is what it is
* what does it mean that there isn't (much) condition difference
* what does it mean that 2-player is harder later but 6-player is easier later

what ""theories"" or proto-theories do we have to work with:

* Robert's CHAI -- predictions unclear since it depends a lot on priors (if priors start out random, then naive should be uniformly hopeless, otherwise earlier should be easier ?), also doesn't account for practice effects / better sampling of utterance components over time
* claim that utterances get more opaque over time 
* larger games stick closer to priors / have less game-to-game variability (a la Guilbeault paper)
* vague notion that group size (in this range) should yield continuous behavior (same trajectory, different speed parameters)
* auxiliary theory of task demands!
* ??? 

""theories"" also need to be consistent with our other sources of evidence:

* reduction trajectories
* semantic trajectories
* item-level stuff

## Overall accuracy

We didn't really have priors on this. So, these (first round) accuracies are substantially lower than in-game listener (first round) accuracies -- might expect some satisficing / some its easier to have a conversation than to read it / and also some this isn't including the right details. 

The big question might be how we see really big image to image differences, so to some extent it's not that accuracy is like 62% it's that it's a certain mix of 35-85%. 

Theory implications: ???

## Why is initial accuracy the same-ish for 2 and 6? 

People don't start off doing anything different for group size, and maybe people on this task don't read that far? / what questions people have are sufficiently random that the clarifications aren't that differential?

Here we have the issue that in real games, initial accuracy for 2p is marginally? greater than 6p, so there may or may not be something to explain here...

How far people actually read / time might also be part of it 


## Why aren't there more differences between conditions? 

* the descriptions aren't actually that different (how do we quantify different-ness?) -- but this is contra some of the similarity trajectories we see where 2p converges much faster
* it is kinda strange that this is more squished than in game listener differences -- so is having it be interactive make a big difference? (regardless of in-order-ness) but is it actually more squished -- 2 v 6 in the first experiment isn't a big different, but that might mostly be partial pooling of the model? 
* is this a product of some mixture of some being quite identifiable and others being a guess between 2 or 3 or ... 
* THIS IS NOT SOMETHING I HAVE A GOOD ANSWER TO!

## Why different trajectories?

so, descriptions vary and we can think of "goodness to partner" and "goodness to naive" as having a shared component "overall goodness" and then a "path dependence" component (or something)

and we might imagine that both of these increase over time generally

things that don't narrow down the set of tangrams (diamond head) isn't useful at all

maybe should factor out a "nichiness" part <-- how broad is the prior knowledge -> like is this based on a general "what bunnies look like" or a pop-culture thing or a our-personal-conversation-history

seems like there might also be a : how commonly does this description come up v how well it fits the image (/differentially fits the image). Like "cobra" might not be commonly used but may be high fit? 

So one question is: is everything the same path, but with different time components ... or are there different paths in some way? Issue is that we think path speed is a group x tangram thing

so there's a few things that contribute to *difficulty* like tangram & maybe group dynamic (difficulty might be externally measureable with kilogram or image distance from others or something)

is there a way to info theory this? 

what's a non-ad-hoc way...

could deep dive on a couple games to see if within a game / image we see u shape 

but what do we think the parameters are...


# Fun with fake data!

there's like lots of functional forms that one could fit

but what do we think the process - plausible ones are 

on a per-trajectory basis: 
* could asymtote up as it is better / more clearly described
* if in-group clarity > something, then can reduce which may or may not reduce clarity depending on in-groupiness?

no idea what those do with description length, going to have to build some toy process model aren't we? 

```{r}

two <- tibble(num=2,block=1:6, acc=seq(62,56.6, length.out=6))
three <- tibble(num=3,block=1:6, acc=seq(62.4,59.2, length.out=6))
four <- tibble(num=4,block=1:6, acc=seq(62.8,61.6, length.out=6))
five <- tibble(num=5,block=1:6, acc=seq(63,64.1, length.out=6))

six <- tibble(num=6,block=1:6, acc=seq(63.4, 66.6, length.out=6))

all <- two |> rbind(three,four,five,six)


ggplot(all, aes(x=block, y=acc, color=as.factor(num)))+geom_point()+geom_line()
```
```{r}

x <-c(1,3,6)
y <- c(62.8, 66, 61.6)

#coef(lm(y ~ poly(x, 2, raw = TRUE)))

blah <- function(y){
  x=y
  return(59.36+4.*x-.61*x^2)}

#blah(1:6)

two <- tibble(num=2,block=1:6, acc=c(62,64.9, 65.9, 64.9, 61.9, 56.6))
three <- tibble(num=3,block=1:6, acc=c(62.4, 64.9, 65.8, 65.2, 62.9, 59.2))
four <- tibble(num=4,block=1:6, acc=c(62.8,64.92, 65.87, 65.6, 64.11, 61.6))
five <- tibble(num=5,block=1:6, acc=c(63,65.12, 66.3, 66.5, 65.75, 64.2))

six <- tibble(num=6,block=1:6, acc=c(63.4,64.7, 65.7, 66.3, 66.5, 66.3))

all <- two |> rbind(three,four,five,six)


ggplot(all, aes(x=block, y=acc, color=as.factor(num)))+geom_point()+geom_line()
```



# Next steps:

* could collect more data on the same thing (not V's favorite idea)
* could pull things where in-game patterns were more different (2 thick v 6 thin for example)
* could pull intermediates (middling game size & middling rounds)
* could deep dive in a couple games to see if there's any u shape / non-linearity
* could go straight to some incremental viewing to get at how much people are reading? get some sort of RT separate from selection time ??



# Feedback reading time



```{r, eval=F}
survey <- raw |>
  filter(trial_type == "survey-text") |>
  select(workerid, response) |>
  write_csv(here("data/expt1_survey.csv"))

# now go run python file "deal_with_survey.py" to unmangle to json...

survey <- read.csv(here("data/expt1_survey_fixed.csv")) |>
  mutate(foo = map(response, fromJSON)) |>
  unnest_wider(foo) |>
  select(-response)
```

understand: Were the instructions and task clear? Was there anything you found confusing? 

```{r, eval=F}
survey |> select(understand)
```

length: How was the task length? Would you have preferred fewer or more items / a shorter or longer task? (Assume time estimate and payment scale with length).

```{r, eval=F}
survey |> select(length)
```

errors: Were there any problems or errors with the experiment?

```{r, eval=F}
survey |> select(errors)
```

interface: Is there anything that would make the interface better? (ex. bigger text, or a different placement of text and buttons)

```{r, eval=F}
survey |> select(interface)
```

other: Any other comments? 

```{r, eval=F}
survey |> select(other)
```

no issues from pilot study or expt 1

