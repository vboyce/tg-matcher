---
title: "tg-matcher 4: Yoked v shuffled"
output:
  html_document:
    toc: true
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(dev = "png", dev.args = list(type = "cairo-png"))
options(knitr.table.format = "html")
knitr::opts_chunk$set(echo = F)
library(tidyverse)
library(viridis)
library(here)
library(ggthemes)
library(knitr)
library(ggtext)
library(ggimage)
library(jsonlite)
library(brms)
library(rstan)
library(rstanarm)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
theme_set(theme_bw())

dat_loc <- "data/tgmatcheryoked-trials.csv"
pred_loc <- "code/predictors"
```


# Boring stuff
## Read in data

```{r}
raw <- read_csv(here(dat_loc)) |>
  select(-proliferate.condition)

free_response <- raw |>
  filter(is.na(correct_tangram)) |>
  select(workerid, stimulus, response, rt)

good_stuff <- raw |>
  filter(!is.na(correct_tangram)) |>
  select(
    workerid, button_rt, correct, correct_tangram, condition,
    gameId, selected, text, trial_index, type, rt, orig_trialNum, orig_repNum
  ) |>
  mutate(workerid = as.factor(workerid)) |>
  mutate(
    matcher_trialNum = (trial_index - 3) %/% 3,
    matcher_repNum = matcher_trialNum %/% 12
  ) |>
  mutate(workerid = ifelse(workerid == "3157" & condition == "yoked", "3157a", workerid)) |> # somehow two participants were assigned to 3157 -- but each set looks complete?
  filter(workerid != "141") |>
  filter(workerid != "35") # exclude two participants who didn't finish
```

TODO there's something weird where participant 3157 has data for two complete expts (yoked and shuffled). Looks from free response like it's two separate people who just both got the same participant number?  

```{r}
good_stuff |>
  select(workerid, condition) |>
  unique() |>
  group_by(condition) |>
  tally()
```

<!--##  Bonus-->
```{r, eval=F}
# worker <- read_csv(here("data/tgmatcheryoked-workerids.csv")) |> mutate(workerid = as.factor(workerid))
#
# bonuses <- good_stuff |> filter(type=="selection") |>
#   group_by(workerid) |>
#   summarize(bonus = round(sum(correct) * .05, 2)) |>
#   left_join(worker) |>
#   select(prolific_participant_id, bonus) |>
#   write_csv(here("bonus.csv"))
#
# cost <- bonuses |>
#   mutate(cost = bonus * 4 / 3) |>
#   summarize(s = sum(cost))
#
```

# Checks
## SPR trials

```{r}
spr <- good_stuff |>
  filter(type == "reading") |>
  select(-button_rt, -correct, -selected) |>
  mutate(rt = map(rt, fromJSON)) |>
  unnest(rt)

spr_sum <- spr |>
  group_by(workerid) |>
  summarize(RT = sum(rt) / 1000)

ggplot(spr |> filter(rt < 2000), aes(x = rt)) +
  geom_histogram() +
  geom_vline(aes(xintercept = 100))
```
## Selections

```{r}
selections <- good_stuff |>
  filter(type == "selection") |>
  mutate(correct = as.numeric(selected == correct_tangram))

selections |> ggplot(aes(x = orig_repNum, y = button_rt / 1000, color = condition)) +
  geom_jitter(alpha = .05) +
  stat_summary() +
  geom_hline(yintercept = 3)

selections |> ggplot(aes(x = trial_index, y = button_rt / 1000, color = condition)) +
  geom_jitter(alpha = .05) +
  geom_smooth() +
  geom_hline(yintercept = 3)


selections |>
  group_by(gameId, correct_tangram, orig_repNum, condition) |>
  summarize(pct_correct = mean(correct)) |>
  ggplot(aes(x = orig_repNum, y = pct_correct, color = condition)) +
  geom_jitter(alpha = .1, color = "black") +
  stat_summary(fun.data = "mean_cl_boot") +
  geom_smooth() +
  theme(legend.position = "bottom")

selections |>
  group_by(trial_index, condition, gameId) |>
  summarize(pct_correct = mean(correct)) |>
  ggplot(aes(x = trial_index, y = pct_correct, color = condition)) +
  geom_jitter(alpha = .1, color = "black") +
  geom_smooth() +
  coord_cartesian(ylim = c(0, 1)) +
  theme(legend.position = "bottom")
```


## Accuracy

```{r}
acc_by_participant <- good_stuff |>
  filter(type == "selection") |>
  group_by(workerid, condition) |>
  summarize(acc = sum(correct) / n())
```

Maybe 1 or 2 random guessers, not bad. 

```{r}
ggplot(acc_by_participant, aes(x = condition, y = acc)) +
  geom_jitter(height = 0, width = .1) +
  coord_cartesian(ylim = c(0, 1), xlim = c(.5, 2.5), expand = F) +
  geom_hline(yintercept = 1 / 12, linetype = "dashed", color = "blue") +
  stat_summary(data.fun = "mean_cl_boot", color = "blue") +
  labs(y = "Percent correct")
```

## Distribution across games

Small number randomness means we have 5-18 in each cell, but that's decent!

```{r}
good_stuff |>
  select(workerid, gameId, condition) |>
  unique() |>
  group_by(gameId, condition) |>
  tally() |>
  pivot_wider(names_from = condition, values_from = n)
```

# Accuracy

```{r}
good_stuff |>
  filter(type == "selection") |>
  group_by(workerid, condition, orig_repNum) |>
  summarize(acc = sum(correct) / n()) |>
  ggplot(aes(x = orig_repNum, y = acc, color = condition)) +
  geom_jitter(alpha = .1) +
  theme(legend.position = "bottom") +
  geom_smooth()



good_stuff |>
  filter(type == "selection") |>
  group_by(gameId, condition, correct_tangram) |>
  summarize(acc = sum(correct) / n()) |>
  ggplot(aes(x = reorder(correct_tangram, acc, mean), y = acc, color = condition)) +
  geom_jitter(alpha = .5) +
  theme(legend.position = "none") +
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2))

good_stuff |>
  filter(type == "selection") |>
  group_by(gameId, condition, correct_tangram, orig_repNum) |>
  summarize(acc = sum(correct) / n()) |>
  ggplot(aes(x = reorder(correct_tangram, acc, mean), y = acc, color = condition)) +
  theme(legend.position = "none") +
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2)) +
  facet_wrap(~orig_repNum)

good_stuff |>
  filter(type == "selection") |>
  group_by(gameId, condition, orig_repNum, workerid) |>
  summarize(acc = sum(correct) / n()) |>
  ggplot(aes(x = reorder(str_sub(gameId, 1, 2), acc, mean), y = acc, color = condition)) +
  theme(legend.position = "none") +
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2))
```
    
```{r}  
good_stuff |>
  filter(type == "selection") |>
  group_by(workerid, condition, orig_repNum) |>
  summarize(acc = sum(correct) / n()) |>
  ggplot(aes(x = orig_repNum, y = acc, color = condition)) +
  geom_jitter(alpha = .1, color = "black") +
  stat_summary(fun.data = "mean_cl_boot") +
  geom_smooth() +
  theme(legend.position = "bottom")

good_stuff |>
  filter(type == "selection") |>
  group_by(gameId, condition, trial_index) |>
  summarize(acc = sum(correct) / n()) |>
  ggplot(aes(x = trial_index, y = acc, color = condition)) +
  geom_jitter(alpha = .1, color = "black") +
  geom_smooth() +
  coord_cartesian(ylim = c(0, 1)) +
  theme(legend.position = "bottom")
```

```{r}  
good_stuff |>
  filter(type == "selection") |>
  group_by(gameId, condition, orig_repNum, matcher_repNum) |>
  summarize(acc = sum(correct) / n()) |>
  ggplot(aes(x = orig_repNum, y = acc, group = matcher_repNum, color = matcher_repNum)) +
  geom_jitter(alpha = .1, color = "black") +
  stat_summary(fun.data = "mean_cl_boot") +
  theme(legend.position = "bottom") +
  facet_wrap(~condition)

good_stuff |>
  filter(type == "selection") |>
  group_by(gameId, condition, trial_index) |>
  summarize(acc = sum(correct) / n()) |>
  ggplot(aes(x = trial_index, y = acc, color = condition)) +
  geom_jitter(alpha = .1, color = "black") +
  geom_smooth() +
  coord_cartesian(ylim = c(0, 1)) +
  theme(legend.position = "bottom")
```

## For HSP

```{r}
summary2 <- good_stuff |>
  filter(type == "selection") |>
  group_by(condition, gameId, matcher_repNum) |>
  summarize(correct = sum(correct) / n())

good_stuff |>
  filter(type == "selection") |>
  group_by(condition, workerid, matcher_repNum) |>
  ggplot(aes(x = matcher_repNum + 1, y = as.numeric(correct), color = condition)) +
  geom_jitter(data = summary2, alpha = .4, position = position_dodge(width = .2)) +
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2)) +
  stat_summary(fun.data = "mean_cl_boot", geom = "line", position = position_dodge(width = .2, )) +
  labs(y = "Accuracy", x = "Trial block for matchers") +
  theme(legend.position = "bottom") +
  scale_x_continuous(breaks = seq(1, 6, 1)) +
  coord_cartesian(ylim = c(0, 1), xlim = c(.5, 6.5), expand = F) +
  theme(
    legend.title = element_blank(),
    panel.grid = element_blank()
  )

ggsave(here("writing/hsp2.png"), width = 5, height = 3)

summary <- good_stuff |>
  filter(type == "selection") |>
  group_by(condition, gameId, orig_repNum) |>
  summarize(correct = sum(correct) / n())


good_stuff |>
  filter(type == "selection") |>
  group_by(condition, correct_tangram, orig_repNum) |>
  # summarize(acc = sum(correct) / n()) |>
  ggplot(aes(x = orig_repNum + 1, y = as.numeric(correct), color = condition)) +
  # geom_jitter(alpha = .1) +
  geom_jitter(data = summary, alpha = .4, position = position_dodge(width = .2)) +
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2)) +
  stat_summary(fun.data = "mean_cl_boot", geom = "line", position = position_dodge(width = .2, )) +
  theme(legend.position = "bottom") +
  scale_x_continuous(breaks = seq(1, 6, 1)) +
  coord_cartesian(ylim = c(0, 1), xlim = c(.5, 6.5), expand = F) +
  labs(y = "Accuracy", x = "Block in original game") +
  theme(
    legend.title = element_blank(),
    panel.grid = element_blank()
  )

ggsave(here("writing/hsp1.png"), width = 5, height = 3)
```
TODO 

Logistic model of target selection accuracy: Accuracy ~ original_rep_num * condition + viewing_order + (1 | gameId) + (1 | tangram) + (1 | participant)

Time to selection:  Selection_time ~ original_rep_num * condition + viewing_order + (1 | gameId) + (1 | tangram) + (1 | participant)


```{r}
for_acc_mod <- good_stuff |>
  filter(type == "selection") |>
  mutate(correct = as.numeric(correct)) |>
  select(workerid, correct, orig_repNum, condition, matcher_trialNum, gameId, correct_tangram)

acc_priors <- c(
  set_prior("normal(0, 1)", class = "b"),
  set_prior("normal(0,1)", class = "sd")
)

library(lme4)
# test <- glmer(correct~orig_repNum*condition + matcher_trialNum +     (1|gameId)+(1|correct_tangram)+(1|workerid), family=binomial(), data=for_acc_mod)

foo <- brm(correct ~ orig_repNum * condition + matcher_trialNum + (1 | gameId) + (1 | correct_tangram) + (1 | workerid), family = bernoulli(link = "logit"), data = for_acc_mod, prior = acc_priors, file = "acc1.rds")
```
# SPR

need to re-match the words and RTs

```{r}
labeled_yoked <- read_csv(here(pred_loc, "labeled_yoked.csv")) |> rename(orig_repNum = repNum)

spr <- good_stuff |>
  filter(type == "reading") |>
  select(-button_rt, -correct, -selected) |>
  mutate(rt = map(rt, fromJSON)) |>
  left_join(labeled_yoked) |>
  mutate(words = str_split(words, "\\s+")) |>
  unnest(rt, words) |>
  group_by(workerid, correct_tangram, condition, gameId, text, type, orig_trialNum, orig_repNum, matcher_trialNum) |>
  mutate(words = lag(words)) |>
  filter(!is.na(words)) |> 
  mutate(is_from_speaker = case_when(
    words == "speaker**" ~ 1,
    words == "listener**" ~ 0,
    T ~ NA
  )) |>
  fill(is_from_speaker) |>
  mutate(
    is_valid_word = case_when(
      is.na(words) ~ 0,
      words %in% c("speaker**", "listener**") ~ 0,
      T ~ 1
    ),
    word_index = cumsum(is_valid_word),
    is_valid_speaker = ifelse(is_from_speaker == 1 & is_valid_word == 1, 1, 0),
    word_speaker_index = cumsum(is_valid_speaker),
    word_speaker_index = ifelse(is_valid_speaker, word_speaker_index, NA),
    overall_index = row_number()
  ) |>
  select(-is_valid_word, -is_valid_speaker, -trial_index, -matcher_repNum)
```

TODO check units for everything 

```{r, message=T}
kl_pred <- read_csv(here(pred_loc, "kl_full.csv")) |>
  # select(-text, -partial, -condition, -trialNum) |>
  select( -trialNum) |>
  rename(word_speaker_index = partial_length, correct_tangram = tangram, orig_repNum = repNum) |> 
  select(kl,correct_tangram, gameId, orig_repNum, word_speaker_index)

llm_concat_probs <- read_csv(here(pred_loc, "llm_probs_concat.csv")) |> 
  group_by(full_text, word, word_idx) |> 
  summarize(logprob=mean(logprob)) |> # deal with stochasticity in surprisal estimates for when the exact same phrase occurred repeatedly
  mutate(word_speaker_index = word_idx + 1) |>
  rename(words=full_text) |> 
  ungroup() |> 
  left_join(read_csv(here(pred_loc, "expt_4_games_concat.csv")), by=c("words")) |> 
  mutate(surprisal_llm_concat = logprob * log2(exp(1)) * -1) |>
  select(correct_tangram, gameId, orig_repNum = repNum, word_speaker_index, surprisal_llm_concat)

vlm_concat_probs <- read_csv(here(pred_loc, "vlm_probs_concat.csv")) |>
   group_by(full_text, word, word_idx) |> 
  summarize(logprob=mean(logprob)) |> # deal with stochasticity in surprisal estimates for when the exact same phrase occurred repeatedly
    mutate(word_speaker_index = word_idx + 1) |>
  rename(words=full_text) |> 
  ungroup() |> 
  left_join(read_csv(here(pred_loc, "expt_4_games_concat.csv"))) |>
  mutate(surprisal_vlm_concat = logprob * log2(exp(1)) * -1) |>
  select(correct_tangram, gameId, orig_repNum = repNum, word_speaker_index, surprisal_vlm_concat)

llm_as_seen_probs <- read_csv(here(pred_loc, "llm_probs_as_seen.csv")) |> 
  group_by(full_text, word_idx, word) |> 
  summarize(logprob=mean(logprob)) |> 
  mutate(overall_index = word_idx+1, words=full_text) |>
  left_join(read_csv(here(pred_loc, "expt_4_games_as_seen.csv"))) |>
  mutate(surprisal_llm_as_seen = logprob * log2(exp(1)) * -1) |>
  select(correct_tangram, gameId, orig_repNum = repNum, overall_index, surprisal_llm_as_seen, word)

word_freq <- read_csv(here(pred_loc, "word_freq.csv"))

test <- read_csv(here(pred_loc, "expt_4_games_concat.csv")
)
```

A very crappy first pass at seeing if there's any signal 

```{r}
#177,185

spr_labeled <- spr |>
  left_join(kl_pred, by = c("correct_tangram", "gameId", "orig_repNum", "word_speaker_index")) |> 
left_join(word_freq) |> 
  left_join(llm_concat_probs, by=c("correct_tangram", "gameId","orig_repNum", "word_speaker_index")) |> 
  left_join(vlm_concat_probs) |> 
  filter(!is.na(words)) |>
  mutate(word_len = ifelse(words %in% c("speaker**", "listener**"), NA, str_length(words))) |> ungroup() 


spr_resid_1 <- spr_labeled |> 
  mutate(surprisal_llm_concat_resid = lm(surprisal_llm_concat ~ freq + word_len, data=spr_labeled, na.action=na.exclude) |> residuals())

spr_resid_2 <- spr_resid_1 |> 
  mutate(surprisal_vlm_concat_resid = lm(surprisal_vlm_concat ~ freq + word_len + surprisal_llm_concat_resid, data=spr_resid_1, na.action=na.exclude) |> residuals())

spr_resid_3 <- spr_resid_2|> 
  mutate(kl_resid = lm(kl ~ freq + word_len + surprisal_llm_concat_resid, data=spr_resid_2, na.action=na.exclude) |> residuals())


spr_resid_lag <- spr_resid_3 |>
  mutate(across(c("freq", "word_len", "surprisal_llm_concat_resid", "surprisal_vlm_concat_resid", "kl_resid"), list(lag1 = ~ lag(.x), lag2 = ~ lag(.x, 2), lag3 = ~ lag(.x, 3)))) |>
  ungroup() |>
  filter(is_from_speaker == 1) |>
  select(-text, -type) |>
  filter(rt < 2000)

write_csv(spr_resid_lag, "spr_resid_lag.csv")


```

TODO check that we have values for everything we expect to

```{r}
library(lme4)


everything_lag3 <- lmer( rt ~ word_len * freq + kl_resid + surprisal_llm_concat_resid + surprisal_vlm_concat_resid+
                         word_len_lag1 * freq_lag1+ kl_resid_lag1 + surprisal_llm_concat_resid_lag1 + surprisal_vlm_concat_resid_lag1+
                         word_len_lag2 * freq_lag2+ kl_resid_lag2 + surprisal_llm_concat_resid_lag2 + surprisal_vlm_concat_resid_lag2+
                         word_len_lag3 * freq_lag3+ kl_resid_lag3 + surprisal_llm_concat_resid_lag3 + surprisal_vlm_concat_resid_lag3+
  orig_repNum * condition + matcher_trialNum+
     (1 | workerid) + (1 | correct_tangram) + (1 | gameId), 
data = spr_resid_lag)

```


```{r}

summary(everything_lag3)$coef |> as_tibble(rownames="term") |> filter(abs(`t value`)>2) |> View()

```

```{r}

spr_labeled <- read_csv("spr_resid_lag.csv")

spr_priors <- c(
  set_prior("normal(400,100)", class = "Intercept"),
  set_prior("normal(0, 5)", class = "b"),
  set_prior("normal(0, 5)", class = "sd")

)

foo <- brm( rt ~ word_len * freq + kl_resid + surprisal_llm_concat_resid + surprisal_vlm_concat_resid+
                         word_len_lag1 * freq_lag1+ kl_resid_lag1 + surprisal_llm_concat_resid_lag1 + surprisal_vlm_concat_resid_lag1+
                         word_len_lag2 * freq_lag2+ kl_resid_lag2 + surprisal_llm_concat_resid_lag2 + surprisal_vlm_concat_resid_lag2+
                         word_len_lag3 * freq_lag3+ kl_resid_lag3 + surprisal_llm_concat_resid_lag3 + surprisal_vlm_concat_resid_lag3+
  orig_repNum * condition + matcher_trialNum+
     (1 | workerid) + (1 | correct_tangram) + (1 | gameId), 
           data = spr_labeled, prior = spr_priors,
           control = list(adapt_delta = 0.99),
           file = "everything_lag3.rds")

summary(foo)
```

# What we said in the pre-reg

Condition differences: condition refers to yoked or shuffled.

Logistic model of target selection accuracy: Accuracy ~ original_rep_num * condition + viewing_order + (1 | gameId) + (1 | tangram) + (1 | participant)

Time to selection:  Selection_time ~ original_rep_num * condition + viewing_order + (1 | gameId) + (1 | tangram) + (1 | participant)

Per-word reading time analyses: We will compare different predictors of RTs. Because SPR notoriously has lagged effects, we will consider models that incorporate the predictors of either just the target word, or the target word as well as up to three prior words. We are unsure of exactly what predictors will work, so these should be considered exploratory analyses. 

We are unsure of how well mixed effects will fit, but we will aim for this structure. 

Base_model: RT ~ (word_len + word_freq) + original_rep_num * condition + viewing_order + (1| gameId) +  (1| participant) + (1|tangram)

KL divergence of information: From our CLIP model, we will look at the KL divergence in the modelâ€™s predicted distribution of adding that word based on the distributions for the partial description up to that word and the partial description including that word.  RT ~ (word_len + word_freq + KL_div) + original_rep_num * condition + viewing_order + (1| gameId) +  (1| participant) + (1|tangram)

LM surprisal: We will use a non-vision language model (a llama model) to calculate per-word surprisal. RT ~ (word_len + word_freq + surprisal) + original_rep_num * condition + viewing_order + (1| gameId) +  (1| participant) + (1|tangram)

VLM surprisal: We will use a vision language model (llama 3.2) to calculate per-word surprisal conditioned on either the target image or the grid of images with the target highlighted. RT ~ (word_len + word_freq + vision_surprisal) + original_rep_num * condition + viewing_order + (1| gameId) +  (1| participant) + (1|tangram)

We will also do model comparisons using models with multiple of these predictors into order to look at which sources of information (KL divergence, surprisal, vlm surprisal) lead to better fitting models and whether adding multiple of these sources provides additional fit. 

We will also look at which items and  words are best or worst fit by these models. 

Outliers and Exclusions
Describe exactly how outliers will be defined and handled, and your precise rule(s) for excluding observations.
We will exclude individual word RTs that are greater than 2000 ms. 
