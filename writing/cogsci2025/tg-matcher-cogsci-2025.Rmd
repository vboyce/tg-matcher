---
title: "Not so opaque after all: Conventions formed in reference games are mostly understandable to outsiders"
bibliography: tg-matcher-cogsci.bib
csl: apa7.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Veronica Boyce (vboyce@stanford.edu)} \\ Department of Psychology \\ Stanford University \And {\large \bf Ben Prystawski (benpry@stanford.edu)} \\Department of Psychology \\ Stanford University
    \AND {\large \bf Alvin Wei Ming Tan (tanawm@stanford.edu)} \\ Department of Psychology \\ Stanford University \And {\large \bf Michael C. Frank (mcfrank@stanford.edu)} \\ Department of Psychology \\ Stanford University}

abstract: >
    In-groups can create conventionalized language, but this jargon may in fact be understandable to those outside the group. The formation of temporary linguistic conventions between individuals is often studied in iterated reference games, where over repeated reference to the same targets, a describer--matcher pair establishes partner-specific shorthand names for targets. One open question is how understandable these referring expressions are to others who were not part of the convention formation process. We take an outside angle on understanding convention formation, using experiments with naïve matchers and computational models to assess the opacity of descriptions from iterated reference games. Both human matchers and the computational model are well above chance accuracy, with variation in performance primarily driven by the target image rather than where or when the description came from. This additional perspective can inform work on how conventions are formed and how efficient such conventions actually are. 
    
keywords: >
    reference games; convention formation; computational modeling; opacity
    
output: cogsci2024::cogsci_paper
header-includes:
  - \usepackage{booktabs}
#- \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(
  fig.width = 3, fig.height = 3, fig.crop = F,
  fig.pos = "tb", fig.path = "figs/",
  echo = F, warning = F, cache = F,
  message = F, sanitize = T
)

library(png)
library(grid)
library(ggplot2)
library(xtable)
library(tidyverse)
library(here)
library(brms)
library(rstan)
library(rstanarm)
library(ggthemes)
library(jsonlite)
library(ggthemes)
library(scales)
library(viridis)
library(ggridges)
library(cowplot)
library(tidybayes)

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
theme_set(theme_bw())

data_loc <- "data"
mod_loc <- "analysis-code/models"
images <- "experiments/expt1/assets/images"
prediction_loc <- "model-code/model_predictions"
mod_results <- "analysis-code/models/summary"
mod_form <- "analysis-code/models/formulae"
mod_me <- "analysis-code/models/mixed_fx"
```

```{r data_loading, include=FALSE}
expt_1_data <- read_csv(here(data_loc, "expt1_full_data.csv")) |>
  select(-proliferate.condition) |>
  filter(!is.na(response)) |>
  filter(!is.na(correct_tangram)) |>
  select(
    workerid, button_rt, condition, correct, correct_tangram,
    gameId, selected, text, trial_index
  ) |>
  mutate(workerid = as.factor(workerid)) |>
  mutate(rt_sec = button_rt / 1000) |>
  separate(condition, c("group_size", NA, "round")) |>
  mutate(
    group_size = str_c(group_size, "_player"),
    round = str_c("round_", round),
    correct = as.numeric(correct)
  ) |>
  group_by(workerid) |>
  mutate(trial_order = row_number()) |>
  ungroup() |>
  mutate(source = "naïve", expt = "Expt 2", thickness = "medium")

expt_2_data <- read_csv(here(data_loc, "expt2_full_data.csv")) |>
  select(-proliferate.condition) |>
  filter(!is.na(response)) |>
  filter(!is.na(correct_tangram)) |>
  select(
    workerid, button_rt, condition, correct, correct_tangram,
    gameId, selected, text, trial_index
  ) |>
  mutate(workerid = as.factor(workerid)) |>
  mutate(rt_sec = button_rt / 1000) |>
  separate(condition, c("group_size", "thickness", "round")) |>
  mutate(
    condition = str_c(group_size, "_", thickness),
    group_size = str_c(group_size, "_player"),
    round = str_c("round_", round),
    correct = as.numeric(correct)
  ) |>
  group_by(workerid) |>
  mutate(trial_order = row_number()) |>
  ungroup() |>
  mutate(source = "naïve", expt = "Expt 2")

expt_3_data <- read_csv(here(data_loc, "tgmatchercalibration-trials.csv")) |>
  select(-proliferate.condition) |>
  filter(!is.na(response)) |>
  filter(!is.na(correct_tangram)) |>
  select(
    workerid, button_rt, correct, correct_tangram,
    gameId, selected, text, trial_index
  ) |>
  mutate(workerid = as.factor(workerid)) |>
  mutate(
    rt_sec = button_rt / 1000,
    correct = as.numeric(correct)
  ) |>
  group_by(workerid) |>
  mutate(trial_order = row_number()) |>
  ungroup() |>
  mutate(source = "naïve")

expt_4_data <- read_csv(here(data_loc, "tgmatcheryoked-trials.csv")) |>
  select(-proliferate.condition) |>
  filter(!is.na(correct_tangram)) |>
  select(
    workerid, button_rt, correct, correct_tangram, condition,
    gameId, selected, text, trial_index, type, rt, orig_trialNum, orig_repNum
  ) |>
  mutate(workerid = as.factor(workerid)) |>
  mutate(
    matcher_trialNum = (trial_index - 3) %/% 3,
    matcher_repNum = matcher_trialNum %/% 12
  ) |>
  mutate(workerid = ifelse(workerid == "3157" & condition == "yoked", "3157a", workerid)) |> # somehow two participants were assigned to 3157 -- but each set looks complete?
  filter(workerid != "141") |>
  filter(workerid != "35") |> # exclude two participants who didn't finish
  mutate(source = "naïve", round = str_c("round_", orig_repNum + 1))

mlp_mod <- read_csv(here(prediction_loc, "mlp_best.csv")) |>
  pivot_longer(p_A:p_L) |>
  mutate(name = str_sub(name, -1)) |>
  filter(tangram == name) |>
  mutate(
    round = str_c("round_", repNum + 1),
    correct = value,
    source = "model"
  ) |>
  select(correct_tangram = tangram, gameId, round, correct, source)


# Original study accuracies
url <- "https://raw.githubusercontent.com/vboyce/multiparty-tangrams/main/"

one_round_results <- read_rds(str_c(url, "data/study1/round_results.rds")) %>% mutate(rotate = "rotate")
two_a_round_results <- read_rds(str_c(url, "data/study2a/round_results.rds")) %>% mutate(rotate = "no_rotate")
two_b_round_results <- read_rds(str_c(url, "data/study2b/round_results.rds")) %>% mutate(rotate = "full_feedback")
two_c_round_results <- read_rds(str_c(url, "data/study2c/round_results.rds")) |> mutate(rotate = "emoji")
three_round_results <- read_rds(str_c(url, "data/study3/round_results.rds")) |> rename(`_id` = "X_id", condition = name)

one_chat <- read_csv(str_c(url, "data/study1/filtered_chat.csv")) |> mutate(rotate = str_c(as.character(numPlayers), "_rotate"))
two_a_chat <- read_csv(str_c(url, "data/study2a/filtered_chat.csv")) |> mutate(rotate = "no_rotate")
two_b_chat <- read_csv(str_c(url, "data/study2b/filtered_chat.csv")) |>
  mutate(rotate = "full_feedback") |>
  select(-`row num`)
two_c_chat <- read_csv(str_c(url, "data/study2c/filtered_chat.csv")) |>
  mutate(rotate = "emoji") |>
  select(-type)
three_chat <- read_csv(str_c(url, "data/study3/filtered_chat.csv")) |>
  inner_join(read_rds(str_c(url, "data/study3/round_results.rds")) |> select(gameId, trialNum, condition = name) |> unique()) |>
  select(-rowid, -type)

original_results_raw <- one_round_results |>
  rbind(two_a_round_results) |>
  rbind(two_b_round_results) |>
  rbind(two_c_round_results) |>
  mutate(activePlayerCount = NA) |>
  rename(condition = rotate) |>
  rbind(three_round_results) |>
  mutate(
    round = str_c("round_", repNum + 1),
    correct_tangram = tangram,
    correct = ifelse(correct, 1, 0),
    source = "original"
  )

original_results <- original_results_raw |>
  group_by(gameId, correct_tangram, round, source) |>
  summarize(correct = mean(correct)) |>
  select(gameId, correct_tangram, round, correct, source)

original_length <- one_chat |>
  rbind(two_a_chat) |>
  rbind(two_b_chat) |>
  rbind(two_c_chat) |>
  mutate(activePlayerCount = NA) |>
  rename(condition = rotate) |>
  rbind(three_chat) |>
  filter(!is.chitchat) |>
  filter(role == "speaker") |>
  mutate(correct_tangram = str_sub(target, -5, -5)) |>
  group_by(repNum, gameId, correct_tangram, condition, numPlayers) |>
  mutate(utt_length_words = str_count(spellchecked, "\\W+") + 1) %>%
  summarize(
    text = paste0(text, collapse = ", "),
    total_num_words = sum(utt_length_words, na.rm = T) %>% as.numeric(),
    log_words = log(total_num_words)
  ) |>
  mutate(round = str_c("round_", repNum + 1)) |>
  select(gameId, correct_tangram, round, total_num_words, log_words)
```


```{r, eval=F}
# model predicctions (expt 3)

mod_3 <- here(mod_loc, "acc_4.rds") |> read_rds()

preds_3 <- expand_grid(
  matcher_trialNum = 0:71,
  condition = c("yoked", "shuffled"),
  orig_repNum= 0:5, 
    correct_tangram = c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L"))|>
  mutate(matching = matcher_trialNum %/% 12 == orig_repNum) |> 
  filter(matching | condition=="shuffled") |> 
   add_linpred_draws(mod_3, value = "predicted", re_formula = ~ (1 | correct_tangram)) |> 
  group_by(condition, orig_repNum, .draw) |>
  summarize(predicted = mean(predicted)) |>
  group_by(condition, orig_repNum) |>
  summarize(
    mean = mean(predicted),
    low = quantile(predicted, .025),
    high = quantile(predicted, .975)
  ) |> write_rds(here(mod_loc, "predicted", "acc_3.rds"))

mod_3_no_trial <- here(mod_loc, "acc_4_no_trial.rds") |> read_rds()

preds_3_no_trial <- expand_grid(
  matcher_trialNum = 0:71,
  condition = c("yoked", "shuffled"),
  orig_repNum= 0:5, 
    correct_tangram = c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L"))|>
  mutate(matching = matcher_trialNum %/% 12 == orig_repNum) |> 
  filter(matching | condition=="shuffled") |> 
   add_linpred_draws(mod_3_no_trial, value = "predicted", re_formula = ~ (1 | correct_tangram)) |> 
  group_by(condition, orig_repNum, .draw) |>
  summarize(predicted = mean(predicted)) |>
  group_by(condition, orig_repNum) |>
  summarize(
    mean = mean(predicted),
    low = quantile(predicted, .025),
    high = quantile(predicted, .975)
  ) |> write_rds(here(mod_loc, "predicted", "acc_3_no_trial.rds"))

mod_3_mlp <- here(mod_loc, "acc_yoked_mlp_beta.rds") |> read_rds()

preds_3_mlp <- expand_grid(
  orig_repNum= 0:5, 
    correct_tangram = c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L"))|>
   add_linpred_draws(mod_3_mlp, value = "predicted", re_formula = ~ (1 | correct_tangram)) |> 
  group_by(orig_repNum, .draw) |>
  summarize(predicted = mean(predicted)) |>
  group_by(orig_repNum) |>
  summarize(
    mean = mean(predicted),
    low = quantile(predicted, .025),
    high = quantile(predicted, .975)
  ) |> write_rds(here(mod_loc, "predicted", "acc_3_mlp.rds"))

```

```{r, eval=F}
# model predictions  (expt 2)

mod_2a <- here(mod_loc, "acc_1.rds") |> read_rds()
mod_2b <- here(mod_loc, "acc_2.rds") |> read_rds()

preds_2a <- expand_grid(
  trial_order = 1:60, group_size = c("6_player", "2_player"),
  thickness = c("medium"),
  round = c("round_1", "round_6"),
  correct_tangram = c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L")
) |>
  add_linpred_draws(mod_2a, value = "predicted", re_formula = ~ (group_size * round | correct_tangram))


preds_2b <- expand_grid(
  trial_order = 1:60, group_size = c("6_player", "2_player"),
  thickness = c("thin", "thick"),
  round = c("round_1", "round_6"),
  correct_tangram = c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L")
) |>
  add_linpred_draws(mod_2b, value = "predicted", re_formula = ~ (group_size * thickness * round | correct_tangram))

preds_2 <- preds_2a |> bind_rows(preds_2b)

preds_2_by_cond <- preds_2 |>
  group_by(group_size, round, thickness, .draw) |>
  summarize(predicted = mean(predicted)) |>
  group_by(group_size, round, thickness) |>
  summarize(
    mean = mean(predicted),
    low = quantile(predicted, .025),
    high = quantile(predicted, .975)
  ) |>
  write_rds(here(mod_loc, "predicted", "acc_2_cond.rds"))

preds_2_by_tangram <- preds_2 |>
    group_by(group_size, round, thickness, .draw, correct_tangram) |>
  summarize(predicted = mean(predicted)) |>
  group_by(group_size, correct_tangram) |>
  summarize(
    mean = mean(predicted),
    low = quantile(predicted, .025),
    high = quantile(predicted, .975)
  ) |>
  write_rds(here(mod_loc, "predicted", "acc_2_tangram.rds"))


# model predictions mlp expt 2

mod_2_mlp <- here(mod_loc, "acc_mlp_1_2_beta.rds") |> read_rds()

preds_2_mlp <- expand_grid(
  group_size = c("6_player", "2_player"),
  thickness = c("thick", "thin", "medium"),
  round = c("round_1", "round_6"),
  correct_tangram = c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L")
) |>
  add_linpred_draws(mod_2_mlp, value = "predicted", re_formula = ~ (group_size * thickness * round | correct_tangram))

preds_2_mlp_condition <- preds_2_mlp |>
  group_by(group_size, round, thickness, .draw) |>
  summarize(predicted = mean(predicted)) |>
  group_by(group_size, round, thickness) |>
  summarize(
    mean = mean(predicted),
    low = quantile(predicted, .025),
    high = quantile(predicted, .975)
  ) |>
  write_rds(here(mod_loc, "predicted", "acc_2_mlp_cond.rds"))

preds_2_mlp_tangram <- preds_2_mlp |>
  group_by(group_size, round, thickness, .draw, correct_tangram) |>
  summarize(predicted = mean(predicted)) |>
  group_by(group_size, correct_tangram) |>
  summarize(
    mean = mean(predicted),
    low = quantile(predicted, .025),
    high = quantile(predicted, .975)
  ) |>
  write_rds(here(mod_loc, "predicted", "acc_2_mlp_tangram.rds"))
```

```{r, eval=F}
library(tidybayes)

save_summary <- function(model) {
  intervals <- gather_draws(model, `b_.*`, regex = T) %>% mean_qi()

  stats <- gather_draws(model, `b_.*`, regex = T) %>%
    mutate(above_0 = ifelse(.value > 0, 1, 0)) %>%
    group_by(.variable) %>%
    summarize(pct_above_0 = mean(above_0)) %>%
    left_join(intervals, by = ".variable") %>%
    mutate(
      lower = .lower,
      upper = .upper,
      Term = str_sub(.variable, 3, -1),
      Estimate = .value
    ) %>%
    select(Term, Estimate, lower, upper)

  stats
}

save_me <- function(model) {
  intervals <- gather_draws(model, `sd_.*`, regex = T) %>%
    mean_qi() |>
    separate(.variable, into = c("group", "Term"), sep = "__") |>
    mutate(
      lower = .lower,
      upper = .upper,
      group = str_sub(group, 4, -1),
      Estimate = .value
    ) %>%
    select(group, Term, Estimate, lower, upper)

  intervals
}

do_model <- function(path) {
  model <- read_rds(here(mod_loc, path))
  save_summary(model) |> write_rds(here(mod_loc, "summary", path))
  model$formula |> write_rds(here(mod_loc, "formulae", path))
  print(summary(model))
}

do_me <- function(path) {
  model <- read_rds(here(mod_loc, path))
  message(path)
  save_me(model) |> write_rds(here(mod_loc, "mixed_fx", path))
}

mods_me <- c(
  "acc_1.rds", "acc_2.rds", "acc_4.rds", "acc_mlp_1_2_beta.rds", "acc_yoked_mlp_beta.rds", "yoked_shuffled_original.rds"
) |> walk(~ do_me(.))


mods <- list.files(path = here(mod_loc), pattern = ".*rds") |> walk(~ do_model(.))
```

```{r}
stats <- function(model, row, decimal = 2) {
  model <- model |>
    mutate(
      Estimate = round(Estimate, digits = decimal),
      Lower = round(lower, digits = decimal),
      Upper = round(upper, digits = decimal),
      `Credible Interval` = str_c("[", Lower, ", ", Upper, "]")
    ) |>
    select(Term, Estimate, `Credible Interval`)
  str_c(model[row, 1], ": ", model[row, 2], " ", model[row, 3])
}

stats_text <- function(model, row, decimal = 2) {
  model <- model |>
    mutate(
      Estimate = round(Estimate, digits = decimal),
      Lower = round(lower, digits = decimal),
      Upper = round(upper, digits = decimal),
      `Credible Interval` = str_c("[", Lower, ", ", Upper, "]")
    ) |>
    select(Term, Estimate, `Credible Interval`)
  str_c(model[row, 2], "  ", model[row, 3])
}

form <- function(model_form) {
  dep <- as.character(model_form$formula[2])
  ind <- as.character(model_form$formula[3])

  str_c(dep, " ~ ", ind) |>
    str_replace_all(" ", "") |>
    str_replace_all("\\*", " ${\\\\times}$ ") |>
    str_replace_all("\\+", "&nbsp;${+}$ ") |>
    str_replace_all("~", " ${\\\\sim}$ ")
}
```


# Introduction

Groups of people often have terms that are used within a group, such as regional dialects, field-specific jargon, or terms of art related to specific theoretic orientations. Those who are not part of the group may use different terms for the same targets, but may nonetheless be able to understand or guess at the meanings of in-group terms. While these terms can represent stable conventions shared by sizable communities, temporary naming conventions can also develop rapidly among small groups of people when there is a need to refer to something without a canonical name that distinguishes it in context. 

This formation of temporary linguistic conventions between individuals is often studied in iterated reference games. In these games, a describer tells their partner how to sort or match a series of abstract images [e.g., @clark1986; @hawkins2020b]. Over repeated rounds of referring to the same targets, pairs usually develop conventionalized nicknames for the target images. These nicknames are often partner-specific, in that different pairs develop different nicknames for the same targets. When describing the shapes for people who were not part of the group, people return to more elaborated descriptions, indicating an expectation that others may be unable to understand the convention, and need a longer or different referential description [@yoon2018; @wilkes-gibbs1992; @hawkins2021]. Participants treat later-stage conventions as more opaque to outsiders than earlier-stage descriptions, and the differentiation of labels across groups over time is a signal of arbitrariness in the conventions [@hawkins2020b; @boyce2024]. 

How arbitrary and opaque are these conventions, really? One way to conceptualize the opacity of a referring expression is by considering the semantic distance between the signifier and the referent. Under the assumption of a modality-independent global semantics (i.e., not conditioned by partner-specific meaning), expressions that are transparent have signifiers and referents that are semantically close, such that any member of the sociolinguistic community sharing the global semantics should be able to identify the appropriate referent given the signifier. In contrast, expressions that are opaque have signifiers and referents that are semantically distant, such that the relations between them are more arbitrary and inaccessible to the general community without the formation of additional conventions (which may be partner- or group-specific).

Assessing opacity thus requires us to have a measure of modality-independent global semantics. Such semantics are difficult to directly obtain for humans, since we rarely have explicit semantic formulations of stimuli, much less formulations that are unified across multiple modalities. However, another approach is to look at how understandable conventions are to others who were not part of the interaction that originated the convention. The ability of a naïve comprehender to understand a referring expression presented without context provides a proxy measure, since the comprehender's judgements are not conditioned on any context-specific conventions. Conventions that are more arbitrary and opaque should thus be harder for naïve matchers to understand, and we can determine how the features of the utterances and the conditions they were produced under affect their opacity, providing empirical tests of whether descriptions become more opaque over the course of the reference game. 

Some prior work has investigated how well naïve matchers can understand the descriptions produced in the course of an iterated reference game, with a focus on the role of conversational shared history. @murfitt2001 recorded descriptions from 8 participants who described shapes either solo or in a matching game with a partner and played the descriptions to new matchers, either in order or in reverse order. Naïve listeners were more accurate when the heard descriptions in order. @schober1989 found that matchers in an iterated reference game achieved higher accuracies than overhearers who listened to the entire game in order, and overhearers who listened to recordings starting in the third round did even worse. Nonetheless, all matchers were far above chance, and their accuracies rose over rounds. <!-- not sure whether to include --> <!--While not framed as testing the opacity of referential conventions, -->@leung2024 had adults and children serve as naïve matchers, listening to descriptions from the the first and last rounds of a parent--child iterated reference game. They found no significant difference in how comprehensible early and late descriptions were to naïve matchers, and while naïve matchers had high accuracy<!-- (88% on 2-way choice)-->, they were slightly less accurate than in-game matchers overall. <!-- the comparison comes from eye-balling plots, since not all the numbers are reported. -->

In an iterated reference game using drawing (rather than text) as the communication modality, @hawkins2023a compared the accuracy of in-game matchers to naïve matchers in yoked and shuffled conditions. The yoked matchers saw all the trials from one game in order, while the shuffled matchers saw trials sampled from 10 games but in trial order. In-game matchers were more accurate overall than yoked matchers who were in turn more accurate than shuffled matchers.<!--Note they don't report this in the paper, but their data and code are available and I found where I could get the numbers 88 and 75 (actually 76?,) and 69 is the corresponding one for shuffled--> Over the course of trials, both in-game and yoked matchers showed steeper improvement in accuracy than shuffled matchers. 

Across the prior literature, while naïve matchers have worse accuracy than in-game matchers, their performance is still far above chance, suggesting that the convention--target relationship is not purely arbitrary. In fact, even when pairs of participants try to obfuscate their meaning to match images with each other but not an overhearer, an overhearing participant can still do quite well [@clark1987a]. Nonetheless, receiving more context from an interaction---and in particular, having that context be in order---is beneficial to matchers. Except for the shuffled condition of @hawkins2023a, these studies do not address how opaque descriptions from different points in the game are without prior context. <!-- So I can't actually tell from the paper or the supplement whether the leung et al stuff was presented in order per game or even all from the same game to each participant. I'm guessing it is per game, but they also clean up the referential expressions and don't give feedback so...-->Such an approach would be important to have truly naïve matchers that lack even the context of trial order, providing a clearer understanding of opacity.

Another possible measure of modality-independent global semantics is computational in origin. Computational methods have enabled the embedding of various stimuli (including images and text) into high-dimensional feature spaces; these embeddings have properties which suggest that they are reasonable approximations of humans' semantic spaces, including similarity in representational geometries [e.g., @grand2022; @muttenthaler2021]. Indeed, embeddings from neural network models have been used as a form of semantics in a range of reference game scenarios [e.g., @gul2024; @ji2022; @kang2020; @le2022; @ohmer2022]. In particular, such embeddings can be treated as context-independent semantic representations, since they are not updated to account for convention formation within an iterated reference game; hence, they can serve as a computational comparison to human performance on a naïve matching task.

In the current work, we address how the process of convention formation shapes the levels of opacity of the referring expressions created at different time points in an iterated reference game. Using reference expressions created in different games from @boyce2024, we use both human experiments and models to assess when and why expressions are opaque or understandable to outside observers. 

```{r interface, fig.env = "figure", fig.pos = "t!", out.width="100%", fig.align = "center", fig.cap = "Experimental Setup and Procedure. TODO \\label{game}", cache=FALSE}
knitr::include_graphics("matcher-diagram.pdf", error = FALSE)
```

# Task setup

## Materials

We drew on the corpus of reference game transcripts and results from @boyce2024. This corpus is made up of 6-round iterated reference games using the same 12 target images. Games were played in conditions varying in how large the describer--matcher groups were (2--6 participants) and how "thick" the communication channels were. For our naïve matcher experiments, we sampled different subsets of this corpus. In presenting the transcripts, we excluded utterances that were marked by @boyce2024 as not containing any referential content (i.e. purely greetings, meta-commentary, or off-topic chitchat). Within the samples, we also did not show transcripts that contained swear words or crude or sexual language. We used the entire corpus for our computational modeling component. 

## Experimental procedure 

We recruited English-speaking participants from Prolific. Participants were directed to the experiment, where the task was explained to them. On each trial, participants saw the full transcript from that trial, containing all the chat messages marked by whether they were from the speaker or a listener. Participants selected the image they thought was the target from the tableau of 12 (Figure \ref{game}). Participants received feedback on whether they were right or wrong on each trial. Except when the specific viewing order was part of the experimental manipulation, we randomized the order of trials, subject to the constraint that the same target could not repeat on adjacent trials. The task was implemented in jsPsych [@leeuw2023]. We paid participants $10 an hour plus a bonus of 5 cents per correct response. All our experimental code is at TODO LINK. 

## Computational models

We used CLIP (`clip-vit-large-patch14`) as a listener model for our domain [@radford2021]. CLIP is a natural choice for reference games, as the model is trained to estimate the correspondence between images and phrases in natural language. We pre-processed the text by concatenating all the messages sent by the speaker for a given trial, and ran CLIP for the concatenated text and all 12 tangram shapes. We then computed probabilities for each tangram shape using CLIP's logits. The simplest way to do this is simply taking the softmax of the logits. However, the tangram shapes were possibly out the distribution for the model, which led it to favor some images over others regardless of the content of the text. To fix this, we trained readout models that made more calibrated predictions using CLIP's logits.

```{r, eval=F}
df_classifiers <- read_csv(here("data/classifier_comparison-openai--clip-vit-large-patch14.csv"))
df_clf_comp <- df_classifiers |>
  group_by(classifier, params) |>
  summarize(mean_acc = mean(accuracy), sd_acc = sd(accuracy))

df_best_clfs <- df_clf_comp |>
  group_by(classifier) |>
  mutate(
    mean_acc = round(mean_acc, 2),
    sd_acc = round(sd_acc, 2)
  ) |>
  arrange(desc(mean_acc)) |>
  ungroup()

xtable(df_best_clfs)

df_clf_comp |> arrange(desc(mean_acc))

# TODO: add base CLIP results
```

```{=latex}
\begin{table}
\caption{Cross-validated accuracies for classifiers. Standard deviations in accuracy across the 10 folds are shown in parentheses. Best performance within each model class is underlined, and best overall performance is bolded.}
\label{tab:classifier_comparison}
\centering

  \begin{tabular}{p{1em}lr}
    \toprule
    \multicolumn{2}{l}{Classifier} & Accuracy \\ 
    \midrule
    \multicolumn{2}{l}{Base CLIP TODO} & \\
    & No penalty & \underline{\smash{TODO BEN}} \\
    \multicolumn{2}{l}{Logistic regression} & \\
    & No penalty & \underline{\smash{0.50 (0.01)}} \\ 
    & \vspace{1mm}L2 penalty & 0.50  (0.01) \\ 
    \multicolumn{2}{l}{Random forest} & \\
    & 10 estimators & 0.46 (0.02) \\
    & 50 estimators  & 0.51 (0.02)\\ 
    & 100 estimators & 0.52 (0.02) \\ 
    & \vspace{1mm}500 estimators & \underline{\smash{0.52 (0.02)}} \\ 
    \multicolumn{2}{l}{Gradient-boosted tree} & \\
    & 10 estimators & 0.48 (0.02) \\ 
    & \vspace{1mm}100 estimators & \underline{\smash{0.51 (0.02)}} \\ 
    \multicolumn{2}{l}{Multi-layer perceptron} & \\
    & 1 $\times$ 32-dim hidden layer & 0.50 (0.01) \\ 
    & 1 $\times$ 100-dim hidden layer  & 0.52 (0.01) \\ 
    & 1 $\times$ 512-dim hidden layer & 0.53 (0.02) \\ 
    & 1 $\times$ 1028-dim hidden layer & 0.53 (0.02) \\ 
    & 2 $\times$ 32-dim hidden layers  & 0.51 (0.02) \\ 
    & 2 $\times$ 100-dim hidden layers & \underline{\smash{\textbf{0.55 (0.02)}}} \\ 
    \bottomrule
	\end{tabular}
\end{table}
```

We trained different readout models to assign probabilities to features using CLIP's logits as features. Models were trained to maximize task performance (i.e., to assign high probability to the target tangram given the concatenated speaker utterance). We compared four types of models: a random forest, a logistic regression model, a multi-layer perceptron (MLP), and a gradient-boosted tree. Classifiers were implemented in the `scikit-learn` and `XGBoost` libraries [@pedregosa2011scikit; @chen2016xgboost]. Table \ref{tab:classifier_comparison} shows the cross-validated accuracy of different readout models, as well as the performance of CLIP with no readout. The MLP with two hidden layers of size 100 performed the best, so we use its predictions in subsequent analysis.

# Experiment 1
```{r}
# this is all because I didn't keep the trial number source recorded, so we have a fun time rejoining
ParseJSONColumn <- function(x) {
  str_replace_all(x, "'", '"') %>%
    str_replace_all('Don"t know', "Don't know") %>%
    str_replace_all('don"t', "don't") |>
    str_replace_all("None", '"NA"') |>
    str_replace_all('"SAFE"', "'SAFE'") |>
    str_replace_all('he"s', "he's") |>
    str_replace_all('doesn"t', "doesn't") |>
    str_replace_all('hasn"t', "hasn't") |>
    str_replace_all('it"s', "it's") |>
    str_replace_all('It"s', "It's") |>
    str_replace_all('X" shaped', "'X' shaped") |>
    str_replace_all('"missing', "missing") |>
    str_replace_all('"flying"', "flying") |>
    str_replace_all('"skating"', "skating") |>
    str_replace_all('"partially sitting"', "partially sitting") |>
    str_replace_all('"kneeling" and ', "kneeling and ") |>
    str_replace_all('"partially kneeling"', "partially kneeling") |>
    str_replace_all('and "bunny"', "and bunny") |>
    str_replace_all('"square"', "square") |>
    str_replace_all('heads"', "heads") |>
    str_replace_all('"italy"', "italy") |>
    str_replace_all('they"re', "they're") |>
    str_replace_all('"but why"', "'but why'") |>
    fromJSON(flatten = T)
}

labels <- read_csv(here("expt_prep_code/labelled.csv")) |>
  mutate(text = str_replace_all(text, "'", "") |> str_replace_all('"', "")) |>
  group_by(tangram, gameId, trialNum, repNum, value, grouping) |>
  summarize(text = str_c(text, collapse = " "))

expt_3_ready <- expt_3_data |>
  mutate(new = map(text, ParseJSONColumn)) |>
  select(-text) |>
  unnest(new) |>
  mutate(text = text |> str_replace_all("'", ""), tangram = correct_tangram) |>
  group_by(workerid, correct, tangram, gameId, trial_order) |>
  summarize(text = str_c(text, collapse = " ")) |>
  left_join(labels)

expt_3_summary <- expt_3_ready |>
  group_by(text, value, tangram, grouping) |>
  summarize(human_acc = mean(correct), human_n = n()) |>
  ungroup() |>
  mutate(grouping = as.factor(grouping) |> reorder(value)) |>
  group_by(grouping)

expt_3_groups <- expt_3_summary |>
  group_by(grouping) |>
  mutate(value = mean(value))

expt_3_cor <- cor.test(expt_3_summary$value, expt_3_summary$human_acc)
```

```{r fig-calibration, fig.env="figure", fig.pos = "t", fig.align = "center", out.width="70%", fig.width=3, fig.height=3, fig.cap = "Correlation between human and CLIP-MLP probability of target across deciles of CLIP-MLP probability. Colored points are individual descriptions, black line is the bootstrapped mean and 95% CI across descriptions for each decile. \\label{calibration}" }
expt_3_summary |> ggplot(aes(y = human_acc, x = value)) +
  geom_point(aes(color = grouping), alpha = .7) +
  # geom_line(data = summ, color = "black") +
  stat_summary(data = expt_3_groups) +
  stat_summary(data = expt_3_groups, fun.data = "mean_cl_boot", geom = "line") +
  theme(legend.position = "none") +
  scale_color_viridis(discrete = T) +
  labs(x = "CLIP-MLP target probability", y = "Human accuracy") +
  coord_equal()
```

Our CLIP-MLP computational model was optimized for task accuracy.
To validate whether this objective also results in human-like response patterns, we conducted a calibration experiment to determine if, for any given utterance, the model-assigned target probability was aligned with the probability that a naïve human matcher would choose the target image.
<!-- We hypothesized that there would be a significant correlation between the target choice probability of the model and the target choice probability of naïve human matchers. -->

## Methods

We first obtained target probabilities from our CLIP-MLP model for all utterances from @boyce2024. We then used stratified sampling to select 217 trials by dividing model-predicted probabilities into deciles and choosing approximately 22 utterances per decile, spanning the 12 different possible target images.

We recruited `r expt_3_data |> select(workerid) |> n_distinct()` participants who each saw 64 trials randomly sampled from the 217 tested trials. On average, each trial was seen by `r expt_3_summary |> pull(human_n) |> mean() |> round()` participants. This experiment was pre-registered at https://osf.io/6pv5e.

## Results

We obtained human accuracies on each trial by dividing the number of participants who selected the target by the total number of participants who saw the trial (Figure \ref{fig:fig-calibration}). There was a small but significant positive correlation between model-predicted probabilities and human accuracies ($r$ = `r round(expt_3_cor$estimate, digits = 2)` [`r round(expt_3_cor$conf.int[1], digits = 2)`, `r round(expt_3_cor$conf.int[2], digits = 2)`]). This result suggests that model predictions were calibrated to human response patterns, albeit not perfectly. It is possible to use these calibration results to tune model predictions to better approximate human responses; we leave this approach for future work. Nonetheless, the observed positive correlation suggests that our computational model is a reasonable approximation of human accuracies, validating its use in subsequent experiments as a computational comparison.



```{r}
expt_2_relevant_games <- expt_1_data |>
  bind_rows(expt_2_data) |>
  select(gameId, group_size, thickness, round) |>
  unique()

expt_2_fig_data <- expt_1_data |>
  bind_rows(expt_2_data) |>
  bind_rows(mlp_mod |> inner_join(expt_2_relevant_games)) |>
  bind_rows(original_results |> inner_join(expt_2_relevant_games)) |>
  mutate(thickness = factor(thickness, levels = c("thin", "medium", "thick"))) |>
  mutate(source = factor(source, levels = c("original", "naïve", "model"), labels = c("Original", "naïve human", "CLIP-MLP"))) |>
  mutate(round = str_sub(round, -1))


predicted_expt_2_fig <- read_rds(here(mod_loc, "predicted/acc_2_cond.rds")) |>
  mutate(source = "naïve") |>
  bind_rows(read_rds(here(mod_loc, "predicted/acc_2_mlp_cond.rds")) |> mutate(source = "model")) |>
  mutate(source = factor(source, levels = c("original", "naïve", "model"), labels = c("Original", "naïve human", "CLIP-MLP"))) |>
  mutate(across(mean:high, inv_logit_scaled)) |>
  mutate(thickness = factor(thickness, levels = c("thin", "medium", "thick"))) |>
  mutate(round = str_sub(round, -1))


tangram_predicted_expt_2_fig <- read_rds(here(mod_loc, "predicted/acc_2_tangram.rds")) |>
  mutate(source = "naïve") |>
  bind_rows(read_rds(here(mod_loc, "predicted/acc_2_mlp_tangram.rds")) |> mutate(source = "model")) |>
  mutate(source = factor(source, levels = c("original", "naïve", "model"), labels = c("Original", "naïve human", "CLIP-MLP"))) |>
  mutate(across(mean:high, inv_logit_scaled))
```

```{r fig-condition, fig.env="figure", fig.pos = "t", fig.align = "center", out.width="90%", fig.width=4, fig.height=3, fig.cap = "Accuracies for naïve humans and the CLIP-MLP model for Experiment 2. Point estimates and 95% CrI are predictions from the fixed effects of logistic and beta regressions. Bootstrapped mean accuracy from the original matchers is included as a ceiling, and random chance as a baseline. \\label{expt2-condition}" }
expt_2_fig_data |> ggplot(aes(x = round, color = source, y = correct, group = interaction(source, group_size), shape = group_size, lty = group_size)) +
  stat_summary(data = expt_2_fig_data |> filter(source == "Original"), geom = "point", fun.data = "mean_cl_boot", position = position_dodge(.4)) +
  stat_summary(data = expt_2_fig_data |> filter(source == "Original"), geom = "line", fun.data = "mean_cl_boot", position = position_dodge(.4)) +
  
#stat_summary(data = expt_2_fig_data , geom = "point", fun.data = "mean_cl_boot", position = position_dodge(.4)) +
 # stat_summary(data = expt_2_fig_data , geom = "line", fun.data = "mean_cl_boot", position = position_dodge(.4)) +
  geom_pointrange(data = predicted_expt_2_fig, aes(y = mean, ymax = high, ymin = low), position = position_dodge(.4)) +
  geom_line(data = predicted_expt_2_fig, aes(y = mean), position = position_dodge(.4)) +
  facet_grid(. ~ thickness) +
  scale_y_continuous(lim = c(0, 1), expand = c(0, 0)) +
  geom_hline(yintercept = 1 / 12, lty = "dashed") +
  labs(x = "Round", y = "Accuracy") +
  scale_color_manual(breaks = c("Original", "naïve human", "CLIP-MLP"), values = c("#7570B3", "#1B9E77", "#D95F02")) +
  theme(
    legend.position = "bottom", legend.box = "vertical",
    strip.background = element_blank(),
    legend.title = element_blank(),
    legend.margin = margin(c(-10, 1, 0, 1))
  )
```


```{r fig-2, fig.env="figure", fig.pos = "t", fig.align = "center", out.width="90%", fig.width=4, fig.height=3, fig.cap = "Accuracies for naïve humans and the CLIP-MLP model for Experiment 2, split out by target image. Point estimates and 95% CI are predictions from the fixed effects and by-tangram random effects of logistic and beta regressions, bootstrapped across conditions. Bootstrapped mean accuracy from the original matchers is included as a ceiling, and random chance as a baseline. \\label{expt2-tangram}" }
library(ggtext)
correct_tangram <- c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L")
labels <- c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L") |> map(~ str_c("<img src=", here(images, str_c("tangram_", ., ".png")), " width='20'/>"))


acc_by_target <- expt_1_data |>
  bind_rows(expt_2_data) |>
  group_by(correct_tangram) |>
  summarize(acc = sum(correct) / n()) |>
  arrange(acc)

foo <- tibble(correct_tangram, labels) |>
  left_join(acc_by_target) |>
  arrange(acc)

acc_by_type_target <- expt_2_fig_data |>
  group_by(correct_tangram, group_size, round, thickness, source) |>
  summarize(acc = sum(correct) / n()) |>
  mutate(correct_tangram = factor(correct_tangram, levels = acc_by_target$correct_tangram))

ggplot(acc_by_type_target, aes(x = correct_tangram, y = acc, color = source, shape = group_size, lty = group_size)) +
  geom_pointrange(data = tangram_predicted_expt_2_fig, aes(y = mean, ymax = high, ymin = low), position = position_dodge(.4)) +
  #  stat_summary(data = acc_by_type_target, aes(color = source), geom = "point", position = position_dodge(width = .6)) +

  stat_summary(data = acc_by_type_target |> filter(source == "Original"), aes(color = source), geom = "point", position = position_dodge(width = .6)) +
  coord_cartesian(ylim = c(0, 1), xlim = c(.5, 12.5), expand = F) +
  geom_hline(yintercept = 1 / 12, linetype = "dashed") +
  scale_x_discrete(name = NULL, labels = foo$labels) +
  scale_color_manual(breaks = c("Original", "naïve human", "CLIP-MLP"), values = c("#7570B3", "#1B9E77", "#D95F02")) +
  labs(y = "Accuracy") +
  theme(
    legend.position = "bottom", legend.box = "vertical",
    strip.background = element_blank(),
    legend.title = element_blank(),
    axis.text.x = element_markdown(color = "black", size = 11),
    legend.margin = margin(c(-10, 1, 0, 1))
  )


# guide_legend()# acc_by_target
```

# Experiment 2

As a starting point for examining what makes referential expressions more or less opaque, we focused on referring expressions from the first and last rounds of games. Principles of convention formation and people's behavior when switching to a new partner suggest that later-round utterances are more opaque and thus harder to understand. One counterargument is that later rounds are the result of describers' accumulated practice refining descriptions to be maximally communicative and to pick out the most visually salient features. We included descriptions from games of different sizes and communication thicknesses. Based on the patterns of cross-game similarity in @boyce2024, we expected that smaller and thicker games, whose descriptions diverged fastest, would have more idiosyncratic and opaque conventions than larger groups with thinner communication channels. 

## Methods

### Experiment 2a
To establish a baseline of how well naïve matchers could understand descriptions without context, we ran a 2x2 within subjects experiment. We drew the target transcripts from 2- and 6-player games from Experiment 1 of @boyce2024 and from the first and last blocks of these games. These games had medium-thick communication channels. <!-- in that the matchers could send textual messages to the shared chat interface, but the describer role rotated each round, and matchers recieved feedback only about whether their selection was correct or not.-->
We recruited `r expt_1_data |> select(workerid) |> n_distinct()` participants who each saw 60 trials (15 in each of the 4 conditions). Overall, participants saw `r expt_1_data |> select(gameId, round, correct_tangram) |> n_distinct()` transcripts from `r expt_1_data |> select(gameId) |> n_distinct()` games. This experiment was pre-registered at https://osf.io/k45dr. 

### Experiment 2b
After observing limited condition differences in Experiment 2a, we ran a follow-up experiment on descriptions from Experiment 3 of @boyce2024, where the communication channel thicknesses were more extreme. Here, we used a 2x2x2 within subjects design, drawing our transcripts from the first and last rounds of thick and thin, 2- and 6- person games. <!--The "thick" condition had a consistent describer throughout the entire game, let matchers send messages to the chat freely, and showed all participants feedback on everyone's selections, including what the correct answer was. In contrast, the "thin" condition, rotated the role of describer, gave feedback only on individual selections, and describers could not send text messages to the chat, but could only communicate via 4 emoji buttons (to indicate level of understanding). --> In the thin condition, original matchers could only contribute to the chat by sending one of 4 emojis; as the emojis did not have referential content, we did not include them in the transcripts shown to naïve matchers. 
For experiment 2b, we recruited `r expt_2_data |> select(workerid) |> n_distinct()` participants who each saw 64 trials (8 in each of the 8 conditions). Overall, participants saw `r expt_2_data |> select(gameId, round, correct_tangram) |> n_distinct()` transcripts from `r expt_2_data |> select(gameId) |> n_distinct()` games.  This experiment was pre-registered at  https://osf.io/rdp5k. 

```{r, eval=F}
acc_priors <- c(
  set_prior("normal(0,1)", class = "b"),
  set_prior("normal(0,1)", class = "sd"),
  set_prior("lkj(1)", class = "cor")
)


acc_mod_1 <- brm(
  correct ~ group_size * round + trial_order +
    (group_size * round | correct_tangram) +
    (group_size * round + trial_order | workerid),
  data = expt_1_data,
  family = bernoulli(),
  file = here(mod_loc, "acc_1"),
  prior = acc_priors,
  control = list(adapt_delta = .95)
)

# summary(acc_mod_1)
```

```{r, eval=F}
acc_mod_2 <- brm(
  correct ~ group_size * thickness * round + trial_order +
    (group_size * thickness * round | correct_tangram) +
    (group_size * thickness * round + trial_order | workerid),
  data = expt_2_data,
  family = bernoulli(),
  file = here(mod_loc, "acc_2"),
  prior = acc_priors,
  control = list(adapt_delta = .95)
)


# summary(acc_mod_2)
```

```{r}
original_acc <- original_results |>
  rename(original_correct = correct) |>
  ungroup() |>
  select(round, correct_tangram, gameId, original_correct)

expt_1_data_augment <- expt_1_data |>
  left_join(original_length) |>
  left_join(original_acc)
expt_2_data_augment <- expt_2_data |>
  left_join(original_length) |>
  left_join(original_acc)
```

```{r, eval=F}
acc_mod_1_orig_acc <- brm(
  correct ~ original_correct + group_size * round + trial_order +
    (group_size * round | correct_tangram) +
    (group_size * round + trial_order | workerid),
  data = expt_1_data_augment,
  family = bernoulli(),
  file = here(mod_loc, "acc_1_orig_acc"),
  prior = acc_priors,
  control = list(adapt_delta = .95)
)

acc_mod_1_orig_length <- brm(
  correct ~ log_words + group_size * round + trial_order +
    (group_size * round | correct_tangram) +
    (group_size * round + trial_order | workerid),
  data = expt_1_data_augment,
  family = bernoulli(),
  file = here(mod_loc, "acc_1_orig_length"),
  prior = acc_priors,
  control = list(adapt_delta = .95)
)

acc_mod_2_orig_acc <- brm(
  correct ~ original_correct + group_size * thickness * round + trial_order +
    (group_size * thickness * round | correct_tangram) +
    (group_size * thickness * round + trial_order | workerid),
  data = expt_2_data_augment,
  family = bernoulli(),
  file = here(mod_loc, "acc_2_orig_acc"),
  prior = acc_priors,
  control = list(adapt_delta = .95)
)

acc_mod_2_orig_length <- brm(
  correct ~ log_words + group_size * thickness * round + trial_order +
    (group_size * thickness * round | correct_tangram) +
    (group_size * thickness * round + trial_order | workerid),
  data = expt_2_data_augment,
  family = bernoulli(),
  file = here(mod_loc, "acc_2_orig_length"),
  prior = acc_priors,
  control = list(adapt_delta = .95)
)
```

```{r}
acc_mod_1 <- read_rds(here(mod_results, "acc_1.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))


acc_mod_1_me <- read_rds(here(mod_me, "acc_1.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_1_form <- read_rds(here(mod_form, "acc_1.rds"))

acc_mod_2 <- read_rds(here(mod_results, "acc_2.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_2_form <- read_rds(here(mod_form, "acc_2.rds"))

acc_mod_2_me <- read_rds(here(mod_me, "acc_2.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_mod_1_orig_acc <- read_rds(here(mod_results, "acc_1_orig_acc.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_mod_2_orig_acc <- read_rds(here(mod_results, "acc_2_orig_acc.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_mod_1_orig_length <- read_rds(here(mod_results, "acc_1_orig_length.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_mod_2_orig_length <- read_rds(here(mod_results, "acc_2_orig_length.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_mod_mlp <- read_rds(here(mod_results, "acc_mlp_1_2_beta.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_mlp_form <- read_rds(here(mod_form, "acc_mlp_1_2_beta.rds"))
acc_mod_mlp_me <- read_rds(here(mod_me, "acc_mlp_1_2_beta.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_mod_mlp_orig_acc <- read_rds(here(mod_results, "acc_mlp_1_2_orig_acc_beta.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_mod_mlp_orig_length <- read_rds(here(mod_results, "acc_mlp_1_2_orig_length_beta.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
```

## Results

### Experiment 2a
For Experiment 2a, we ran a mixed effects logistic model of naïve matcher accuracy: `r form(acc_1_form)`. Overall, naïve matchers were right more often than not, which was far above the 1/12 = 0.083 expected by random chance (OR = `r stats_text(acc_mod_1, 1)`).
There were not large effects of condition (Figure \ref{expt2-condition} middle panel). Participants tended to be less accurate at descriptions from the last round (OR of last round = `r stats_text(acc_mod_1, 4)`). There was not a clear effect of original group size (OR of 6-player game = `r stats_text(acc_mod_1, 2)`), but there was an interaction between round and group size (OR = `r stats_text(acc_mod_1, 3)`). Later transcripts from larger games were easier to understand, but earlier transcripts from smaller games were easier to understand.

Much of the variation in accuracy was driven not by condition, but by the target image (OR of standard deviation of image distribution = `r stats_text(acc_mod_1_me, 3)`). Some images were much easier to identify as the target than others (Figure \ref{expt2-tangram}). 

### Experiment 2b

For Experiment 2b we ran a similar mixed effects logistic model to consider the effects of group size, thickness, round, and their interactions. <!-- `r form(acc_2_form)` --> Naïve matchers were above chance (OR = `r stats_text(acc_mod_2, 1)`, Figure \ref{expt2-condition} ). Similar to experiment 2a, there were not substantial effects of condition. Last round descriptions had slightly lower accuracy (OR of last round = `r stats_text(acc_mod_2, 6)`), but there was an interaction with thickness, where for thin games, last round descriptions were less opaque (OR  = `r stats_text(acc_mod_2, 8)`). Again some of the uncertainty in estimating the fixed effects was driven by the strong variation based on target image (OR of SD of images = `r stats_text(acc_mod_2_me, 5)`, Figure \ref{expt2-tangram}). 

### Additional predictors

As additional post-hoc predictors, we examined the accuracy of the in-game matchers from @boyce2024 and the length of the description. In both experiments, in-game accuracy was predictive of naïve matcher accuracy (Expt 2a OR = `r stats_text(acc_mod_1_orig_acc, 4)`, Expt 2b OR = `r stats_text(acc_mod_2_orig_acc, 6)`). The log number of words in the description was not predictive in Experiment 2a (OR = `r stats_text(acc_mod_1_orig_length, 4)`), but longer descriptions were slightly beneficial in Experiment 2b (OR = `r stats_text(acc_mod_2_orig_length, 6)`). 

## Model results

As a computational comparison, we looked at the CLIP-MLP model's performance on the same descriptions. We used the probability the model assigned to the correct target as our dependent measure and fit a beta regression on the descriptions from Experiment 2: `r form(acc_mlp_form)`. The CLIP-MLP model was far above chance, but had lower accuracy than the human participants (OR = `r stats_text(acc_mod_mlp, 1)`). 

None of the fixed effects in the model were significant, and there was wide uncertainty for all of them. There was substantial by-target image variation (`r stats_text(acc_mod_mlp_me, 7)`) and substantial by-target variation in the effect of later round (`r stats_text(acc_mod_mlp_me, 8)`). 

As additional predictors, we checked the effect of in-game matcher accuracy and the length of the description. CLIP-MLP had higher accuracy when in-game matcher accuracy was higher (OR = `r stats_text(acc_mod_mlp_orig_acc, 8)`), and the model did better on shorter descriptions (OR for log words = `r stats_text(acc_mod_mlp_orig_length, 8)`). Long descriptions may be more difficult because they are further further from the model's training distribution of image captions. 

## Interim summary

Overall, naïve human matchers were fairly accurate overall, but less accurate than matchers in the original game. Perhaps surprisingly, this level of accuracy was fairly consistent across descriptions from different times in the game and different game conditions. The largest source of variability in accuracy was from the target images; while there was some variabiliity in accuracy by images for the original matchers, there was substantially more variability for naïve matchers. The computational model showed a similar pattern of large effects of image, but had overall lower accuracy than naïve human matchers.

```{r, eval=F}
acc_priors_mlp <- c(
  set_prior("normal(0,.2)", class = "b"),
  set_prior("normal(0,.2)", class = "sd"),
  set_prior("lkj(1)", class = "cor")
)


acc_priors <- c(
  set_prior("normal(0,1)", class = "b"),
  set_prior("normal(0,1)", class = "sd"),
  set_prior("lkj(1)", class = "cor")
)

expt_2_relevant_games <- expt_1_data |>
  bind_rows(expt_2_data) |>
  select(gameId, group_size, thickness, round) |>
  unique()

expt_2_mlp <- mlp_mod |>
  inner_join(expt_2_relevant_games) |>
  left_join(original_length) |>
  left_join(original_acc)

acc_mod_mlp <- brm(
  correct ~ group_size * thickness * round +
    (group_size * thickness * round | correct_tangram),
  data = expt_2_mlp,
  file = here(mod_loc, "acc_mlp_1_2"),
  prior = acc_priors_mlp,
  control = list(adapt_delta = .95)
)

acc_mod_mlp <- brm(
  correct ~ group_size * thickness * round +
    (group_size * thickness * round | correct_tangram),
  data = expt_2_mlp,
  family = Beta(link = "logit"),
  file = here(mod_loc, "acc_mlp_1_2_beta"),
  prior = acc_priors,
  control = list(adapt_delta = .95)
)

acc_mod_mlp_orig_acc <- brm(
  correct ~ original_correct + group_size * thickness * round +
    (group_size * thickness * round | correct_tangram),
  data = expt_2_mlp,
  family = Beta(link = "logit"),
  file = here(mod_loc, "acc_mlp_1_2_orig_acc_beta"),
  prior = acc_priors,
  control = list(adapt_delta = .95)
)

acc_mod_mlp_orig_length <- brm(
  correct ~ log_words + group_size * thickness * round +
    (group_size * thickness * round | correct_tangram),
  data = expt_2_mlp,
  family = Beta(link = "logit"),
  file = here(mod_loc, "acc_mlp_1_2_orig_length_beta"),
  prior = acc_priors,
  control = list(adapt_delta = .95)
)
```


# Experiment 3

The experiment of naïve matchers in Experiment 2 differed from in-game matchers in several ways. In-game matchers recieved descriptions from a consistent group, recieved descriptions in the order they were created, and were present participants during the game. In Experiment 3, we focus on the role of context and group-specific interaction history to tease apart some of these differences.

## Methods

We compared naïve matchers in "yoked" and "shuffled" conditions. In the "yoked" condition, naïve matchers saw all the descriptions from a single game in the order they originally occurred. In the "shuffled" condition, naïve matchers saw all the descriptions from a single game in a randomized order. This is the same yoking but a different shuffling than that used in @hawkins2023a. 

Because some descriptions are already fairly comprehensible in isolation, we focused on the role of context for games that showed strong group-specificity. We hand-picked 10 games from @boyce2024 on the basis of high in-game matcher accuracy, strong patterns of descriptions shortening over repetition, and the use of idiosyncratic or non-modal referring expressions. Thus, these games showed the hallmarks of strong conventionalization to terms that were more likely to be opaque to outsiders. 

We recruited 196 participants (99 in the yoked condition and 97 in shuffled) who each saw all 72 trials of one of the 10 games. This experiment was pre-registered at https://osf.io/zqwp5. Participants read the transcripts in a modified self-paced reading procedure where they uncovered the text word by word (revealed words stayed visible); only after uncovering the entire transcript could participants select an image. We do not analyze the reading time data here.

```{r fig-yoked, fig.env="figure", fig.pos = "t", fig.align = "center", out.width="90%", fig.width=5, fig.height=3, fig.cap = "Accuracies for Experiment 3. Error bars are bootstrapped 95% CIs. TODO not using predictions because those fuzz out round to round differences. \\label{yoked}" }
expt_4_acc_data <- expt_4_data |>
  filter(type == "selection") |>
  mutate(correct = as.numeric(correct)) |>
  select(workerid, correct, orig_repNum, round, condition, matcher_trialNum, gameId, correct_tangram, source)

expt_3_relevant_games <- expt_4_acc_data |>
  select(gameId, round) |>
  unique()
expt_3_fig_data <- expt_4_acc_data |>
  bind_rows(mlp_mod |> inner_join(expt_3_relevant_games)) |>
  bind_rows(original_results |> inner_join(expt_3_relevant_games)) |>
  mutate(condition = ifelse(!is.na(condition), condition, source)) |>
  mutate(round = str_sub(round, -1)) |>
  mutate(condition = factor(condition, levels = c("original", "yoked", "shuffled", "model"), labels = c("Original", "Yoked", "Shuffled", "CLIP-MLP")))


predicted_expt_3_fig <- read_rds(here(mod_loc, "predicted/acc_3.rds")) |>
  bind_rows(read_rds(here(mod_loc, "predicted/acc_3_mlp.rds")) |> mutate(condition = "model")) |>
  mutate(across(mean:high, inv_logit_scaled)) |>
  mutate(round = orig_repNum + 1) |>
  mutate(condition = factor(condition, levels = c("original", "yoked", "shuffled", "model"), labels = c("Original", "Yoked", "Shuffled", "CLIP-MLP")))

predicted_expt_3_fig_no_trial <- read_rds(here(mod_loc, "predicted/acc_3_no_trial.rds")) |>
  bind_rows(read_rds(here(mod_loc, "predicted/acc_3_mlp.rds")) |> mutate(condition = "model")) |>
  mutate(across(mean:high, inv_logit_scaled)) |>
  mutate(round = orig_repNum + 1) |>
  mutate(condition = factor(condition, levels = c("original", "yoked", "shuffled", "model"), labels = c("Original", "Yoked", "Shuffled", "CLIP-MLP")))

expt_3_fig_data |> ggplot(aes(x = round, color = condition, y = correct, group = condition)) +
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .6), geom = "line") +
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .6)) +
  # geom_pointrange(data = predicted_expt_3_fig, aes(y = mean, ymax = high, ymin = low), position=position_dodge(.6)) +
  #geom_line(data = predicted_expt_3_fig, aes(y = mean), position=position_dodge(.6), lty="dashed") +
  #   geom_pointrange(data = predicted_expt_3_fig_no_trial |> filter(condition!="CLIP-MLP"), aes(y = mean, ymax = high, ymin = low), position=position_dodge(.6)) +
 # geom_line(data = predicted_expt_3_fig_no_trial |> filter(condition!="CLIP-MLP"), aes(y = mean), position=position_dodge(.6), lty="dotted") +
  scale_y_continuous(lim = c(0, 1), expand = c(0, 0)) +
  geom_hline(yintercept = 1 / 12, lty = "dashed") +
  labs(x = "Round", y = "Accuracy") +
  scale_color_manual(breaks = c("Original", "Yoked", "Shuffled", "CLIP-MLP"), values = c("#7570B3", "#E7298A", "#1B9E77", "#D95F02")) +
  theme(
    legend.position = "bottom", legend.box = "vertical",
    strip.background = element_blank(),
    legend.title = element_blank(),
    legend.margin = margin(c(-10, 1, 0, 1))
  )
```

```{r, eval=F}
for_acc_mod_4 <- expt_4_data |>
  filter(type == "selection") |>
  mutate(correct = as.numeric(correct)) |>
  select(workerid, correct, orig_repNum, condition, matcher_trialNum, gameId, correct_tangram)

acc_priors <- c(
  set_prior("normal(0, 1)", class = "b"),
  set_prior("normal(0,1)", class = "sd")
)


acc_mod_4 <- brm(correct ~ orig_repNum * condition + matcher_trialNum + (1 | gameId) + (1 | correct_tangram) + (1 | workerid), family = bernoulli(link = "logit"), data = for_acc_mod, prior = acc_priors, file = here(mod_loc, "acc_4.rds"))


acc_mod_4 <- brm(correct ~ orig_repNum * condition + (1 | gameId) + (1 | correct_tangram) + (1 | workerid), family = bernoulli(link = "logit"), data = for_acc_mod_4, prior = acc_priors, file = here(mod_loc, "acc_4_no_trial.rds"))
```


```{r, eval=F}
acc_priors_mlp <- c(
  set_prior("normal(0,1)", class = "b"),
  set_prior("normal(0,1)", class = "sd")
)

expt_3_relevant_games <- expt_4_acc_data |>
  select(gameId, round, orig_repNum) |>
  unique()

yoked_relevant_mlp <- mlp_mod |> inner_join(expt_3_relevant_games)


acc_mod_yoked_mlp <- brm(correct ~ orig_repNum + (1 | gameId) + (1 | correct_tangram),
  data = yoked_relevant_mlp,
  prior = acc_priors_mlp,
  family = Beta(link = "logit"),
  file = here(mod_loc, "acc_yoked_mlp_beta"),
  control = list(adapt_delta = .95)
)
```

```{r, eval=F}
original_subset <- original_results_raw |>
  inner_join(expt_3_relevant_games) |>
  mutate(order = "yoked", setting = "original") |>
  select(workerid = playerId, correct, correct_tangram = tangram, orig_repNum = repNum, gameId, matcher_trialNum = trialNum, order, setting)

yoked_shuffled_original <- expt_4_data |>
  filter(type == "selection") |>
  mutate(correct = as.numeric(correct)) |>
  select(workerid, correct, orig_repNum, condition, matcher_trialNum, gameId, correct_tangram) |>
  mutate(order = condition, setting = "new") |>
  bind_rows(original_subset)

acc_priors <- c(
  set_prior("normal(0, 1)", class = "b"),
  set_prior("normal(0,1)", class = "sd")
)


acc_mod_compare_orig <- brm(correct ~ orig_repNum * order + orig_repNum * setting + matcher_trialNum + (1 | gameId) + (1 | correct_tangram) + (1 | workerid), family = bernoulli(link = "logit"), data = yoked_shuffled_original, prior = acc_priors, file = here(mod_loc, "yoked_shuffled_original.rds"))
```

```{r}
acc_mod_4 <- read_rds(here(mod_results, "acc_4.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_mod_4_me <- read_rds(here(mod_me, "acc_4.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_4_form <- read_rds(here(mod_form, "acc_4.rds"))

acc_mlp_4 <- read_rds(here(mod_results, "acc_yoked_mlp_beta.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_mlp_4_me <- read_rds(here(mod_me, "acc_yoked_mlp_beta.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_4_mlp_form <- read_rds(here(mod_form, "acc_yoked_mlp_beta.rds"))


acc_compare <- read_rds(here(mod_results, "yoked_shuffled_original.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_compare_me <- read_rds(here(mod_me, "yoked_shuffled_original.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_4_form_compare <- read_rds(here(mod_form, "yoked_shuffled_original.rds"))
```

## Results 

Our primary question of interest was how much having the conversation history would help make later round descriptions more understandable to participants in the yoked condition. 

We compared accuracy across the yoked and shuffled conditions with a logistic regression: `r form(acc_4_form)`. The descriptions were more transparent when they were presented in a yoked order (OR = `r stats_text(acc_mod_4, 2)`, Figure \ref{yoked}). In the shuffled condition, there was no main effect of round number (OR for one round later = `r stats_text(acc_mod_4, 4)`), but there was a marginal interaction where the benefit of the yoked condition decreased for later rounds (OR for one round later = `r stats_text(acc_mod_4, 5)`). This was offset by matchers in both conditions improving at the task over time (OR for one trial later in matcher viewing order = `r stats_text(acc_mod_4, 3)`). In the yoked condition round and trial number were aligned, so an improvement over time could be either from matcher practice or from descriptions being easier to understand. In the shuffled condition, matcher practice effects did not correlate with position in the original game. 

Comparing to the performance of in-game matchers, we can separate out the benefits of seeing the descriptions in order versus being a participant in the group: `r form(acc_4_form_compare)`. There is a benefit to seeing the items in order (OR = `r stats_text(acc_compare, 3)`) and a larger benefit to being a participant during the game (OR = `r stats_text(acc_compare, 7)`). The benefit of seeing the items in order wanes in later blocks (OR = `r stats_text(acc_compare, 5)`), but the benefit of being in the game does not (OR = `r stats_text(acc_compare, 6)`). In all cases, there is a baseline improvement over trials (OR = `r stats_text(acc_compare, 2)`). 

The accuracy of the CLIP-MLP model is worse than the shuffled human results, and does not show change across rounds (OR for one round later = `r stats_text(acc_mlp_4, 2)`). The larger difference between naïve human and CLIP-MLP accuracies in Experiment 3 than Experiment 2 could suggest that even the shuffled ordering still provides useful context (e.g., the consistent set of images) that helps matchers understand the conventions. This history is not available to the CLIP-MLP model which sees every description as a one-shot task. 

<!-- QUERY: we could run the model without matcher_trial_num which flips the direction of interactions because it centers later, and also you get to see the benefit of (matcher) experience purely in terms of the repNum. I think it's better the way it is, but raising it. -->

<!-- NOTE: not showing model estimates in Figure 5 b/c the fit isn't great, not sure why... --> 

# Discussion 

Conventions are formed in reference games in a partner-specific way, where different groups follow different paths through semantic space as they form conventions. However, the referential descriptions used over the course of convention formation remain relatively understandable to outsiders. Across multiple experiments with human matchers, we found that naïve human matchers were far above chance accuracy at identifying the targets, with variation explained more by the target image than the round or game condition the descriptions came from. Even for games selected for strong conventionalization, naïve matchers had high accuracy that was aided if they saw the descriptions in order. 

We also tested a computational model built on CLIP with a multi-layer perceptron readout as a way of approximating the context-independent semantic distance between descriptions and images. The CLIP-MLP model was far above chance in its assignment of probabilities to target images, and although its probabilities were lower than human matcher accuracies, the model mirrored the qualitative pattern of larger effects from target image than from description source. 

Our experimental and computational results were only on a specific set of iterated reference game transcripts targeting a specific set of 12 images and so numeric results may not generalize to other reference games with different array sizes and sets of images. There are potential non-measured differences between in-game matchers and naïve matchers because naïve matchers did not have to wait for descriptions to appear and thus had more choice about how closely to read descriptions or how long to spend looking at images. We also cannot know whether naïve matchers (or for that matter, in-game matchers) fully "comprehended" the language in the descriptions, or were making inferences about the meaning on a more analytic level. 

This work suggests that conventions formed within a small group may still be fairly comprehensible to those outside the group, who may produce different descriptions themselves. This finding raises questions around how well-calibrated describers are to their matcher's level of knowledge, and whether the process of convention formation is actually efficient. Even naïve matchers can often understand the shorthand descriptions, but in reference games, describers choose elaborated descriptions with new matchers. In a game, norms of cooperation and conversation may lead describers to start new matchers with elaborated descriptions designed to give them a high level of confidence in target selection. Describers are also constrained by their need to come up with a description in real time. However, the high level of understanding and the lack of substantial benefit from early round descriptions does raise empirical questions about how calibrated describers are to the level of information necessary. 

We found large variation in how accurate the computation model and naïve matchers were on different target images. This effect raises questions about what makes some images much easier to identify. They might be more iconic, with a narrower prior over different ways they could be conceptualized, or they may be further from competitors within this pool of images. We are limited by the 12 images we used, but future work sampling across larger sets of images [such as @ji2022] could probe image-level factors. Future work could also explore within-description sources of variation and how the structure and word choice of utterance correlates with naïve matcher accuracy. Computational models could be especially beneficial because they could be run on subsets or ablations of descriptive text. 

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
