---
title: "Idiosyncratic but not opaque: Conventions formed in reference games are interpretable by naïve humans and vision–language models"
bibliography: tg-matcher-cogsci.bib
csl: apa7.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Veronica Boyce (vboyce@stanford.edu)} \\ Department of Psychology \\ Stanford University \And {\large \bf Ben Prystawski (benpry@stanford.edu)} \\Department of Psychology \\ Stanford University
    \AND {\large \bf Alvin Wei Ming Tan (tanawm@stanford.edu)} \\ Department of Psychology \\ Stanford University \And {\large \bf Michael C. Frank (mcfrank@stanford.edu)} \\ Department of Psychology \\ Stanford University}

abstract: >
    In-group linguistic conventions vary in whether they are opaque to outsiders (teen slang like "rizz") or understandable to outsiders (regionalisms like "roundabout"). The formation of temporary linguistic conventions between individuals is often studied in iterated reference games, where over repeated reference to the same targets, a describer--matcher pair establishes partner-specific shorthand names for targets. One open question is how understandable these referring expressions are to others who were not part of the convention formation process. We use computational models and experiments with naïve matchers to assess the opacity of descriptions from iterated reference games. Both human matchers and the computational model are well above chance accuracy, suggesting that the conventions substantially reflect aspects of shared semantic associations. This additional perspective provides insights into how convention formation relates to models of lexical uncertainty and efficiency. [TODO THIS LAST SENTENCE STILL SUCKS, please someone fix; 137/150 words atm] 
    
keywords: >
    reference games; convention formation; computational modeling; opacity; pragmatics
    
output: cogsci2024::cogsci_paper
header-includes:
  - \usepackage{booktabs}
#- \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(
  fig.width = 3, fig.height = 3, fig.crop = F,
  fig.pos = "tb", fig.path = "figs/",
  echo = F, warning = F, cache = F,
  message = F, sanitize = T
)

library(png)
library(grid)
library(ggplot2)
library(xtable)
library(tidyverse)
library(here)
library(brms)
library(rstan)
library(rstanarm)
library(ggthemes)
library(jsonlite)
library(ggthemes)
library(scales)
library(viridis)
library(ggridges)
library(cowplot)
library(tidybayes)

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
theme_set(theme_bw())

data_loc <- "data"
mod_loc <- "analysis-code/models"
images <- "experiments/expt1/assets/images"
prediction_loc <- "model-code/model_predictions"
mod_results <- "analysis-code/models/summary"
mod_form <- "analysis-code/models/formulae"
mod_me <- "analysis-code/models/mixed_fx"

source(here("analysis-code/helper_cogsci.R"))
```




TODO SLIM DOWN INTRO -- target 1.5 pages

TODO should we just drop the efficiency angle? (for space reasons?)

# Introduction
"He's got rizz." The idea that teen slang is arbitrary and opaque to outsiders (i.e. older generations) is enough of a cultural touchstone that late night comedy shows have segments about it. Teens are far from the only groups that form naming conventions that are specific to the in-group. Many groups form temporally stable conventions shared by communities, including professional jargon, regionalisms, and of course slang. In-group language can be temporally stable conventions shared by sizable communities, but temporary naming conventions can arise in smaller groups to refer to a particular entity that doesn’t have a label or even to refer back to group in-jokes. 

This formation of temporary linguistic conventions between individuals is often studied in iterated reference games. In these games, a describer tells their partner how to sort or match a series of abstract images [e.g., @clark1986; @hawkins2020b]. Over repeated rounds of referring to the same targets, pairs usually develop conventionalized nicknames for the target images. These nicknames are often partner-specific, in that different pairs develop different nicknames for the same targets. When describing the shapes for people who were not part of the group, people return to more elaborated descriptions, indicating an expectation that others may be unable to understand the convention, and need a longer or different referential description [@yoon2018; @wilkes-gibbs1992; @hawkins2021]. 

In one-shot interactions, the choice of how to map a referring expression to a target can be modeled using Bayesian pragmatics models such as Rational Speech Acts model [cite] that assume speakers and listeners reasoning about each other. [? cite examples of where this has been used in oneshot ref games] Building on these models for one-shot interactions, the Continual Hierarchical Adaptation through Inference model (CHAI) introduces partner-specific update rules to allow for updates to the lexicon after each interaction, which can account for the shortening of referring expressions observed in games [cite]. Initially, speakers have uncertainty about the listeners word-meaning mappings (lexicon) and so may use multiple words to triangulate a meaning, but when that succeeds, the updating then leads the agent to be able to successfully use individual words for the same meaning as the word-meaning association has been strengthened. 

In models like CHAI, both speaker and listener agents are jointly reasoning about each other, but the same types of models can be used when the interaction is one-sided. How can listeners understand speakers when they don’t know the semantic lexicon of the speaker? Following Bayesian approaches to pragmatics and communication, one approach is to model listeners as Bayesian agents who jointly infer the meaning of a speaker’s utterance and their model of the speaker’s lexicon.  This Bayesian joint updating has been used to model word learning, but also to model person-to-person or group-to-group differences in lexicon [citations TODO]. Notably, lexical uncertainty models can account for times when a listener infers non-shared aspects of the speaker’s lexicon. Often this doesn’t mean learning totally arbitrary meaning to word mappings for each possible interlocutor, but more like learning a hierarchical set of modifications to a lexicon where some words have slightly different extensions for different speakers and different circumstances [@schuster2020; @hawkins2021a]. However, in some experimental settings, people can even learn contronymic mappings, where the same symbol has context-dependent opposite meanings [@misyak2016].

When groups of people form conventions, are they generally opaque to outsiders (like "rizz") or are they generally inferable using some lexical uncertainty (like "roundabout")? [is "rotary" more obscure? maybe switch?] Iterated reference games provide a controlled test environment for studying convention formation where the pressures driving convention formation are purely communicative. Thus, they provide a test case for studying to what extent opacity is an inherent consequence of convention formation, or whether it is a separate sociolinguistic effect for marking in-groupness. 
Conventions in iterated reference games are formed between people without a (known) shared group identity prior to the interaction, but, as seen in partner-switch experiments, people do not use the conventions with new partners. Do temporary conventions formed out of communicative need reflect overall semantic properties, or are they arbitrary in a way that requires one to “have been there”? One way to operationalize whether term -- meaning alignments are random or reflective of small deviations from the shared lexicon is to look at the semantic distance between the signifier and the referent in semantic space. Expressions that are more transparent are those where signifiers and referents that are semantically close, such that any member of the language  community sharing the global semantics should be able to identify the appropriate referent given the signifier. In contrast, expressions that are opaque have signifiers and referents that are semantically distant, such that the relations between them are more arbitrary and inaccessible to the general community without the formation of additional conventions (which may be partner- or group-specific). 

How can we measure the semantic distances between a conventionalized referring expression and its target referent? One computational option is to use vision-language models as a way to operationalize a shared semantic space for both language and target image. Computational methods have enabled the embedding of various stimuli (including images and text) into high-dimensional feature spaces; these embeddings have properties which suggest that they are reasonable approximations of humans' semantic spaces, including similarity in representational geometries [e.g., @grand2022; @muttenthaler2021]. Indeed, embeddings from neural network models have been used as a form of semantics in a range of reference game scenarios [e.g., @gul2024; @ji2022; @kang2020; @le2022; @ohmer2022]. In particular, such embeddings can be treated as the default context- and speaker-independent lexicon, since they are not updated to account for convention formation within an iterated reference game. 

An alternative way of measuring the transparency of conventionalized referring expressions is to directly measure how often naive humans, who were not part of the group who formed the convention, can correctly select the target referent given the referring expression. Prior work with naive matchers has focused on the role of conversational shared history as an aid to understanding later, more conventionalized descriptions. Overall, naive matchers tend to do better the more their observation history resembles that of the original conversation. Naive listeners in @murfitt2001 did better when they heard descriptions in order instead of in reverse order. @schober1989 found that matchers in an iterated reference game achieved higher accuracies than overhearers who listened to the entire game in order, and overhearers who listened to recordings starting in the third round did even worse. In an iterated reference game using drawing as the communication modality, @hawkins2023a found that yoked matchers who saw all the trials from one game in order outperformed shuffled matchers who saw trials sampled from 10 games but in trial order. 

Across the prior literature, while naïve matchers have worse accuracy than in-game matchers, their performance was still far above chance, suggesting that the convention--target relationship is not purely arbitrary. In fact, even when pairs of participants try to obfuscate their meaning to match images with each other but not an overhearer, an overhearing participant can still do quite well [@clark1987a]. Nonetheless, receiving more context from an interaction---and in particular, having that context be in order---is beneficial to matchers. Except for the shuffled condition of @hawkins2023a, these studies do not address how opaque descriptions from different points in the game are without prior context. Such an approach would be important to have truly naïve matchers that lack even the context of trial order, providing a clearer understanding of when these expressions are opaque and when they are merely idiosyncratic.

In the current work, we address how the process of convention formation shapes the levels of opacity of the referring expressions created at different time points in an iterated reference game. We draw our reference game expressions from @boyce2024, which has a large corpus of iterated reference games played by groups of different sizes, all playing online and communicating via a chat box. This corpus is made up of 6-round iterated reference games using the same 12 target images. Games were played in conditions varying in how large the describer--matcher groups were (2--6 participants) and how "thick" the communication channels were.  The varied conditions within a consistent framework allow us to test how opacity of referring expressions vary depending on the conditions they were produced under. 

Using reference expressions created in different games from @boyce2024, we use both human experiments and models to assess when and why expressions are opaque or understandable to outside observers. We first present a computational approach using a  vision-language model with a read-out head to determine the semantic similarities between referring expressions and their targets and validate against naive human matchers (Experiment 1). We compare the opacity referring expressions across different game conditions and time points using both naive human matchers and the model (Experiment 2). Finally, we address the role of conversation history by comparing human matcher performance on shuffled versus ordered game transcripts (Experiment 3). 

```{r interface, fig.env = "figure", fig.pos = "t!", out.width="100%", fig.align = "center", fig.cap = "Experimental Setup. Naive matchers read transcripts from trials in reference games from Boyce et al. (2024) and selected which image they thought was being described. Matchers recieved bonus payments for correct selections. \\label{game}", cache=FALSE}
knitr::include_graphics("matcher-diagram.pdf", error = FALSE)
```

# Task setup

## Materials
We drew our referential expressions from @boyce2024. For our naïve matcher experiments, we sampled different subsets of this corpus. In presenting the transcripts, we excluded utterances that were marked by @boyce2024 as not containing any referential content. Within the samples, we also did not show transcripts that contained swear words or crude or sexual language. We used the entire corpus for our computational modeling component. 

## Experimental procedure 

We recruited English-speaking participants from Prolific. Participants were directed to the experiment, where the task was explained to them. On each trial, participants saw the full transcript from that trial, containing all the chat messages marked by whether they were from the speaker or a listener. Participants selected the image they thought was the target from the tableau of 12 (Figure \ref{game}). Participants received feedback on whether they were right or wrong on each trial. Except when the specific viewing order was part of the experimental manipulation, we randomized the order of trials, subject to the constraint that the same target could not repeat on adjacent trials. The task was implemented in jsPsych [@leeuw2023]. We paid participants $10 an hour plus a bonus of 5 cents per correct response. All our experimental code is at TODO OSF LINK GOES HERE. 

## Computational models

We used CLIP (`clip-vit-large-patch14`) as a listener model for our domain [@radford2021] TODO BEN say model architecture/type. CLIP is a natural choice for reference games, as the model is trained to estimate the correspondence between images and phrases in natural language. We pre-processed the text by concatenating all the messages sent by the speaker for a given trial, and ran CLIP for the concatenated text and all 12 tangram shapes. We then computed probabilities for each tangram shape using CLIP's logits. The simplest way to do this is simply taking the softmax of the logits. However, the tangram shapes were possibly out the distribution for the model, which led it to favor some images over others regardless of the content of the text. To fix this, we trained readout models that made more calibrated predictions using CLIP's logits.

```{r, eval=F}
df_classifiers <- read_csv(here("model-code/ classifier_comparison-openai--clip-vit-large-patch14.csv"))
df_clf_comp <- df_classifiers |>
  group_by(classifier, params) |>
  summarize(mean_acc = mean(accuracy), sd_acc = sd(accuracy))

df_best_clfs <- df_clf_comp |>
  group_by(classifier) |>
  mutate(
    mean_acc = round(mean_acc, 2),
    sd_acc = round(sd_acc, 2)
  ) |>
  arrange(desc(mean_acc)) |>
  ungroup()

xtable(df_best_clfs)

df_clf_comp |> arrange(desc(mean_acc))

```

```{=latex}
\begin{table}
\caption{Cross-validated accuracies for classifiers. Standard deviations in accuracy across the 10 folds are shown in parentheses. Best performance within each model class is underlined, and best overall performance is bolded.}
\label{tab:classifier_comparison}
\centering

  \begin{tabular}{p{1em}lr}
    \toprule
    \multicolumn{2}{l}{Classifier} & Accuracy \\ 
    \midrule
        \multicolumn{2}{l}{Random baseline} & \smash{0.08} \\
    \multicolumn{2}{l}{CLIP without readout} & \smash{0.31} \\
    \multicolumn{2}{l}{Logistic regression} & \\
    & No penalty & \underline{\smash{0.50 (0.01)}} \\ 
    & \vspace{1mm}L2 penalty & 0.50  (0.01) \\ 
    \multicolumn{2}{l}{Random forest} & \\
    & 10 estimators & 0.46 (0.02) \\
    & 50 estimators  & 0.51 (0.02)\\ 
    & 100 estimators & 0.52 (0.02) \\ 
    & \vspace{1mm}500 estimators & \underline{\smash{0.52 (0.02)}} \\ 
    \multicolumn{2}{l}{Gradient-boosted tree} & \\
    & 10 estimators & 0.48 (0.02) \\ 
    & \vspace{1mm}100 estimators & \underline{\smash{0.51 (0.02)}} \\ 
    \multicolumn{2}{l}{Multi-layer perceptron} & \\
    & 1 $\times$ 32-dim hidden layer & 0.50 (0.01) \\ 
    & 1 $\times$ 100-dim hidden layer  & 0.52 (0.01) \\ 
    & 1 $\times$ 512-dim hidden layer & 0.53 (0.02) \\ 
    & 1 $\times$ 1028-dim hidden layer & 0.53 (0.02) \\ 
    & 2 $\times$ 32-dim hidden layers  & 0.51 (0.02) \\ 
    & 2 $\times$ 100-dim hidden layers & \underline{\smash{\textbf{0.55 (0.02)}}} \\ 
    \bottomrule
	\end{tabular}
\end{table}
```

We trained different readout models to assign probabilities to features using CLIP's logits as features. Models were trained to maximize task performance (i.e., to assign high probability to the target tangram given the concatenated speaker utterance). We compared four types of models: a random forest, a logistic regression model, a multi-layer perceptron (MLP), and a gradient-boosted tree. Classifiers were implemented in the `scikit-learn` and `XGBoost` libraries [@pedregosa2011scikit; @chen2016xgboost]. Table \ref{tab:classifier_comparison} shows the cross-validated accuracy of different readout models, as well as the performance of CLIP with no readout. The MLP with two hidden layers of size 100 performed the best, so we use its predictions in subsequent analysis. TODO BEN clarify when things are held out v not!

# Experiment 1
```{r}
expt_3_cor <- cor.test(expt_3_summary$value, expt_3_summary$human_acc)
```

```{r fig-calibration, fig.env="figure", fig.pos = "t", fig.align = "center", out.width="70%", fig.width=3, fig.height=3, fig.cap = "Correlation between human accuracy and CLIP-MLP probability of target in Experiment 1.  Points are individual descriptions, colored by decile of CLIP-MLP probability, black line is the bootstrapped mean and 95% CI across descriptions for each decile. \\label{calibration}" }
expt_3_summary |> ggplot(aes(y = human_acc, x = value)) +
  geom_point(aes(color = grouping), alpha = .7) +
  # geom_line(data = summ, color = "black") +
  stat_summary(data = expt_3_groups) +
  stat_summary(data = expt_3_groups, fun.data = "mean_cl_boot", geom = "line") +
  theme(legend.position = "none") +
  scale_color_viridis(discrete = T) +
  labs(x = "CLIP-MLP probability", y = "Human accuracy") +
  coord_equal()
```

Our CLIP-MLP computational model was optimized for task accuracy.
To validate whether this objective also results in human-like response patterns, we conducted a calibration experiment to determine if, for any given utterance, the model-assigned target probability was aligned with the probability that a naïve human matcher would choose the target image.
<!-- We hypothesized that there would be a significant correlation between the target choice probability of the model and the target choice probability of naïve human matchers. -->

## Methods

We first obtained target probabilities from our CLIP-MLP model for all utterances from @boyce2024. We then used stratified sampling to select 217 trials by dividing model-predicted probabilities into deciles and choosing approximately 22 utterances per decile, spanning the 12 different possible target images.
We recruited `r expt_3_data |> select(workerid) |> n_distinct()` participants who each saw 64 trials randomly sampled from the 217 tested trials. On average, each trial was seen by `r expt_3_summary |> pull(human_n) |> mean() |> round()` participants. This experiment was pre-registered at https://osf.io/6pv5e.

## Results and discussion

We obtained human accuracies on each trial by dividing the number of participants who selected the target by the total number of participants who saw the trial (Figure \ref{fig:fig-calibration}). There was a modest but significant positive correlation between model-predicted probabilities and human accuracies ($r$ = `r round(expt_3_cor$estimate, digits = 2)` [`r round(expt_3_cor$conf.int[1], digits = 2)`, `r round(expt_3_cor$conf.int[2], digits = 2)`]). This result suggests that model predictions were calibrated to human response patterns, albeit not perfectly. It is possible to use these calibration results to tune model predictions to better approximate human responses; we leave this approach for future work. Nonetheless, the observed positive correlation suggests that our computational model carries some signal about human accuracies, validating its use in subsequent experiments as a computational comparison.



```{r}
expt_2_relevant_games <- expt_1_data |>
  bind_rows(expt_2_data) |>
  select(gameId, group_size, thickness, round) |>
  unique()

expt_2_fig_data <- expt_1_data |>
  bind_rows(expt_2_data) |>
  bind_rows(mlp_mod |> inner_join(expt_2_relevant_games)) |>
  bind_rows(original_results |> inner_join(expt_2_relevant_games)) |>
  mutate(thickness = factor(thickness, levels = c("thin", "medium", "thick"))) |>
  mutate(source = factor(source, levels = c("original", "naïve", "model"), labels = c("Original matchers", "Naïve matchers", "CLIP-MLP"))) |>
  mutate(round = str_sub(round, -1))


predicted_expt_2_fig <- read_rds(here(mod_loc, "predicted/acc_2_cond.rds")) |>
  mutate(source = "naïve") |>
  bind_rows(read_rds(here(mod_loc, "predicted/acc_2_mlp_cond.rds")) |> mutate(source = "model")) |>
  mutate(source = factor(source, levels = c("original", "naïve", "model"), labels = c("Original matchers", "Naïve matchers", "CLIP-MLP"))) |>
  mutate(across(mean:high, inv_logit_scaled)) |>
  mutate(thickness = factor(thickness, levels = c("thin", "medium", "thick"))) |>
  mutate(round = str_sub(round, -1))


tangram_predicted_expt_2_fig <- read_rds(here(mod_loc, "predicted/acc_2_tangram.rds")) |>
  mutate(source = "naïve") |>
  bind_rows(read_rds(here(mod_loc, "predicted/acc_2_mlp_tangram.rds")) |> mutate(source = "model")) |>
  mutate(source = factor(source, levels = c("original", "naïve", "model"), labels = c("Original matchers", "Naïve matchers", "CLIP-MLP"))) |>
  mutate(across(mean:high, inv_logit_scaled))
```

```{r fig-condition, fig.env="figure", fig.pos = "t", fig.align = "center", out.width="90%", fig.width=4, fig.height=3, fig.cap = "Accuracies for naïve human matchers and the CLIP-MLP model for Experiments 2a and 2b, grouped by the source of the referential description. Facets are the communication thickness of the original game and x-axis is when in the game the transcript caome form. Point estimates and 95% CrI are predictions from the fixed effects of logistic and beta regressions. Bootstrapped mean accuracy from the original matchers is included as a ceiling, and random chance as a baseline. \\label{expt2-condition}" }
expt_2_fig_data |> ggplot(aes(x = round, color = source, y = correct, group = interaction(source, group_size), shape = group_size, lty = group_size)) +
  stat_summary(data = expt_2_fig_data |> filter(source == "Original matchers"), geom = "point", fun.data = "mean_cl_boot", position = position_dodge(.4)) +
  stat_summary(data = expt_2_fig_data |> filter(source == "Original matchers"), geom = "line", fun.data = "mean_cl_boot", position = position_dodge(.4)) +
#stat_summary(data = expt_2_fig_data , geom = "point", fun.data = "mean_cl_boot", position = position_dodge(.4)) +
 # stat_summary(data = expt_2_fig_data , geom = "line", fun.data = "mean_cl_boot", position = position_dodge(.4)) +
  geom_pointrange(data = predicted_expt_2_fig, aes(y = mean, ymax = high, ymin = low), position = position_dodge(.4)) +
  geom_line(data = predicted_expt_2_fig, aes(y = mean), position = position_dodge(.4)) +
  facet_grid(. ~ thickness) +
  scale_y_continuous(lim = c(0, 1), expand = c(0, 0)) +
  geom_hline(yintercept = 1 / 12, lty = "dashed") +
  labs(x = "Round in original game", y = "Accuracy") +
  scale_color_manual(breaks = c("Original matchers", "Naïve matchers", "CLIP-MLP"), values = c("#7570B3", "#1B9E77", "#D95F02")) +
  theme(
    legend.position = "bottom", legend.box = "vertical",
    strip.background = element_blank(),
    legend.title = element_blank(),
    legend.margin = margin(c(-10, 0, 0, -25))
  )
```


```{r fig-2, fig.env="figure", fig.pos = "t", fig.align = "center", out.width="90%", fig.width=4, fig.height=3, fig.cap = "Accuracies for naïve human matchers and the CLIP-MLP model for Experiments 2a and 2b, split out by target image. Point estimates and 95% CI are predictions from the fixed effects and by-tangram random effects of logistic and beta regressions, bootstrapped across conditions. Bootstrapped mean accuracy from the original matchers is included as a ceiling, and random chance as a baseline. \\label{expt2-tangram}" }
library(ggtext)
correct_tangram <- c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L")
labels <- c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L") |> map(~ str_c("<img src=", here(images, str_c("tangram_", ., ".png")), " width='20'/>"))


acc_by_target <- expt_1_data |>
  bind_rows(expt_2_data) |>
  group_by(correct_tangram) |>
  summarize(acc = sum(correct) / n()) |>
  arrange(acc)

foo <- tibble(correct_tangram, labels) |>
  left_join(acc_by_target) |>
  arrange(acc)

acc_by_type_target <- expt_2_fig_data |>
  group_by(correct_tangram, group_size, round, thickness, source) |>
  summarize(acc = sum(correct) / n()) |>
  mutate(correct_tangram = factor(correct_tangram, levels = acc_by_target$correct_tangram))

ggplot(acc_by_type_target, aes(x = correct_tangram, y = acc, color = source, shape = group_size, lty = group_size)) +
  geom_pointrange(data = tangram_predicted_expt_2_fig, aes(y = mean, ymax = high, ymin = low), position = position_dodge(.4)) +
  #  stat_summary(data = acc_by_type_target, aes(color = source), geom = "point", position = position_dodge(width = .6)) +

  stat_summary(data = acc_by_type_target |> filter(source == "Original matchers"), aes(color = source), geom = "point", position = position_dodge(width = .6)) +
  coord_cartesian(ylim = c(0, 1), xlim = c(.5, 12.5), expand = F) +
  geom_hline(yintercept = 1 / 12, linetype = "dashed") +
  scale_x_discrete(name = NULL, labels = foo$labels) +
  scale_color_manual(breaks = c("Original matchers", "Naïve matchers", "CLIP-MLP"), values = c("#7570B3", "#1B9E77", "#D95F02")) +
  labs(y = "Accuracy") +
  theme(
    legend.position = "bottom", legend.box = "vertical",
    strip.background = element_blank(),
    legend.title = element_blank(),
    axis.text.x = element_markdown(color = "black", size = 11),
    legend.margin = margin(c(-10, 0, 0, -25))
  )


# guide_legend()# acc_by_target
```

# Experiment 2

As a starting point for examining what makes referential expressions more or less opaque, we focused on referring expressions from the first and last rounds of games. Principles of convention formation and people's behavior when switching to a new partner suggest that later-round utterances are more opaque and thus harder to understand. One counterargument is that later rounds are the result of describers' accumulated practice refining descriptions to be maximally communicative and to pick out the most visually salient features. To distinguish these hypotheses, we ran a recognition experiment including descriptions from games of different sizes and communication thicknesses. Based on the patterns of cross-game similarity in @boyce2024, we expected that smaller and thicker games, whose descriptions diverged fastest, would have more idiosyncratic and opaque conventions than larger groups with thinner communication channels. 

## Methods

### Experiment 2a
To establish a baseline of how well naïve matchers could understand descriptions without context, we ran a 2x2 within subjects experiment. We drew the target transcripts from 2- and 6-player games from Experiment 1 of @boyce2024 and from the first and last blocks of these games. These games had medium-thick communication channels, where matchers could send text messages to the shared chat interface, but the describer role rotated each round, and matchers received limited feedback. We recruited `r expt_1_data |> select(workerid) |> n_distinct()` participants who each saw 60 trials (15 in each of the 4 conditions). Overall, participants saw `r expt_1_data |> select(gameId, round, correct_tangram) |> n_distinct()` transcripts from `r expt_1_data |> select(gameId) |> n_distinct()` games. This experiment was pre-registered at https://osf.io/k45dr. 

### Experiment 2b
After observing limited condition differences in Experiment 2a, we ran a follow-up experiment on descriptions from Experiment 3 of @boyce2024, where the communication channel thicknesses were more extreme. Here, we used a 2x2x2 within subjects design, drawing our transcripts from the first and last rounds of thick and thin, 2- and 6- person games. In the  "thick" condition, matchers could send text messages to the shared chat interface, one person was the the describer role the whole game, and matchers recived feedback on everyone's selections. In contrast, in the "thin" condition,  original matchers could only contribute to the chat by sending one of 4 emojis, the describer role rotated, and matchers recieved limited feedback. As the emojis did not have referential content, we did not include them in the transcripts shown to naïve matchers. 
For experiment 2b, we recruited `r expt_2_data |> select(workerid) |> n_distinct()` participants who each saw 64 trials (8 in each of the 8 conditions). Overall, participants saw `r expt_2_data |> select(gameId, round, correct_tangram) |> n_distinct()` transcripts from `r expt_2_data |> select(gameId) |> n_distinct()` games.  This experiment was pre-registered at  https://osf.io/rdp5k. 


```{r}
acc_mod_1 <- read_rds(here(mod_results, "acc_1.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))


acc_1_form <- read_rds(here(mod_form, "acc_1.rds"))

acc_mod_2 <- read_rds(here(mod_results, "acc_2.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_2_form <- read_rds(here(mod_form, "acc_2.rds"))


acc_mod_1_orig_acc <- read_rds(here(mod_results, "acc_1_orig_acc.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_mod_2_orig_acc <- read_rds(here(mod_results, "acc_2_orig_acc.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_mod_1_orig_length <- read_rds(here(mod_results, "acc_1_orig_length.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_mod_2_orig_length <- read_rds(here(mod_results, "acc_2_orig_length.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_mod_mlp <- read_rds(here(mod_results, "acc_mlp_1_2_beta.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_mlp_form <- read_rds(here(mod_form, "acc_mlp_1_2_beta.rds"))

acc_mod_mlp_orig_acc <- read_rds(here(mod_results, "acc_mlp_1_2_orig_acc_beta.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_mod_mlp_orig_length <- read_rds(here(mod_results, "acc_mlp_1_2_orig_length_beta.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

#expt_1_data |> summarize(m=mean(correct)) |> mutate(m=round(m*100))

acc_mod_1_me <- read_rds(here(mod_me, "acc_1.rds")) #|> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_mod_2_me <- read_rds(here(mod_me, "acc_2.rds")) #|> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_mod_mlp_me <- read_rds(here(mod_me, "acc_mlp_1_2_beta.rds")) #|> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))


acc_mod_1_sbert <- read_rds(here(mod_results, "acc_1_sbert.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_mod_2_sbert <- read_rds(here(mod_results, "acc_2_sbert.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_mod_mlp_sbert <- read_rds(here(mod_results, "acc_mlp_1_2_sbert_beta.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))


```

## Results

### Experiment 2a
For Experiment 2a, we ran a Bayesian mixed effects logistic model of naïve matcher accuracy in brms [@burkner2018].^[`r form(acc_1_form)`] Overall, naïve matchers were right 62% of the time, which was far above the 1/12 = 8.3% expected by random chance (OR = `r stats_text(acc_mod_1, 1)`).
There were not large effects of condition (Figure \ref{expt2-condition} middle panel). Participants tended to be less accurate at descriptions from the last round (OR of last round = `r stats_text(acc_mod_1, 4)`). There was not a clear effect of original group size (OR of 6-player game = `r stats_text(acc_mod_1, 2)`), but there was an interaction between round and group size (OR = `r stats_text(acc_mod_1, 3)`). Later transcripts from larger games were easier to understand, but earlier transcripts from smaller games were easier to understand.
Much of the variation in accuracy was driven by the target image, which accounted for more variation than participant differences (standard deviation of image distribution = `r stats_text(acc_mod_1_me, 3)`; SD of participant distribution = `r stats_text(acc_mod_1_me, 7)`). Some images were much easier to identify as the target than others (Figure \ref{expt2-tangram}). 

### Experiment 2b

For Experiment 2b we ran a similar Bayesian mixed effects logistic model.^[`r form(acc_2_form)`] Naïve matchers were above chance (OR = `r stats_text(acc_mod_2, 1)`, Figure \ref{expt2-condition} ). Similar to experiment 2a, there were not substantial effects of condition. Last round descriptions had slightly lower accuracy (OR of last round = `r stats_text(acc_mod_2, 6)`), but there was an interaction with thickness, where for thin games, last round descriptions were less opaque (OR  = `r stats_text(acc_mod_2, 8)`). Again some of the uncertainty in estimating the fixed effects was driven by the strong variation based on target image (`r stats_text(acc_mod_2_me, 5)`), which again exceeded participant variation (`r stats_text(acc_mod_2_me, 13)`).

### Additional predictors

As additional post-hoc predictors, we examined the accuracy of the in-game matchers from @boyce2024 and the length of the description. In both experiments, in-game accuracy was predictive of naïve matcher accuracy (Expt 2a OR = `r stats_text(acc_mod_1_orig_acc, 4)`, Expt 2b OR = `r stats_text(acc_mod_2_orig_acc, 6)`). The log number of words in the description was not predictive in Experiment 2a (OR = `r stats_text(acc_mod_1_orig_length, 4)`), but longer descriptions were slightly beneficial in Experiment 2b (OR = `r stats_text(acc_mod_2_orig_length, 6)`). 

The pattern of results for when conventions became more opaque was similar to the pattern of which game conditions produced descriptions that diverged the most in semantic space in @boyce2024. As a post-hoc test of whether opacity might be related to semantic divergence, we used the mean semantic similarity between an utterance and other utterances in the same condition as an additional predictor of accuracy.^[Semantic similarity was operationalized as cosine similarity between S-BERT embeddings [@reimers2019], the measure of semantic distance used in @boyce2024.] Similarity to other utterances was strongly predictive of increased accuracy in both experiments (Expt 2a: OR = `r stats_text(acc_mod_1_sbert, 3)`, Expt 2b: OR = `r stats_text(acc_mod_2_sbert, 4)`) and was more predictive for the last round descriptions (Expt 2a: OR = `r stats_text(acc_mod_1_sbert, 4)`, Expt 2b: OR = `r stats_text(acc_mod_2_sbert, 5)`). While exploratory, this analysis suggests that referring expressions that are further from shared semantic priors are harder for naive listeners to understand. 

## Model results

As a computational comparison, we looked at the CLIP-MLP model's performance on the same descriptions. We used the probability the model assigned to the correct target as our dependent measure and fit a Bayesian mixed effects beta regression on the descriptions from Experiment 2.^[`r form(acc_mlp_form)`] The CLIP-MLP model was far above chance, but had lower accuracy than the human participants (OR = `r stats_text(acc_mod_mlp, 1)`). The strongest predictor of accuracy was later round (OR = `r stats(acc_mod_mlp, 8)`), but even this was uncertain. There was substantial by-target image variation (`r stats_text(acc_mod_mlp_me, 7)`). 

In additional models, we checked the effect of in-game matcher accuracy, length of the description, and semantic divergence. CLIP-MLP had higher accuracy when in-game matcher accuracy was higher (OR = `r stats_text(acc_mod_mlp_orig_acc, 8)`), and the model did better on shorter descriptions (OR for log words = `r stats_text(acc_mod_mlp_orig_length, 8)`). Long descriptions may be difficult because they are further further from the model's training distribution of image captions.
Semantic similarity to other descriptions from the same type of games was predictive of higher accuracy (OR = `r stats_text(acc_mod_mlp_sbert, 5)`), especially for last round utterances (OR = `r stats_text(acc_mod_mlp_sbert, 6)`). 

## Discussion

Overall, naïve human matchers were fairly accurate overall, but less accurate than matchers in the original game. The computational model was less accurate, but still far above chance. <!-- The largest source of variability in accuracy was from the target images; while there was some variability in accuracy by images for the original matchers, there was substantially more variability for naïve matchers. --> <!--The computational model showed a similar pattern of large effects of image, but had overall lower accuracy than naïve human matchers.--> 
The largest source of variability in accuracy was from target images, and whether earlier or later utterances were more opaque varied by game condition. 
<!--For 2-player medium games and both thick conditions, naive matchers understood the earlier (pre-convention) descriptions better, suggestive of a slight increase in opacity. However, in 6-player medium games, the reverse pattern held, and there was no substantial difference in interpretability across timepoints for the thin games. This pattern is associated with how cohesive groups were and how much games diverged semantically from each other.--> While not a pre-planned analysis, the level of semantic divergence from other expressions was strongly predictive of the opacity of the expression (for both humans and the model), suggesting that descriptions that were closer to shared semantic priors were more transparent.

# Experiment 3

The experiment of naïve matchers in Experiment 2 differed from in-game matchers in several ways. In-game matchers recieved descriptions from a consistent group, recieved descriptions in the order they were created, and were present participants during the game. In Experiment 3, we focused on the role of context and group-specific interaction history to tease apart some of these differences. Our primary question of interest was how much seeing the entire the conversation history in order would help participants understand later round descriptions. 

## Methods

We compared naïve matchers in "yoked" and "shuffled" conditions. In the "yoked" condition, naïve matchers saw all the descriptions from a single game in the order they originally occurred. In the "shuffled" condition, naïve matchers saw all the descriptions from a single game in a randomized order. This is the same yoking but a different shuffling than that used in @hawkins2023a. 

Because some descriptions are already fairly comprehensible in isolation, we focused on games that showed strong group-specificity. We hand-picked 10 games from @boyce2024 on the basis of high in-game matcher accuracy, strong patterns of descriptions shortening over repetition, and the use of idiosyncratic or non-modal referring expressions. Thus, these games showed the hallmarks of strong conventionalization to terms that were more likely to be opaque to outsiders. 

We recruited 196 participants (99 in the yoked condition and 97 in shuffled) who each saw all 72 trials of one of the 10 games. This experiment was pre-registered at https://osf.io/zqwp5. Participants read the transcripts in a modified self-paced reading procedure where they uncovered the text word by word (revealed words stayed visible); only after uncovering the entire transcript could participants select an image. We do not analyze the reading time data here.

```{r fig-yoked, fig.env="figure", fig.pos = "t", fig.align = "center", out.width="90%", fig.width=5, fig.height=3, fig.cap = "Accuracies for Experiment 3. Error bars are bootstrapped 95% CIs. TODO not using predictions because those fuzz out round to round differences. \\label{yoked}" }
expt_4_acc_data <- expt_4_data |>
  filter(type == "selection") |>
  mutate(correct = as.numeric(correct)) |>
  select(workerid, correct, orig_repNum, round, condition, matcher_trialNum, gameId, correct_tangram, source)

expt_3_relevant_games <- expt_4_acc_data |>
  select(gameId, round) |>
  unique()
expt_3_fig_data <- expt_4_acc_data |>
  bind_rows(mlp_mod |> inner_join(expt_3_relevant_games)) |>
  bind_rows(original_results |> inner_join(expt_3_relevant_games)) |>
  mutate(condition = ifelse(!is.na(condition), condition, source)) |>
  mutate(round = str_sub(round, -1)) |>
  mutate(condition = factor(condition, levels = c("original", "yoked", "shuffled", "model"), labels = c("Original", "Yoked", "Shuffled", "CLIP-MLP")))


predicted_expt_3_fig <- read_rds(here(mod_loc, "predicted/acc_3.rds")) |>
  bind_rows(read_rds(here(mod_loc, "predicted/acc_3_mlp.rds")) |> mutate(condition = "model")) |>
  mutate(across(mean:high, inv_logit_scaled)) |>
  mutate(round = orig_repNum + 1) |>
  mutate(condition = factor(condition, levels = c("original", "yoked", "shuffled", "model"), labels = c("Original", "Yoked", "Shuffled", "CLIP-MLP")))

predicted_expt_3_fig_no_trial <- read_rds(here(mod_loc, "predicted/acc_3_no_trial.rds")) |>
  bind_rows(read_rds(here(mod_loc, "predicted/acc_3_mlp.rds")) |> mutate(condition = "model")) |>
  mutate(across(mean:high, inv_logit_scaled)) |>
  mutate(round = orig_repNum + 1) |>
  mutate(condition = factor(condition, levels = c("original", "yoked", "shuffled", "model"), labels = c("Original", "Yoked", "Shuffled", "CLIP-MLP")))

expt_3_fig_data |> ggplot(aes(x = round, color = condition, y = correct, group = condition)) +
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .6), geom = "line") +
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .6)) +
  # geom_pointrange(data = predicted_expt_3_fig, aes(y = mean, ymax = high, ymin = low), position=position_dodge(.6)) +
  #geom_line(data = predicted_expt_3_fig, aes(y = mean), position=position_dodge(.6), lty="dashed") +
  #   geom_pointrange(data = predicted_expt_3_fig_no_trial |> filter(condition!="CLIP-MLP"), aes(y = mean, ymax = high, ymin = low), position=position_dodge(.6)) +
 # geom_line(data = predicted_expt_3_fig_no_trial |> filter(condition!="CLIP-MLP"), aes(y = mean), position=position_dodge(.6), lty="dotted") +
  scale_y_continuous(lim = c(0, 1), expand = c(0, 0)) +
  geom_hline(yintercept = 1 / 12, lty = "dashed") +
  labs(x = "Round", y = "Accuracy") +
  scale_color_manual(breaks = c("Original", "Yoked", "Shuffled", "CLIP-MLP"), values = c("#7570B3", "#E7298A", "#1B9E77", "#D95F02")) +
  theme(
    legend.position = "bottom", legend.box = "vertical",
    strip.background = element_blank(),
    legend.title = element_blank(),
    legend.margin = margin(c(-10, 1, 0, 1))
  )
```


```{r}
acc_mod_4 <- read_rds(here(mod_results, "acc_4.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_mod_4_me <- read_rds(here(mod_me, "acc_4.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_4_form <- read_rds(here(mod_form, "acc_4.rds"))

acc_mlp_4 <- read_rds(here(mod_results, "acc_yoked_mlp_beta.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_mlp_4_me <- read_rds(here(mod_me, "acc_yoked_mlp_beta.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_4_mlp_form <- read_rds(here(mod_form, "acc_yoked_mlp_beta.rds"))


acc_compare <- read_rds(here(mod_results, "yoked_shuffled_original.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_compare_me <- read_rds(here(mod_me, "yoked_shuffled_original.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_4_form_compare <- read_rds(here(mod_form, "yoked_shuffled_original.rds"))
```

## Results and discussion

Our primary question of interest was how much seeing the conversation history unfold in order would help participants interpret descriptions, especially those from later rounds after conventions had formed.  

We compared accuracy across the yoked and shuffled conditions with a Bayesian mixed effects logistic regression.^[`r form(acc_4_form)`]. The descriptions were more transparent when they were presented in a yoked order (OR = `r stats_text(acc_mod_4, 2)`, Figure \ref{yoked}). In the shuffled condition, there was no main effect of round number (OR for one round later = `r stats_text(acc_mod_4, 4)`), but there was a marginal interaction where the benefit of the yoked condition decreased for later rounds (OR for one round later = `r stats_text(acc_mod_4, 5)`). This was offset by matchers in both conditions improving at the task over time (OR for one trial later in matcher viewing order = `r stats_text(acc_mod_4, 3)`). In the yoked condition round and trial number were aligned, so an improvement over time could be either from matcher practice or from descriptions being easier to understand. In the shuffled condition, matcher practice effects did not correlate with position in the original game. 

Comparing to the performance of in-game matchers, we can separate out the benefits of seeing the descriptions in order versus being a participant in the group.^[`r form(acc_4_form_compare)`] There is a benefit to seeing the items in order (OR = `r stats_text(acc_compare, 3)`) and a larger benefit to being a participant during the game (OR = `r stats_text(acc_compare, 7)`). The benefit of seeing the items in order wanes in later blocks (OR = `r stats_text(acc_compare, 5)`), but the benefit of being in the game does not (OR = `r stats_text(acc_compare, 6)`). In all cases, there is a baseline improvement over trials (OR = `r stats_text(acc_compare, 2)`). 

The accuracy of the CLIP-MLP model is worse than the shuffled human results, and does not show change across rounds (OR for one round later = `r stats_text(acc_mlp_4, 2)`). The larger difference between naïve human and CLIP-MLP accuracies in Experiment 3 than Experiment 2 could suggest that even the shuffled ordering still provides useful context (e.g., the consistent set of images) that helps matchers understand the conventions. This history is not available to the CLIP-MLP model which sees every description as a one-shot task. 


# General discussion 

Real-world conventions vary in whether they are opaque to outsiders ("rizz") or comprehensible even to those who don't produce them ("roundabout"). Convention formation in the real world is subject to a number of pressures (including communicative ones and social ones), but is also difficult to study. As a controlled experimental situation, iterated reference games provide a way of studying convention formation driven by communicative needs. 

Conventions are formed in reference games in a partner-specific way, where different groups follow different paths through semantic space as they form conventions. However, the referential descriptions used over the course of convention formation remain relatively understandable to outsiders. Across multiple experiments with human matchers, we found that naïve human matchers were far above chance accuracy at identifying the targets, with variation explained more by the target image than the round or game condition the descriptions came from.  Even for games selected for strong conventionalization, naïve matchers had high accuracy overall, although this accuracy was increased if they followed the process of conventionalization and saw utterances in order.

There were suggestive patterns that descriptions that were further from the norm were more opaque.
 <!-- which would be consistent with a lexical uncertainty approach where expressions that are closer to overall priors are easier to understand, and groups that have fewer people and thicker channels are more able to break away from these priors and have conventions that drift farther from typical in semantic space. -->
 
We also tested a computational model built on CLIP with a multi-layer perceptron readout as a way of approximating the context-independent semantic distance between descriptions and images. The CLIP-MLP model was far above chance in its assignment of probabilities to target images and correlated with human accuracies, although its probabilities were lower than human matcher accuracies. 

This work suggests that conventions formed within a small group may still be fairly comprehensible to those outside the group, who may produce different descriptions themselves. This finding raises questions around how well-calibrated describers are to their matcher's level of knowledge, and whether the process of convention formation is actually efficient. Even naïve matchers can often understand the shorthand descriptions, especially when there has not been a lot of semantic drift, but in reference games, describers choose elaborated descriptions with new matchers. In a game, norms of cooperation and conversation may lead describers to start new matchers with elaborated descriptions designed to give them a high level of confidence in target selection. Describers are also constrained by their need to come up with a description in real time. However, the high level of understanding and the lack of substantial benefit from early round descriptions does raise empirical questions about how calibrated describers are to the level of information necessary. 

Our experimental and computational results were only on a specific set of iterated reference game transcripts targeting a specific set of 12 images. There are potential non-measured differences between in-game matchers and naïve matchers, particularly in terms of effort and time spent on the task.

Some images may lend themselves to more transparent descriptions because they are more iconic, with a narrower prior over different ways they could be conceptualized, or they may be further from competitors within this pool of images. We are limited by the 12 images we used, but future work sampling across larger sets of images [such as @ji2022] could probe image-level factors. Future work could also explore within-description sources of variation and how the structure and word choice of utterance correlates with naïve matcher accuracy. Computational models could be especially beneficial because they could be run on subsets or ablations of descriptive text. 

A difficulty with scaling up computational models like CHAI and RSA to be a generative model of natural language reference games is the issue of how to handle baseline semantics. Even without a generative model, the lexical uncertainty approach is a useful one for understanding how information is conveyed and conventions formed in iterated reference games. It also provide a useful lens for examining flexibility of semantics and how new meanings arise and are understood. TODO could use another concluding sentence!

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
