---
title: "Idiosyncratic but not opaque: Linguistic conventions formed in reference games are interpretable by naïve humans and vision–language models"
bibliography: tg-matcher-cogsci.bib
csl: apa7.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Veronica Boyce (vboyce@stanford.edu)} \\ Department of Psychology \\ Stanford University \And {\large \bf Ben Prystawski (benpry@stanford.edu)} \\Department of Psychology \\ Stanford University
    \AND {\large \bf Alvin Wei Ming Tan (tanawm@stanford.edu)} \\ Department of Psychology \\ Stanford University \And {\large \bf Michael C. Frank (mcfrank@stanford.edu)} \\ Department of Psychology \\ Stanford University}

abstract: > 
  In-group linguistic conventions can be opaque to non-group members (teen slang like "rizz") or generally interpretable (regionalisms like "roundabout"). The formation of linguistic conventions is often studied in iterated reference games, where over repeated reference to the same targets, a describer--matcher pair establishes partner-specific shorthand names for targets. To what extent does the partner-specificity of these linguistic conventions cause them to be opaque to outsiders? We use computational models and experiments with naïve matchers to assess the opacity of descriptions from iterated reference games. Both human matchers and the computational model perform well above chance, suggesting that the conventions substantially reflect aspects of shared semantic associations. This additional perspective provides insights into how opacity and level of semantic flexibility may play a role in convention formation. TODO WORKSHOP ABSTRACT, especially last sentence, currently at 131 words
    
keywords: >
    reference games; convention formation; computational modeling; opacity; pragmatics
    
output: cogsci2024::cogsci_paper
header-includes:
  - \usepackage{booktabs}
#- \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(
  fig.width = 3, fig.height = 3, fig.crop = F,
  fig.pos = "tb", fig.path = "figs/",
  echo = F, warning = F, cache = F,
  message = F, sanitize = T
)

library(png)
library(grid)
library(ggplot2)
library(xtable)
library(tidyverse)
library(here)
library(brms)
library(rstan)
library(rstanarm)
library(ggthemes)
library(jsonlite)
library(ggthemes)
library(scales)
library(viridis)
library(ggridges)
library(cowplot)
library(tidybayes)

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
theme_set(theme_bw())

data_loc <- "data"
mod_loc <- "analysis-code/models"
images <- "experiments/expt1/assets/images"
prediction_loc <- "model-code/model_predictions"
mod_results <- "analysis-code/models/summary"
mod_form <- "analysis-code/models/formulae"
mod_me <- "analysis-code/models/mixed_fx"

source(here("analysis-code/helper_cogsci.R"))
```


# Introduction
"He's got rizz." The idea that teen slang is arbitrary and opaque to outsiders (i.e. older generations) is enough of a cultural touchstone that late night comedy shows have segments about it. Teens are far from the only ones to have in-group naming conventions; many communities form stable linguistic conventions including professional jargon, regionalisms, and of course slang. <!--Temporary naming conventions can arise in smaller groups to refer to a particular entity that doesn’t have a label or even to refer back to group in-jokes. -->

The formation of linguistic conventions between individuals is often studied experimentally in iterated reference games. In these games, a describer tells their partner how to sort or match a series of abstract images [e.g., @clark1986; @hawkins2020b]. Over repeated rounds of referring to the same targets, pairs develop conventionalized nicknames for the target images. These nicknames are often partner-specific, in that different pairs develop different nicknames for the same targets. When describing the targets to a new person, describers return to more elaborated descriptions, indicating an expectation that the conventions is not an appropriate description to use for new matchers [@yoon2018; @wilkes-gibbs1992; @hawkins2021]. 

For one-shot reference games, the choice of referring expression can be modeled using Bayesian pragmatics models such as Rational Speech Acts model (RSA) that assume speakers and listeners reasoning about each other [@frank2012a; @goodman2016]. The Continual Hierarchical Adaptation through Inference model (CHAI) builds on RSA by adding rules for how agents can update their belief distributions after each interaction, to account for the dynamics of repeated interaction in iterated reference games [@hawkins2021a]. In models like CHAI, both speaker and listener agents are cooperatively reasoning about each other, but related models can be used when the interaction is one-sided, such as when a naive listener is overhearing a conversation. RSA models incorporating lexical uncertainty [citation TODO] treat listeners as Bayesian agents who jointly infer the meaning of a speaker's utterance and their model of the speaker's lexicon. Models incorporating lexical uncertainty have been used to model children's word learning [citation TODO] and person-to-person and group-to-group differences [TODO citations]. Usually the scope of person-to-person variation in lexica is constrained so it's more like learning a hierarchical set of tweaks to a lexicon so some words have slightly different extensions for different speakers and different circumstances, rather than learning completely arbitrary mappings for each person [@schuster2020; @hawkins2021a]. However, in some experimental settings, people can even learn contronymic mappings, where the same symbol has context-dependent opposite meanings [@misyak2016]. 

Conventions in iterated reference games are formed between people without a salient shared group identity prior to the interaction, but, people do not use the conventions with new partners [TODO could put citations here again]. How opaque are the temporary linguistic conventions created in reference games: are they opaque like "rizz" or interpretable like "roundabout"?

One way to measure the opacity of a referring expression is to look at the semantic distance between the signifier and the referent.  Expressions that are more transparent are those where signifiers and referents that are semantically close, such that anyone with the same general lexicon can identify the appropriate referent given the signifier. In contrast, expressions that are opaque have signifiers and referents that are semantically distant in the general lexicon, such that the relations are arbitrary and inaccessible without additional clues such as the partner-specific conversation history. 

How can we measure the semantic distances between a conventionalized referring expression and its target referent? One computational option is to use vision-language models as a way to operationalize a shared semantic space for both language and target image. Computational methods have enabled the embedding of various stimuli (including images and text) into high-dimensional feature spaces; these embeddings have properties which suggest that they are reasonable approximations of humans' semantic spaces, including similarity in representational geometries [e.g., @grand2022; @muttenthaler2021]. Indeed, embeddings from neural network models have been used as a form of semantics in a range of reference game scenarios [e.g., @gul2024; @ji2022; @kang2020; @le2022; @ohmer2022]. In particular, such embeddings can be treated as the default context- and speaker-independent lexicon, since they are not updated to account for convention formation within an iterated reference game. 

An alternative way of measuring the transparency of conventionalized referring expressions is to directly measure how often naive humans, who were not part of the group who formed the convention, can correctly associate the target referent with the referring expression. Prior work with naive matchers has been limited and has focused on the role of conversational history. Overall, naive matchers tend to do better the more their observation history resembles that of the original conversation. Naive listeners did better when they heard descriptions in order instead of in reverse order [@murfitt2001]. Overhearers who listened to the entire game in order outperformed overhearers who listened to recordings starting in the third round [@schober1989]. In an iterated reference game using drawing as the communication modality, yoked matchers who saw all the trials from one game in order outperformed shuffled matchers who saw trials sampled from 10 games but in trial order [@hawkins2023a]. 

Across all three studies, naïve matchers had worse accuracy than in-game matchers, but their performance was still far above chance, suggesting that the convention--target relationship is not purely arbitrary. In fact, even when pairs of participants try to obfuscate their meaning to match images with each other but not an overhearer, an overhearing participant can still do quite well [@clark1987a]. Nonetheless, receiving more context from an interaction---and in particular, having that context be in order---is beneficial to matchers. <!--Except for the shuffled condition of @hawkins2023a, these studies do not address how opaque descriptions are when they are presented without any prior context. <!--Such an approach would be important to have truly naïve matchers that lack even the context of trial order, providing a clearer understanding of when these expressions are opaque and when they are merely idiosyncratic.-->

In the current work, we address how the process of convention formation shapes the levels of opacity of the referring expressions created at different time points in an iterated reference game. We draw our reference game expressions from @boyce2024, which has a large corpus of iterated reference games that were played online using a chatbox for communication. This corpus is made up of 6-round iterated reference games using the same 12 target images. Games varied in how large the describer--matcher groups were (2--6 participants) and how "thick" the communication channels were. The varied conditions within a consistent framework allowed us to test how opacity of referring expressions varies depending on the conditions the referring expressions came from. 

Using reference expressions created in different games from @boyce2024, we use both human experiments and models to assess when and why expressions are opaque or understandable to outside observers. We first present a computational approach using a  vision-language model to measure the semantic similarities between referring expressions and their targets, and we validate our model against naive human matchers (Experiment 1). We then use both naive human matchers and the model to compare the opacity of referring expressions across different game conditions and time points (Experiment 2). Finally, we address the role of conversation history by comparing naive matcher performance on game transcripts present in-order versus out-of-order (Experiment 3). 

```{r interface, fig.env = "figure", fig.pos = "t!", out.width="100%", fig.align = "center", fig.cap = "Experimental Setup. Naive matchers read transcripts from trials in reference games from Boyce et al. (2024) and selected which image they thought was being described. Matchers recieved bonus payments for correct selections. \\label{game}", cache=FALSE}
knitr::include_graphics("matcher-diagram.pdf", error = FALSE)
```

# Task setup

## Materials
We drew our referential expressions from @boyce2024, excluding utterances that were marked as not containing referential content. For our naïve matcher experiments, we sampled different subsets of this corpus, and within the subsets, we excluded transcripts that contained swear words or crude or sexual language. For the computational model, we used the entire corpus, and pre-processed the text by concatenating all the referential messages sent by the describer for a given trial. 

## Experimental procedure 

We recruited English-speaking participants from Prolific. Participants were directed to the experiment, where the task was explained to them. On each trial, participants saw the full transcript from that trial, containing all the chat messages marked by whether they were from the speaker or a listener. Participants selected the image they thought was the target from the tableau of 12 (Figure \ref{game}). Participants received feedback on whether they were right or wrong on each trial. Except when the specific viewing order was part of the experimental manipulation, we randomized the order of trials, subject to the constraint that the same target could not repeat on adjacent trials. The task was implemented in jsPsych [@leeuw2023]. We paid participants $10 an hour plus a bonus of 5 cents per correct response. All our experimental code is at TODO OSF LINK GOES HERE. 

## Computational models

We used CLIP (`clip-vit-large-patch14`) as a listener model for our domain [@radford2021]. CLIP is a vision-language model that uses a text transformer and a vision transformer to embed text and images into the same space, trained to maximize the similarity between representations of images and their English titles and descriptions. It is a natural choice for reference games, as the model is trained to estimate the correspondence between images and phrases in natural language. We ran CLIP for the concatenated describer utterances and all 12 tangram shapes. We then computed probabilities for each tangram shape using CLIP's logits. The simplest way to do this is simply taking the softmax of the logits. However, the tangram shapes were possibly out the distribution for the model, which led it to favor some images over others regardless of the content of the text. To fix this, we trained readout models that made more calibrated predictions using CLIP's logits.

```{r, eval=F}
df_classifiers <- read_csv(here("model-code/ classifier_comparison-openai--clip-vit-large-patch14.csv"))
df_clf_comp <- df_classifiers |>
  group_by(classifier, params) |>
  summarize(mean_acc = mean(accuracy), sd_acc = sd(accuracy))

df_best_clfs <- df_clf_comp |>
  group_by(classifier) |>
  mutate(
    mean_acc = round(mean_acc, 2),
    sd_acc = round(sd_acc, 2)
  ) |>
  arrange(desc(mean_acc)) |>
  ungroup()

xtable(df_best_clfs)

df_clf_comp |> arrange(desc(mean_acc))

```

```{=latex}
\begin{table}
\caption{Cross-validated accuracies for classifiers. Standard deviations in accuracy across the 10 folds are shown in parentheses. Best performance within each model class is underlined, and best overall performance is bolded.}
\label{tab:classifier_comparison}
\centering

  \begin{tabular}{p{1em}lr}
    \toprule
    \multicolumn{2}{l}{Classifier} & Accuracy \\ 
    \midrule
        \multicolumn{2}{l}{Random baseline} & \smash{0.08} \\
    \multicolumn{2}{l}{CLIP without readout} & \smash{0.31} \\
    \multicolumn{2}{l}{Logistic regression} & \\
    & No penalty & \underline{\smash{0.50 (0.01)}} \\ 
    & \vspace{1mm}L2 penalty & 0.50  (0.01) \\ 
    \multicolumn{2}{l}{Random forest} & \\
    & 10 estimators & 0.46 (0.02) \\
    & 50 estimators  & 0.51 (0.02)\\ 
    & 100 estimators & 0.52 (0.02) \\ 
    & \vspace{1mm}500 estimators & \underline{\smash{0.52 (0.02)}} \\ 
    \multicolumn{2}{l}{Gradient-boosted tree} & \\
    & 10 estimators & 0.48 (0.02) \\ 
    & \vspace{1mm}100 estimators & \underline{\smash{0.51 (0.02)}} \\ 
    \multicolumn{2}{l}{Multi-layer perceptron} & \\
    & 1 $\times$ 32-dim hidden layer & 0.50 (0.01) \\ 
    & 1 $\times$ 100-dim hidden layer  & 0.52 (0.01) \\ 
    & 1 $\times$ 512-dim hidden layer & 0.53 (0.02) \\ 
    & 1 $\times$ 1028-dim hidden layer & 0.53 (0.02) \\ 
    & 2 $\times$ 32-dim hidden layers  & 0.51 (0.02) \\ 
    & 2 $\times$ 100-dim hidden layers & \underline{\smash{\textbf{0.55 (0.02)}}} \\ 
    \bottomrule
	\end{tabular}
\end{table}
```

We trained different readout models to assign probabilities to features using CLIP's logits as features. Models were trained to maximize task performance (i.e., to assign high probability to the target tangram given the concatenated speaker utterance). We compared four types of models: a random forest, a logistic regression model, a multi-layer perceptron (MLP), and a gradient-boosted tree. Classifiers were implemented in the `scikit-learn` and `XGBoost` libraries [@pedregosa2011scikit; @chen2016xgboost]. Table \ref{tab:classifier_comparison} shows the cross-validated accuracy of different readout models, as well as the performance of CLIP with no readout. Each readout model was evaluated using 10-fold cross-validation, where the model was trained on 90% of the data and evaluated on the remaining 10%. The MLP with two hidden layers of size 100 performed the best, so we use its predictions in subsequent analysis. In the rest of the analysis, we use the MLP trained on all the data.

# Experiment 1
```{r}
expt_3_cor <- cor.test(expt_3_summary$value, expt_3_summary$human_acc)
```

```{r fig-calibration, fig.env="figure", fig.pos = "t", fig.align = "center", out.width="70%", fig.width=3, fig.height=3, fig.cap = "Correlation between human accuracy and CLIP-MLP probability of target in Experiment 1.  Points are individual descriptions, colored by decile of CLIP-MLP probability, black line is the bootstrapped mean and 95% CI across descriptions for each decile. \\label{calibration}" }
expt_3_summary |> ggplot(aes(y = human_acc, x = value)) +
  geom_point(aes(color = grouping), alpha = .7) +
  # geom_line(data = summ, color = "black") +
  stat_summary(data = expt_3_groups) +
  stat_summary(data = expt_3_groups, fun.data = "mean_cl_boot", geom = "line") +
  theme(legend.position = "none") +
  scale_color_viridis(discrete = T) +
  labs(x = "CLIP-MLP probability", y = "Human accuracy") +
  coord_equal()
```

Our CLIP-MLP computational model was optimized for task accuracy.
To validate whether this objective also results in human-like response patterns, we conducted a calibration experiment to determine if, for any given utterance, the model-assigned target probability was aligned with the probability that a naïve human matcher would choose the target image.
<!-- We hypothesized that there would be a significant correlation between the target choice probability of the model and the target choice probability of naïve human matchers. -->

## Methods

We first obtained target probabilities from our CLIP-MLP model for all utterances from @boyce2024. We then used stratified sampling to select 217 trials by dividing model-predicted probabilities into deciles and choosing approximately 22 utterances per decile, spanning the 12 different possible target images.
We recruited `r expt_3_data |> select(workerid) |> n_distinct()` participants who each saw 64 trials randomly sampled from the 217 tested trials. On average, each trial was seen by `r expt_3_summary |> pull(human_n) |> mean() |> round()` participants. This experiment was pre-registered at https://osf.io/6pv5e.

## Results and discussion

We obtained human accuracies on each trial by dividing the number of participants who selected the target by the total number of participants who saw the trial (Figure \ref{fig:fig-calibration}). There was a modest but significant positive correlation between model-predicted probabilities and human accuracies ($r$ = `r round(expt_3_cor$estimate, digits = 2)` [`r round(expt_3_cor$conf.int[1], digits = 2)`, `r round(expt_3_cor$conf.int[2], digits = 2)`]). This result suggests that model predictions were calibrated to human response patterns, albeit not perfectly. <!--It is possible to use these calibration results to tune model predictions to better approximate human responses; we leave this approach for future work. --> Nonetheless, the observed positive correlation suggests that our computational model carries some signal about human accuracies, validating its use in subsequent experiments as a computational comparison.



```{r}
expt_2_relevant_games <- expt_1_data |>
  bind_rows(expt_2_data) |>
  select(gameId, group_size, thickness, round) |>
  unique()

expt_2_fig_data <- expt_1_data |>
  bind_rows(expt_2_data) |>
  bind_rows(mlp_mod |> inner_join(expt_2_relevant_games)) |>
  bind_rows(original_results |> inner_join(expt_2_relevant_games)) |>
  mutate(thickness = factor(thickness, levels = c("thin", "medium", "thick"))) |>
  mutate(source = factor(source, levels = c("original", "naïve", "model"), labels = c("Original matchers", "Naïve matchers", "CLIP-MLP"))) |>
  mutate(round = str_sub(round, -1))


predicted_expt_2_fig <- read_rds(here(mod_loc, "predicted/acc_2_cond.rds")) |>
  mutate(source = "naïve") |>
  bind_rows(read_rds(here(mod_loc, "predicted/acc_2_mlp_cond.rds")) |> mutate(source = "model")) |>
  mutate(source = factor(source, levels = c("original", "naïve", "model"), labels = c("Original matchers", "Naïve matchers", "CLIP-MLP"))) |>
  mutate(across(mean:high, inv_logit_scaled)) |>
  mutate(thickness = factor(thickness, levels = c("thin", "medium", "thick"))) |>
  mutate(round = str_sub(round, -1))


tangram_predicted_expt_2_fig <- read_rds(here(mod_loc, "predicted/acc_2_tangram.rds")) |>
  mutate(source = "naïve") |>
  bind_rows(read_rds(here(mod_loc, "predicted/acc_2_mlp_tangram.rds")) |> mutate(source = "model")) |>
  mutate(source = factor(source, levels = c("original", "naïve", "model"), labels = c("Original matchers", "Naïve matchers", "CLIP-MLP"))) |>
  mutate(across(mean:high, inv_logit_scaled))
```

```{r fig-condition, fig.env="figure", fig.pos = "t", fig.align = "center", out.width="90%", fig.width=4, fig.height=3, fig.cap = "Accuracies for naïve human matchers and the CLIP-MLP model for Experiments 2a and 2b, grouped by the source of the referential description. Facets are the communication thickness of the original game and x-axis is when in the game the transcript caome form. Point estimates and 95% CrI are predictions from the fixed effects of logistic and beta regressions. Bootstrapped mean accuracy from the original matchers is included as a ceiling, and random chance as a baseline. \\label{expt2-condition}" }
expt_2_fig_data |> ggplot(aes(x = round, color = source, y = correct, group = interaction(source, group_size), shape = group_size, lty = group_size)) +
  stat_summary(data = expt_2_fig_data |> filter(source == "Original matchers"), geom = "point", fun.data = "mean_cl_boot", position = position_dodge(.4)) +
  stat_summary(data = expt_2_fig_data |> filter(source == "Original matchers"), geom = "line", fun.data = "mean_cl_boot", position = position_dodge(.4)) +
#stat_summary(data = expt_2_fig_data , geom = "point", fun.data = "mean_cl_boot", position = position_dodge(.4)) +
 # stat_summary(data = expt_2_fig_data , geom = "line", fun.data = "mean_cl_boot", position = position_dodge(.4)) +
  geom_pointrange(data = predicted_expt_2_fig, aes(y = mean, ymax = high, ymin = low), position = position_dodge(.4)) +
  geom_line(data = predicted_expt_2_fig, aes(y = mean), position = position_dodge(.4)) +
  facet_grid(. ~ thickness) +
  scale_y_continuous(lim = c(0, 1), expand = c(0, 0)) +
  geom_hline(yintercept = 1 / 12, lty = "dashed") +
  labs(x = "Round in original game", y = "Accuracy") +
  scale_color_manual(breaks = c("Original matchers", "Naïve matchers", "CLIP-MLP"), values = c("#7570B3", "#1B9E77", "#D95F02")) +
  theme(
    legend.position = "bottom", legend.box = "vertical",
    strip.background = element_blank(),
    legend.title = element_blank(),
    legend.margin = margin(c(-10, 0, 0, -25))
  )
```


```{r fig-2, fig.env="figure", fig.pos = "t", fig.align = "center", out.width="90%", fig.width=4, fig.height=3, fig.cap = "Accuracies for naïve human matchers and the CLIP-MLP model for Experiments 2a and 2b, split out by target image. Point estimates and 95% CI are predictions from the fixed effects and by-tangram random effects of logistic and beta regressions, bootstrapped across conditions. Bootstrapped mean accuracy from the original matchers is included as a ceiling, and random chance as a baseline. \\label{expt2-tangram}" }
library(ggtext)
correct_tangram <- c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L")
labels <- c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L") |> map(~ str_c("<img src=", here(images, str_c("tangram_", ., ".png")), " width='20'/>"))


acc_by_target <- expt_1_data |>
  bind_rows(expt_2_data) |>
  group_by(correct_tangram) |>
  summarize(acc = sum(correct) / n()) |>
  arrange(acc)

foo <- tibble(correct_tangram, labels) |>
  left_join(acc_by_target) |>
  arrange(acc)

acc_by_type_target <- expt_2_fig_data |>
  group_by(correct_tangram, group_size, round, thickness, source) |>
  summarize(acc = sum(correct) / n()) |>
  mutate(correct_tangram = factor(correct_tangram, levels = acc_by_target$correct_tangram))

ggplot(acc_by_type_target, aes(x = correct_tangram, y = acc, color = source, shape = group_size, lty = group_size)) +
  geom_pointrange(data = tangram_predicted_expt_2_fig, aes(y = mean, ymax = high, ymin = low), position = position_dodge(.4)) +
  #  stat_summary(data = acc_by_type_target, aes(color = source), geom = "point", position = position_dodge(width = .6)) +

  stat_summary(data = acc_by_type_target |> filter(source == "Original matchers"), aes(color = source), geom = "point", position = position_dodge(width = .6)) +
  coord_cartesian(ylim = c(0, 1), xlim = c(.5, 12.5), expand = F) +
  geom_hline(yintercept = 1 / 12, linetype = "dashed") +
  scale_x_discrete(name = NULL, labels = foo$labels) +
  scale_color_manual(breaks = c("Original matchers", "Naïve matchers", "CLIP-MLP"), values = c("#7570B3", "#1B9E77", "#D95F02")) +
  labs(y = "Accuracy") +
  theme(
    legend.position = "bottom", legend.box = "vertical",
    strip.background = element_blank(),
    legend.title = element_blank(),
    axis.text.x = element_markdown(color = "black", size = 11),
    legend.margin = margin(c(-10, 0, 0, -25))
  )


# guide_legend()# acc_by_target
```

# Experiment 2

As a starting point for examining what makes referential expressions more or less opaque, we focused on referring expressions from the first and last rounds of games. Principles of convention formation and people's behavior when switching to a new partner suggest that later-round utterances are more opaque and thus harder to understand. One counterargument is that later rounds are the result of describers' accumulated practice refining descriptions to be maximally communicative. To distinguish these hypotheses, we ran a recognition experiment including descriptions from games of different sizes and communication thicknesses. Based on the patterns of cross-game similarity in @boyce2024, we expected that smaller and thicker games, whose descriptions diverged fastest, would have more idiosyncratic and opaque conventions than larger groups with thinner communication channels. 

## Methods

### Experiment 2a
To establish a baseline of how well naïve matchers could understand descriptions without context, we ran a 2x2 within subjects experiment. We drew the target transcripts from 2- and 6-player games from Experiment 1 of @boyce2024 and from the first and last blocks of these games. These games had medium-thick communication channels, where matchers could send text messages to the chat, the describer role rotated each round, and matchers received limited feedback. We recruited `r expt_1_data |> select(workerid) |> n_distinct()` participants who each saw 60 trials (15 in each of the 4 conditions). Overall, participants saw `r expt_1_data |> select(gameId, round, correct_tangram) |> n_distinct()` transcripts from `r expt_1_data |> select(gameId) |> n_distinct()` games. This experiment was pre-registered at https://osf.io/k45dr. 

### Experiment 2b
After observing limited condition differences in Experiment 2a, we ran a follow-up experiment on descriptions from Experiment 3 of @boyce2024, where the communication channel thicknesses were more extreme. Here, we used a 2x2x2 within subjects design, drawing our transcripts from the first and last rounds of thick and thin, 2- and 6- person games. In the "thick" condition, matchers could send text messages to the chat, one person was the describer for the whole game, and matchers received feedback on everyone's selections. In contrast, in the "thin" condition, matchers could only communicate by sending 4 emojis, the describer role rotated, and matchers recieved limited feedback. As the emojis did not have referential content, we did not include them in the transcripts shown to naïve matchers. 
For experiment 2b, we recruited `r expt_2_data |> select(workerid) |> n_distinct()` participants who each saw 64 trials (8 in each of the 8 conditions). Overall, participants saw `r expt_2_data |> select(gameId, round, correct_tangram) |> n_distinct()` transcripts from `r expt_2_data |> select(gameId) |> n_distinct()` games.  This experiment was pre-registered at  https://osf.io/rdp5k. 


```{r}
acc_mod_1 <- read_rds(here(mod_results, "acc_1.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))


acc_1_form <- read_rds(here(mod_form, "acc_1.rds"))

acc_mod_2 <- read_rds(here(mod_results, "acc_2.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_2_form <- read_rds(here(mod_form, "acc_2.rds"))


acc_mod_1_orig_acc <- read_rds(here(mod_results, "acc_1_orig_acc.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_mod_2_orig_acc <- read_rds(here(mod_results, "acc_2_orig_acc.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_mod_1_orig_length <- read_rds(here(mod_results, "acc_1_orig_length.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_mod_2_orig_length <- read_rds(here(mod_results, "acc_2_orig_length.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_mod_mlp <- read_rds(here(mod_results, "acc_mlp_1_2_beta.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_mlp_form <- read_rds(here(mod_form, "acc_mlp_1_2_beta.rds"))

acc_mod_mlp_orig_acc <- read_rds(here(mod_results, "acc_mlp_1_2_orig_acc_beta.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_mod_mlp_orig_length <- read_rds(here(mod_results, "acc_mlp_1_2_orig_length_beta.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

#expt_1_data |> summarize(m=mean(correct)) |> mutate(m=round(m*100))

acc_mod_1_me <- read_rds(here(mod_me, "acc_1.rds")) #|> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_mod_2_me <- read_rds(here(mod_me, "acc_2.rds")) #|> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_mod_mlp_me <- read_rds(here(mod_me, "acc_mlp_1_2_beta.rds")) #|> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))


acc_mod_1_sbert <- read_rds(here(mod_results, "acc_1_sbert.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_mod_2_sbert <- read_rds(here(mod_results, "acc_2_sbert.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_mod_mlp_sbert <- read_rds(here(mod_results, "acc_mlp_1_2_sbert_beta.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))


```

## Results

### Experiment 2a
For Experiment 2a, we ran a Bayesian mixed effects logistic model of naïve matcher accuracy in brms [@burkner2018].^[`r form(acc_1_form)`] Overall, naïve matchers were right 62% of the time, far above the 1/12 = 8.3% expected by random chance (OR = `r stats_text(acc_mod_1, 1)`).
There were not large effects of condition (Figure \ref{expt2-condition} middle panel). Participants tended to be less accurate at descriptions from the last round (OR of last round = `r stats_text(acc_mod_1, 4)`). There was not a clear effect of original group size (OR of 6-player game = `r stats_text(acc_mod_1, 2)`), but there was an interaction between round and group size (OR = `r stats_text(acc_mod_1, 3)`). Later transcripts from larger games were easier to understand, but earlier transcripts from smaller games were easier to understand.
Much of the variation in accuracy was driven by the target image, which accounted for more variation than participant differences (standard deviation of image distribution = `r stats_text(acc_mod_1_me, 3)`; SD of participant distribution = `r stats_text(acc_mod_1_me, 7)`). Some images were much easier to identify as the target than others (Figure \ref{expt2-tangram}). 

### Experiment 2b

For Experiment 2b we ran a similar Bayesian mixed effects logistic model.^[`r form(acc_2_form)`] Naïve matchers were above chance (OR = `r stats_text(acc_mod_2, 1)`, Figure \ref{expt2-condition} ). Similar to experiment 2a, there were not substantial effects of condition. Last round descriptions had slightly lower accuracy (OR of last round = `r stats_text(acc_mod_2, 6)`), but there was an interaction with thickness, where for thin games, last round descriptions were less opaque (OR  = `r stats_text(acc_mod_2, 8)`). Again there was strong variation based on target image (`r stats_text(acc_mod_2_me, 5)`), which exceeded by-participant variation (`r stats_text(acc_mod_2_me, 13)`).

### Additional predictors

We considered the accuracy of the in-game matchers from @boyce2024 and the length of the description as post-hoc predictors. In both experiments, in-game accuracy was predictive of naïve matcher accuracy (Expt 2a OR = `r stats_text(acc_mod_1_orig_acc, 4)`, Expt 2b OR = `r stats_text(acc_mod_2_orig_acc, 6)`). The log number of words in the description was not predictive in Experiment 2a (OR = `r stats_text(acc_mod_1_orig_length, 4)`), but longer descriptions were slightly beneficial in Experiment 2b (OR = `r stats_text(acc_mod_2_orig_length, 6)`). 

The pattern of which conditions became more opaque in later rounds ressembled the pattern of which conditions produced descriptions that diverged the most in semantic space in @boyce2024. As a post-hoc test of whether opacity might be related to semantic divergence, we used the mean semantic similarity between an utterance and other utterances in the same condition as an additional predictor of accuracy.^[Semantic similarity was operationalized as cosine similarity between S-BERT embeddings [@reimers2019], the measure of semantic distance used in @boyce2024.] Similarity to other utterances was strongly predictive of increased accuracy in both experiments (Expt 2a: OR = `r stats_text(acc_mod_1_sbert, 3)`, Expt 2b: OR = `r stats_text(acc_mod_2_sbert, 4)`) and was more predictive for the last round descriptions (Expt 2a: OR = `r stats_text(acc_mod_1_sbert, 4)`, Expt 2b: OR = `r stats_text(acc_mod_2_sbert, 5)`). While exploratory, this analysis suggests that referring expressions that are further from shared semantic priors are harder for naive listeners to understand. 

## Model results

As a computational comparison, we used the probability the CLIP-MLP model assigned to the correct target as our dependent measure and fit a Bayesian mixed effects beta regression on the descriptions from Experiment 2.^[`r form(acc_mlp_form)`] The CLIP-MLP model was far above chance, but had lower accuracy than the human participants (OR = `r stats_text(acc_mod_mlp, 1)`). The strongest predictor of accuracy was later round (OR = `r stats(acc_mod_mlp, 8)`), but even this was uncertain. There was substantial by-target image variation (`r stats_text(acc_mod_mlp_me, 7)`). 

In additional models, we checked the effect of in-game matcher accuracy, length of the description, and semantic divergence. CLIP-MLP had higher accuracy when in-game matcher accuracy was higher (OR = `r stats_text(acc_mod_mlp_orig_acc, 8)`), and when the descriptions were shorter (OR for log words = `r stats_text(acc_mod_mlp_orig_length, 8)`). The model may perform poorly on long descriptions because they are further from the model's training distribution of image captions.
A description's semantic similarity to other descriptions was predictive of higher accuracy (OR = `r stats_text(acc_mod_mlp_sbert, 5)`), especially for last round utterances (OR = `r stats_text(acc_mod_mlp_sbert, 6)`), in line with the human results.

## Discussion

Overall, naïve human matchers were fairly accurate overall, but less accurate than matchers in the original game, consistent with prior work. The computational model was less accurate, but still far above chance. <!-- The largest source of variability in accuracy was from the target images; while there was some variability in accuracy by images for the original matchers, there was substantially more variability for naïve matchers. --> <!--The computational model showed a similar pattern of large effects of image, but had overall lower accuracy than naïve human matchers.--> 
The largest source of variability in accuracy was from target images, and whether earlier or later utterances were more opaque varied by game condition. 
<!--For 2-player medium games and both thick conditions, naive matchers understood the earlier (pre-convention) descriptions better, suggestive of a slight increase in opacity. However, in 6-player medium games, the reverse pattern held, and there was no substantial difference in interpretability across timepoints for the thin games. This pattern is associated with how cohesive groups were and how much games diverged semantically from each other.--> While not a pre-planned analysis, the level of semantic divergence from other expressions was strongly predictive of the opacity of the expression, suggesting that descriptions that were closer to shared semantic priors were more interpretable.

# Experiment 3

The experience of naïve matchers in Experiment 2 differed from in-game matchers in several ways. In-game matchers received descriptions from a consistent group, in the order they were created, and were the intended audience of the descriptions. In Experiment 3, we focused on the role of context and group-specific interaction history to tease apart some of these differences. Our primary question of interest was how much seeing the entire the conversation history in order would increase the interpretability of later round descriptions. 

## Methods

We compared naïve matchers in "yoked" and "shuffled" conditions. In the "yoked" condition, naïve matchers saw all the descriptions from a single game in the order they originally occurred. In the "shuffled" condition, naïve matchers saw all the descriptions from a single game in a randomized order.^[For clarity, we note this is the same yoking but a different shuffling than that used in @hawkins2023a.]

Because some descriptions are already fairly comprehensible in isolation, we focused on games that showed strong group-specificity. We hand-picked 10 games from @boyce2024 on the basis of high in-game matcher accuracy, strong patterns of descriptions shortening over repetition, and the use of idiosyncratic or non-modal referring expressions. Thus, these games showed the hallmarks of strong conventionalization to terms that were more likely to be opaque to outsiders. 

We recruited 196 participants (99 in the yoked condition and 97 in shuffled) who each saw all 72 trials of one of the 10 games. This experiment was pre-registered at https://osf.io/zqwp5. Participants read the transcripts in a modified self-paced reading procedure where they uncovered the text word-by-word (revealed words stayed visible); only after uncovering the entire transcript could participants select an image. We do not analyze the reading time data here.

```{r fig-yoked, fig.env="figure", fig.pos = "t", fig.align = "center", out.width="90%", fig.width=5, fig.height=3, fig.cap = "Accuracies for Experiment 3. Error bars are bootstrapped 95% CIs. \\label{yoked}" }
expt_4_acc_data <- expt_4_data |>
  filter(type == "selection") |>
  mutate(correct = as.numeric(correct)) |>
  select(workerid, correct, orig_repNum, round, condition, matcher_trialNum, gameId, correct_tangram, source)

expt_3_relevant_games <- expt_4_acc_data |>
  select(gameId, round) |>
  unique()
expt_3_fig_data <- expt_4_acc_data |>
  bind_rows(mlp_mod |> inner_join(expt_3_relevant_games)) |>
  bind_rows(original_results |> inner_join(expt_3_relevant_games)) |>
  mutate(condition = ifelse(!is.na(condition), condition, source)) |>
  mutate(round = str_sub(round, -1)) |>
  mutate(condition = factor(condition, levels = c("original", "yoked", "shuffled", "model"), labels = c("Original", "Yoked", "Shuffled", "CLIP-MLP")))


predicted_expt_3_fig <- read_rds(here(mod_loc, "predicted/acc_3.rds")) |>
  bind_rows(read_rds(here(mod_loc, "predicted/acc_3_mlp.rds")) |> mutate(condition = "model")) |>
  mutate(across(mean:high, inv_logit_scaled)) |>
  mutate(round = orig_repNum + 1) |>
  mutate(condition = factor(condition, levels = c("original", "yoked", "shuffled", "model"), labels = c("Original", "Yoked", "Shuffled", "CLIP-MLP")))

predicted_expt_3_fig_no_trial <- read_rds(here(mod_loc, "predicted/acc_3_no_trial.rds")) |>
  bind_rows(read_rds(here(mod_loc, "predicted/acc_3_mlp.rds")) |> mutate(condition = "model")) |>
  mutate(across(mean:high, inv_logit_scaled)) |>
  mutate(round = orig_repNum + 1) |>
  mutate(condition = factor(condition, levels = c("original", "yoked", "shuffled", "model"), labels = c("Original", "Yoked", "Shuffled", "CLIP-MLP")))

expt_3_fig_data |> ggplot(aes(x = round, color = condition, y = correct, group = condition)) +
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .6), geom = "line") +
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .6)) +
  # geom_pointrange(data = predicted_expt_3_fig, aes(y = mean, ymax = high, ymin = low), position=position_dodge(.6)) +
  #geom_line(data = predicted_expt_3_fig, aes(y = mean), position=position_dodge(.6), lty="dashed") +
  #   geom_pointrange(data = predicted_expt_3_fig_no_trial |> filter(condition!="CLIP-MLP"), aes(y = mean, ymax = high, ymin = low), position=position_dodge(.6)) +
 # geom_line(data = predicted_expt_3_fig_no_trial |> filter(condition!="CLIP-MLP"), aes(y = mean), position=position_dodge(.6), lty="dotted") +
  scale_y_continuous(lim = c(0, 1), expand = c(0, 0)) +
  geom_hline(yintercept = 1 / 12, lty = "dashed") +
  labs(x = "Round", y = "Accuracy") +
  scale_color_manual(breaks = c("Original", "Yoked", "Shuffled", "CLIP-MLP"), values = c("#7570B3", "#E7298A", "#1B9E77", "#D95F02")) +
  theme(
    legend.position = "bottom", legend.box = "vertical",
    strip.background = element_blank(),
    legend.title = element_blank(),
    legend.margin = margin(c(-10, 1, 0, 1))
  )
```


```{r}
acc_mod_4 <- read_rds(here(mod_results, "acc_4.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_mod_4_me <- read_rds(here(mod_me, "acc_4.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_4_form <- read_rds(here(mod_form, "acc_4.rds"))

acc_mlp_4 <- read_rds(here(mod_results, "acc_yoked_mlp_beta.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_mlp_4_me <- read_rds(here(mod_me, "acc_yoked_mlp_beta.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_4_mlp_form <- read_rds(here(mod_form, "acc_yoked_mlp_beta.rds"))


acc_compare <- read_rds(here(mod_results, "yoked_shuffled_original.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_compare_me <- read_rds(here(mod_me, "yoked_shuffled_original.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_4_form_compare <- read_rds(here(mod_form, "yoked_shuffled_original.rds"))
```

## Results and discussion

Our primary question of interest was how much seeing the conversation history unfold in order would help participants interpret descriptions, especially those from later rounds.

We compared accuracy across the yoked and shuffled conditions with a Bayesian mixed effects logistic regression.^[`r form(acc_4_form)`]. The descriptions were more transparent when they were presented in a yoked order (OR = `r stats_text(acc_mod_4, 2)`, Figure \ref{yoked}). In the shuffled condition, there was no main effect of round number (OR for one round later = `r stats_text(acc_mod_4, 4)`), but there was a marginal interaction where the benefit of the yoked condition decreased for later rounds (OR for one round later = `r stats_text(acc_mod_4, 5)`). This was offset by matchers in both conditions improving at the task over time (OR for one trial later in matcher viewing order = `r stats_text(acc_mod_4, 3)`). <!--In the yoked condition round and trial number were aligned, so an improvement over time could be either from matcher practice or from descriptions being easier to understand. In the shuffled condition, matcher practice effects did not correlate with position in the original game. -->

Comparing to the performance of in-game matchers, we separated out the benefits of seeing the descriptions in order versus being a participant in the group.^[`r form(acc_4_form_compare)`] There was a benefit to seeing the items in order (OR = `r stats_text(acc_compare, 3)`) and a larger benefit to being a participant during the game (OR = `r stats_text(acc_compare, 7)`). The benefit of seeing the items in order waned in later blocks (OR = `r stats_text(acc_compare, 5)`), but the benefit of being in the game did not (OR = `r stats_text(acc_compare, 6)`). In all cases, there was a baseline improvement over trials (OR = `r stats_text(acc_compare, 2)`). As a caveat, we note that in-game matchers and naïve matchers may have varied from each other in terms of effort and time spent on the task, and thus the comparison should be interpreted cautiously.

The accuracy of the CLIP-MLP model was worse than the shuffled human results, and did not show change across rounds (OR for one round later = `r stats_text(acc_mlp_4, 2)`). The larger difference between naïve human and CLIP-MLP accuracies in Experiment 3 than Experiment 2 suggests that the shuffled ordering still provides useful context that helps matchers understand the conventions. This history was not available to the CLIP-MLP model which saw every description as a one-shot task. 


# General discussion 

Real-world conventions vary in whether they are opaque to outsiders ("rizz") or interpretable even to those who don't produce them ("roundabout"). Convention formation in the real world is difficult to study, so iterated reference games are used as a controlled way to study convention formation experimentally. Conventions are formed in reference games in a partner-specific way, where different groups follow different paths through semantic space as they form conventions. Despite the number of studies on iterated reference games, few studies have examined whether the descriptions are interpretable by outsiders. 

Across multiple experiments, we found that naïve human matchers were far above chance accuracy at identifying the targets, with variation explained more by the target image than the round or game condition the descriptions came from.  Even for games selected for strong conventionalization, naïve matchers had high accuracy overall, although this accuracy was further increased if they saw the conversation history in order. Exploratory analyses suggested that descriptions that were further from the norm were more opaque. [TODO gloss this better]
 <!-- which would be consistent with a lexical uncertainty approach where expressions that are closer to overall priors are easier to understand, and groups that have fewer people and thicker channels are more able to break away from these priors and have conventions that drift farther from typical in semantic space. -->
 
We also tested a computational model built on CLIP with a multi-layer perceptron readout as a way of approximating the context-independent semantic distance between descriptions and images. The CLIP-MLP model was far above chance in its assignment of probabilities to target images and correlated with human accuracies, although its probabilities were lower than human matcher accuracies. 

This work suggests that conventions formed within a small group may still be fairly comprehensible to those outside the group because many of the semantic relations have not drifted that far from baseline semantics. Thus much of the arbitrariness of conventions seems to be less random, and more like choosing from an array of potential semantic associations. TODO improve this paragraph 
<!--, who may produce different descriptions themselves. This finding raises questions around how well-calibrated describers are to their matcher's level of knowledge, and whether the process of convention formation is actually efficient. Even naïve matchers can often understand the shorthand descriptions, especially when there has not been a lot of semantic drift, but in reference games, describers choose elaborated descriptions with new matchers. In a game, norms of cooperation and conversation may lead describers to start new matchers with elaborated descriptions designed to give them a high level of confidence in target selection. Describers are also constrained by their need to come up with a description in real time. However, the high level of understanding and the lack of substantial benefit from early round descriptions does raise empirical questions about how calibrated describers are to the level of information necessary. -->

Our experimental and computational results were only on a specific set of iterated reference game transcripts targeting a specific set of 12 images. Some images may lend themselves to more transparent descriptions because they are more iconic, with a narrower prior over different ways they could be conceptualized, or they may be further from competitors within this pool of images. We are limited by the 12 images we used, but future work sampling across larger sets of images [such as @ji2022] could probe image-level factors. <!--Future work could also explore within-description sources of variation and how the structure and word choice of utterance correlates with naïve matcher accuracy. Computational models could be especially beneficial because they could be run on subsets or ablations of descriptive text. -->

A difficulty with scaling up computational models like CHAI and RSA to be a generative model of natural language reference games is the issue of how to handle baseline semantics. Even without a generative model, the lexical uncertainty approach is a useful one for understanding how information is conveyed and conventions formed in iterated reference games. Measuring semantic shifts using computational models provides another angle for understand how the process of linguistic convention formation varies depending on the situation. TODO HELP CONCLUDING SENTENCE and paragraph...
TODO potentiall suggest that further work could further look at semantics and opacity to understand dynamics of convention formation or something 

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
