---
title: "Idiosyncratic but not opaque: Linguistic conventions formed in reference games are interpretable by naïve humans and vision–language models"
bibliography: tg-matcher-cogsci.bib
csl: apa7.csl
document-params: "10pt, letterpaper"

author-information: > 
        \author{{\large \bf Veronica Boyce} (vboyce@stanford.edu) \\ Psychology Department, Stanford University \And {\large \bf Ben Prystawski}  \\ Psychology Department, Stanford University \AND {\large \bf Alvin Tan}  \\ Psychology Department, Stanford University \And {\large \bf Michael C. Frank}  \\ Psychology Department, Stanford University}

abstract: > 
  When are in-group linguistic conventions opaque to non-group members (teen slang like "rizz") or generally interpretable (regionalisms like "roundabout")? The formation of linguistic conventions is often studied in iterated reference games, where over repeated reference to the same targets, a describer--matcher pair establishes partner-specific shorthand names for targets. To what extent does the partner-specificity of these linguistic conventions cause them to be opaque to outsiders? We use computational models and experiments with naïve matchers to assess the opacity of descriptions from iterated reference games. Both human matchers and the computational model perform well above chance, suggesting that conventions are not fully arbitrary or opaque, but reflect aspects of shared semantic associations. 
    
keywords: >
    reference games; convention formation; computational modeling; opacity; pragmatics
    
output: cogsci2024::cogsci_paper
header-includes:
  - \usepackage{booktabs}
  - \raggedbottom
  - \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(
  fig.width = 3, fig.height = 3, fig.crop = F,
  fig.pos = "tb", fig.path = "figs/",
  echo = F, warning = F, cache = T,
  message = F, sanitize = T
)

library(png)
library(grid)
library(ggplot2)
library(xtable)
library(tidyverse)
library(here)
library(brms)
library(rstan)
library(rstanarm)
library(ggthemes)
library(jsonlite)
library(ggthemes)
library(scales)
library(viridis)
library(ggridges)
library(cowplot)
library(tidybayes)

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
theme_set(theme_bw())

data_loc <- "data"
mod_loc <- "analysis-code/models"
images <- "experiments/expt1/assets/images"
prediction_loc <- "model-code/model_predictions"
mod_results <- "analysis-code/models/summary"
mod_form <- "analysis-code/models/formulae"
mod_me <- "analysis-code/models/mixed_fx"

source(here("analysis-code/helper_cogsci.R"))
```


# Introduction

When a teen says about someone that "he's got rizz," what does this mean? The idea that teen slang is arbitrary and opaque to outsiders (i.e., older generations) is enough of a cultural touchstone that late night comedy shows have segments about it. Teens are far from the only ones to have in-group naming conventions; many communities form stable linguistic conventions including professional jargon, regionalisms, and of course slang. <!--Temporary naming conventions can arise in smaller groups to refer to a particular entity that doesn’t have a label or even to refer back to group in-jokes. -->

Iterated reference games are commonly used as a model system for studying the formation of linguistic conventions.  In these games, a describer tells their partner how to sort or match a series of abstract images [e.g., @clark1986; @hawkins2020b]. Over repeated rounds of referring to the same targets, pairs develop conventionalized nicknames for the target images. These nicknames are often partner-specific, in that different pairs develop different nicknames for the same targets. When describing the targets to a new person, describers return to more elaborated descriptions, indicating an expectation that prior conventions are not appropriate descriptions to use for new matchers [@yoon2018; @wilkes-gibbs1992; @hawkins2021]. This partner specificity is generally interpreted to be efficient, implying that conventions are not understandable to those outside of the formation process. 


For one-shot reference games, the choice of referring expression can be described as a process of recursive inference, in which speakers reason about how listeners will interpret their utterance and vice versa. This process is described in Bayesian pragmatics models such as the Rational Speech Acts model (RSA) [@frank2012a; @goodman2016]. The Continual Hierarchical Adaptation through Inference model (CHAI) builds on RSA by adding rules for how agents update their belief distributions after each interaction, to account for the dynamics of repeated interaction in iterated reference games [@hawkins2021a]. <!--In models like CHAI, both speaker and listener agents are cooperatively reasoning about each other, but related models can be used when the interaction is one-sided, such as when a naïve listener is overhearing a conversation.--> A key factor permitting variation in referring expressions is lexical uncertainty; RSA models incorporating lexical uncertainty [@bergen2016; @potts2016] treat listeners as Bayesian agents who jointly infer the meaning of a speaker's utterance and their model of the speaker's lexicon. Models incorporating lexical uncertainty have been used to model children's word learning [@bohn2022] as well as person- and group-level differences [@schuster2020; @hawkins2021a]. Usually models assume that person-to-person variation in lexica is small, so the agent is learning a general meaning and a hierarchical set of tweaks to account for slightly different word extensions based on speaker and circumstance, rather than learning arbitrary meanings for each person  [@schuster2020; @hawkins2021a; but see @misyak2016]. <!--However, in some experimental settings, people can even learn contronymic mappings, where the same symbol has context-dependent opposite meanings [@misyak2016]. -->

Conventions in iterated reference games are formed between people without a salient shared group identity prior to the interaction, but people do not use the conventions with new partners. How opaque are the temporary linguistic conventions created in reference games: are they opaque like "rizz" or interpretable like "roundabout"? From a theoretical perspective, measuring this opacity can inform the development of models to account for lexical change in conversation. We attempt to answer this question here. 

We consider opacity of references to be a measure of the semantic distance between the referring expression and the target. Transparent expressions (those with low opacity) have signifiers and referents that are semantically close, such that anyone with the same general lexicon can identify the appropriate referent given the signifier. In contrast, expressions that are opaque have signifiers and referents that are semantically distant in the lexicon, such that the relations are arbitrary and inaccessible without additional clues such as the partner-specific conversation history. We note that properties of utterance such as groundedness or compositionality and properties of the target such as iconicity and nameability may load on opacity but are not the same as opacity. 

One option for measuring the transparency of referring expressions is to use vision--language models to operationalize a shared semantic space for both language and images. Computational methods have enabled the embedding of various stimuli (including images and text) into high-dimensional feature spaces; these embeddings have properties which suggest that they are reasonable approximations of humans' semantic spaces, including similarity in representational geometries [e.g., @grand2022; @muttenthaler2021]. Indeed, embeddings from neural network models have been used as a form of semantics in a range of reference game scenarios [e.g., @gul2024; @ji2022; @kang2020; @le2022; @ohmer2022]. In particular, such embeddings can be treated as the default context- and speaker-independent lexicon, since they are not updated to account for convention formation within an iterated reference game. 

A second option for measuring the transparency of referring expressions is to measure how often naïve humans, who were not part of the group who formed the convention, can correctly associate the target referent with the referring expression.
Prior work with naïve matchers has been limited and has focused on the role of conversational history. Naïve matchers tend to do better the more their observation history resembles that of the original conversation---when hearing descriptions in order instead of in reverse order [@murfitt2001], when listening to the entire game instead of starting in the third round [@schober1989], and when seeing yoked trials from a single game rather than trials sampled across 10 games [@hawkins2023a]. In these studies, naïve matchers had worse accuracy than in-game matchers, but their performance was still far above chance, suggesting that the convention--target relationship is not purely arbitrary. In fact, even when pairs of participants try to obfuscate their meaning, overhearers can often still identify the target referents [@clark1987a].  <!--Except for the shuffled condition of @hawkins2023a, these studies do not address how opaque descriptions are when they are presented without any prior context. <!--Such an approach would be important to have truly naïve matchers that lack even the context of trial order, providing a clearer understanding of when these expressions are opaque and when they are merely idiosyncratic.-->

In the current work, we measure the opacity of the referring expressions created in iterated reference games. We use conversations from @boyce2024, who created a large corpus of iterated reference games that were played online using a chatbox for communication. This corpus is made up of 6-round iterated reference games using the same 12 target images. Games varied in how large the describer--matcher groups were (2--6 participants) and how "thick" the communication channels were (for example, if matchers could send back messages or just emoji). The varied conditions within a consistent framework allowed us to test how the opacity of referring expressions varies depending on the conditions the referring expressions came from. 

We use both human experiments and models to assess when and why expressions are opaque or understandable to outside observers. We first present a computational approach using a vision--language model to measure the semantic similarities between referring expressions and their targets, and we validate our model against naïve human matchers (Experiment 1). We then use both naïve human matchers and the model to compare the opacity of referring expressions across different game conditions and time points (Experiment 2). Finally, we address the role of conversation history by comparing naïve matcher performance on game transcripts presented in order versus out of order (Experiment 3). 

```{r interface, fig.env = "figure", fig.pos = "t!", out.width="100%", fig.align = "center", fig.cap = "Experimental setup. Naïve matchers read transcripts from trials in reference games from Boyce et al. (2024) and selected which image they thought was being described. Matchers recieved bonus payments for correct selections. \\label{game}", cache=FALSE}
knitr::include_graphics("matcher-diagram.pdf", error = FALSE)
```

# Task setup

## Materials

We drew our referring expressions from @boyce2024, excluding utterances that were marked as not containing referential content. For our naïve matcher experiments, we sampled different subsets of this corpus. Within the subsets, we excluded transcripts that contained swear words or crude or sexual language. For the computational model, we used the entire corpus, and pre-processed the text by concatenating all the referential messages sent by the describer for a given trial. 

## Experimental procedure 

We recruited English-speaking participants from Prolific. 
<!-- Participants were directed to the experiment, where the task was explained to them.  -->
On each trial, participants saw the full transcript from that trial, containing all the chat messages marked by whether they were from the speaker or a listener. Participants selected the image they thought was the target from the tableau of 12 (Figure \ref{game}). Participants received feedback on whether they were right or wrong on each trial. Except when the specific viewing order was part of the experimental manipulation, we randomized the order of trials, subject to the constraint that the same target could not repeat on adjacent trials. The task was implemented in jsPsych [@leeuw2023]. We paid participants $10 an hour plus a bonus of 5 cents per correct response. All our experimental code is at [this anonymized repo](https://osf.io/bfk8y/?view_only=165b81c5d69446f18e1bfd23e3d9cb5f). 

## Computational models

We used the Contrastive Language-Image Pretraining model (CLIP; `clip-vit-large-patch14`) as a comprehender model for our domain [@radford2021]. CLIP is a vision-language model that uses a text transformer and a vision transformer to embed text and images into the same space, trained to maximize the similarity between representations of images and their English captions. It is a natural choice for reference games, as the model is trained to estimate the correspondence between images and phrases in natural language. We ran CLIP for the concatenated describer utterances and all 12 tangram shapes. For each utterance, we computed probabilities for each tangram shape using logit scores from CLIP. The simplest way to do this is simply taking the softmax of the logits. However, tangram shapes are outside of the training distribution for the model, perhaps explaining why it favored some images over others regardless of the content of the text. 

```{r, eval=F}
df_classifiers <- read_csv(here("model-code/ classifier_comparison-openai--clip-vit-large-patch14.csv"))
df_clf_comp <- df_classifiers |>
  group_by(classifier, params) |>
  summarize(mean_acc = mean(accuracy), sd_acc = sd(accuracy))

df_best_clfs <- df_clf_comp |>
  group_by(classifier) |>
  mutate(
    mean_acc = round(mean_acc, 2),
    sd_acc = round(sd_acc, 2)
  ) |>
  arrange(desc(mean_acc)) |>
  ungroup()

xtable(df_best_clfs)

df_clf_comp |> arrange(desc(mean_acc))

```

```{=latex}
\begin{table}
\caption{Cross-validated accuracies for classifiers. Standard deviations in accuracy across the 10 folds are shown in parentheses. Best performance within each model class is underlined, and best overall performance is bolded.}
\label{tab:classifier_comparison}
\centering
\small
  \begin{tabular}{p{1em}lr}
    \toprule
    \multicolumn{2}{l}{Classifier} & Accuracy \\ 
    \midrule
        \multicolumn{2}{l}{Random baseline} & \smash{0.08} \\
    \multicolumn{2}{l}{CLIP without readout} & \smash{0.31} \\
    \multicolumn{2}{l}{Logistic regression} & \\
    & No penalty & \underline{\smash{0.50 (0.01)}} \\ 
    & \vspace{1mm}L2 penalty & 0.50  (0.01) \\ 
    \multicolumn{2}{l}{Random forest} & \\
    & 10 estimators & 0.46 (0.02) \\
    & 50 estimators  & 0.51 (0.02)\\ 
    & 100 estimators & 0.52 (0.02) \\ 
    & \vspace{1mm}500 estimators & \underline{\smash{0.52 (0.02)}} \\ 
    \multicolumn{2}{l}{Gradient-boosted tree} & \\
    & 10 estimators & 0.48 (0.02) \\ 
    & \vspace{1mm}100 estimators & \underline{\smash{0.51 (0.02)}} \\ 
    \multicolumn{2}{l}{Multi-layer perceptron} & \\
    & 1 $\times$ 32-dim hidden layer & 0.50 (0.01) \\ 
    & 1 $\times$ 100-dim hidden layer  & 0.52 (0.01) \\ 
    & 1 $\times$ 512-dim hidden layer & 0.53 (0.02) \\ 
    & 1 $\times$ 1028-dim hidden layer & 0.53 (0.02) \\ 
    & 2 $\times$ 32-dim hidden layers  & 0.51 (0.02) \\ 
    & 2 $\times$ 100-dim hidden layers & \underline{\smash{\textbf{0.55 (0.02)}}} \\ 
    \bottomrule
	\end{tabular}
\end{table}
```

To improve the performance of base CLIP, we trained a set of readout models to assign probabilities to images using CLIP's logits as features. Models were trained to maximize task performance (i.e., to assign high probability to the target tangram given the concatenated describer utterance). We compared four types of models: random forest, logistic regression, multi-layer perceptron (MLP), and gradient-boosted tree. Classifiers were implemented in the `scikit-learn` and `XGBoost` libraries [@pedregosa2011scikit; @chen2016xgboost]. Each readout model was evaluated using 10-fold cross-validation, where the model was trained on 90% of the data and evaluated on the remaining 10%. Table \ref{tab:classifier_comparison} shows the cross-validated accuracy of different readout models, as well as the performance of CLIP with no readout. The MLP with two hidden layers of size 100 performed the best on held-out data; in subsequent analyses, we use the MLP trained on all the data.

# Experiment 1

```{r}
expt_3_cor <- cor.test(expt_3_summary$value, expt_3_summary$human_acc)
```

```{r fig-calibration, fig.env="figure", fig.pos = "t", fig.align = "center", out.width="70%", fig.width=3, fig.height=3, fig.cap = "Correlation between human accuracy and CLIP-MLP probability of target in Experiment 1.  Small points are individual descriptions, colored by decile of CLIP-MLP probability, large points and error bars are the bootstrapped mean and 95% CI across descriptions for each decile. \\label{calibration}" }
expt_3_summary |> ggplot(aes(y = human_acc, x = value)) +
  geom_point(aes(color = grouping), alpha = .7) +
  # geom_line(data = summ, color = "black") +
  stat_summary(data = expt_3_groups) +
  stat_summary(data = expt_3_groups, fun.data = "mean_cl_boot", geom = "line") +
  theme(legend.position = "none") +
  scale_color_viridis(discrete = T) +
  labs(x = "CLIP-MLP probability", y = "Human accuracy") +
  coord_equal()
```

Our CLIP-MLP computational model was optimized for task accuracy.
To validate whether this objective also results in human-like response patterns, we conducted a calibration experiment to determine if model-assigned target probabilities were aligned with the probabilities that naïve human matchers would choose particular target images across a range of utterance-target pairs.
<!-- We hypothesized that there would be a significant correlation between the target choice probability of the model and the target choice probability of naïve human matchers. -->

## Methods

We first obtained target probabilities from our CLIP-MLP model for all utterances from @boyce2024. We then used stratified sampling to select 217 trials by dividing model-predicted probabilities into deciles and choosing approximately 22 utterances per decile, spanning the 12 different possible target images.
We recruited `r expt_3_data |> select(workerid) |> n_distinct()` participants who each saw 64 trials randomly sampled from the 217 tested trials. On average, each trial was seen by `r expt_3_summary |> pull(human_n) |> mean() |> round()` participants. This experiment was pre-registered at [this anonymized link](https://osf.io/6pv5e/?view_only=0bc61ddeda83493c844ca554f463ba85).

## Results and discussion

We obtained human accuracies on each trial by dividing the number of participants who selected the target by the total number of participants who saw the trial (Figure \ref{fig:fig-calibration}). There was a modest but significant positive correlation between model-predicted probabilities and human accuracies ($r$ = `r round(expt_3_cor$estimate, digits = 2)` [`r round(expt_3_cor$conf.int[1], digits = 2)`, `r round(expt_3_cor$conf.int[2], digits = 2)`]). This result suggests that model predictions were calibrated to human response patterns, albeit not perfectly. <!--It is possible to use these calibration results to tune model predictions to better approximate human responses; we leave this approach for future work. --> Nonetheless, the observed positive correlation suggests that our computational model carries some signal about human accuracies, validating its use in subsequent experiments as a computational comparison.

```{r}
expt_2_relevant_games <- expt_1_data |>
  bind_rows(expt_2_data) |>
  select(gameId, group_size, thickness, round) |>
  unique()

expt_2_fig_data <- expt_1_data |>
  bind_rows(expt_2_data) |>
  bind_rows(mlp_mod |> inner_join(expt_2_relevant_games)) |>
  bind_rows(original_results |> inner_join(expt_2_relevant_games)) |>
  mutate(thickness = factor(thickness, levels = c("thin", "medium", "thick"))) |>
  mutate(source = factor(source, levels = c("original", "naïve", "model"), labels = c("Original matchers", "Naïve matchers", "CLIP-MLP"))) |>
  mutate(round = str_sub(round, -1))


predicted_expt_2_fig <- read_rds(here(mod_loc, "predicted/acc_2_cond.rds")) |>
  mutate(source = "naïve") |>
  bind_rows(read_rds(here(mod_loc, "predicted/acc_2_mlp_cond.rds")) |> mutate(source = "model")) |>
  mutate(source = factor(source, levels = c("original", "naïve", "model"), labels = c("Original matchers", "Naïve matchers", "CLIP-MLP"))) |>
  mutate(across(mean:high, inv_logit_scaled)) |>
  mutate(thickness = factor(thickness, levels = c("thin", "medium", "thick"))) |>
  mutate(round = str_sub(round, -1))


tangram_predicted_expt_2_fig <- read_rds(here(mod_loc, "predicted/acc_2_tangram.rds")) |>
  mutate(source = "naïve") |>
  bind_rows(read_rds(here(mod_loc, "predicted/acc_2_mlp_tangram.rds")) |> mutate(source = "model")) |>
  mutate(source = factor(source, levels = c("original", "naïve", "model"), labels = c("Original matchers", "Naïve matchers", "CLIP-MLP"))) |>
  mutate(across(mean:high, inv_logit_scaled))
```

```{r fig-condition, fig.env="figure", fig.pos = "t", fig.align = "center", out.width="90%", fig.width=4, fig.height=3, fig.cap = "Accuracies for naïve human matchers and the CLIP-MLP model for Experiments 2a and 2b, grouped by the source of the referential description. Facets are the communication thickness of the original game and x-axis is when in the game the transcript came from. Point estimates and 95% CrI are predictions from the fixed effects of logistic and beta regressions. Bootstrapped mean accuracy from the original matchers is included as a ceiling, and random chance as a baseline. \\label{expt2-condition}" }
expt_2_fig_data |> ggplot(aes(x = round, color = source, y = correct, group = interaction(source, group_size), shape = group_size, lty = group_size)) +
  stat_summary(data = expt_2_fig_data |> filter(source == "Original matchers"), geom = "point", fun.data = "mean_cl_boot", position = position_dodge(.4)) +
  stat_summary(data = expt_2_fig_data |> filter(source == "Original matchers"), geom = "line", fun.data = "mean_cl_boot", position = position_dodge(.4)) +
#stat_summary(data = expt_2_fig_data , geom = "point", fun.data = "mean_cl_boot", position = position_dodge(.4)) +
 # stat_summary(data = expt_2_fig_data , geom = "line", fun.data = "mean_cl_boot", position = position_dodge(.4)) +
  geom_pointrange(data = predicted_expt_2_fig, aes(y = mean, ymax = high, ymin = low), position = position_dodge(.4)) +
  geom_line(data = predicted_expt_2_fig, aes(y = mean), position = position_dodge(.4)) +
  facet_grid(. ~ thickness) +
  scale_y_continuous(lim = c(0, 1), expand = c(0, 0)) +
  geom_hline(yintercept = 1 / 12, lty = "dashed") +
  labs(x = "Round in original game", y = "Accuracy") +
  scale_color_manual(breaks = c("Original matchers", "Naïve matchers", "CLIP-MLP"), values = c("#7570B3", "#1B9E77", "#D95F02")) +
  theme(
    legend.position = "bottom", legend.box = "vertical",
    strip.background = element_blank(),
    legend.title = element_blank(),
    legend.margin = margin(c(-10, 0, 0, -25))
  )
```


```{r fig-2, fig.env="figure", fig.pos = "t", fig.align = "center", out.width="90%", fig.width=4, fig.height=3, fig.cap = "Accuracies for naïve human matchers and the CLIP-MLP model for Experiments 2a and 2b, split out by target image. Point estimates and 95% CI are predictions from the fixed effects and by-tangram random effects of logistic and beta regressions, bootstrapped across conditions. Bootstrapped mean accuracy from the original matchers is included as a ceiling, and random chance as a baseline. \\label{expt2-tangram}" }
library(ggtext)
correct_tangram <- c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L")
labels <- c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L") |> map(~ str_c("<img src=", here(images, str_c("tangram_", ., ".png")), " width='20'/>"))


acc_by_target <- expt_1_data |>
  bind_rows(expt_2_data) |>
  group_by(correct_tangram) |>
  summarize(acc = sum(correct) / n()) |>
  arrange(acc)

foo <- tibble(correct_tangram, labels) |>
  left_join(acc_by_target) |>
  arrange(acc)

acc_by_type_target <- expt_2_fig_data |>
  group_by(correct_tangram, group_size, round, thickness, source) |>
  summarize(acc = sum(correct) / n()) |>
  mutate(correct_tangram = factor(correct_tangram, levels = acc_by_target$correct_tangram))

ggplot(acc_by_type_target, aes(x = correct_tangram, y = acc, color = source, shape = group_size, lty = group_size)) +
  geom_pointrange(data = tangram_predicted_expt_2_fig, aes(y = mean, ymax = high, ymin = low), position = position_dodge(.4)) +
  #  stat_summary(data = acc_by_type_target, aes(color = source), geom = "point", position = position_dodge(width = .6)) +

  stat_summary(data = acc_by_type_target |> filter(source == "Original matchers"), aes(color = source), geom = "point", position = position_dodge(width = .6)) +
  coord_cartesian(ylim = c(0, 1), xlim = c(.5, 12.5), expand = F) +
  geom_hline(yintercept = 1 / 12, linetype = "dashed") +
  scale_x_discrete(name = NULL, labels = foo$labels) +
  scale_color_manual(breaks = c("Original matchers", "Naïve matchers", "CLIP-MLP"), values = c("#7570B3", "#1B9E77", "#D95F02")) +
  labs(y = "Accuracy") +
  theme(
    legend.position = "bottom", legend.box = "vertical",
    strip.background = element_blank(),
    legend.title = element_blank(),
    axis.text.x = element_markdown(color = "black", size = 11),
    legend.margin = margin(c(-10, 0, 0, -25))
  )


# guide_legend()# acc_by_target
```

# Experiment 2

As a starting point for examining the opacity of referring expressions, we focused on referring expressions from the first and last rounds of reference games. 
<!-- Principles of convention formation and people's behavior when switching to a new partner suggest that  -->
Based on the idea that conventions form across repeated communication, later-round utterances should be more opaque. To test this hypothesis, we ran a recognition experiment including descriptions from games of different sizes and communication thicknesses. Based on the patterns of cross-game similarity in @boyce2024, we expected that smaller and thicker games, whose descriptions diverged fastest, would have more idiosyncratic and opaque conventions than larger groups with thinner communication channels. 

## Methods

### Experiment 2a
To establish a baseline of how well naïve matchers could understand descriptions without context, we ran a 2 $\times$ 2 within-subjects experiment, drawing target transcripts from 2- and 6-player games from Experiment 1 of @boyce2024 and from the first and last blocks of these games. These games had medium-thick communication channels, where matchers could send text messages to the chat, the describer role rotated each round, and matchers received limited feedback. We recruited `r expt_1_data |> select(workerid) |> n_distinct()` participants who each saw 60 trials (15 in each of the 4 conditions). Overall, participants saw `r expt_1_data |> select(gameId, round, correct_tangram) |> n_distinct()` transcripts from `r expt_1_data |> select(gameId) |> n_distinct()` games. This experiment was pre-registered at [this anonymized link](https://osf.io/k45dr/?view_only=1f4bbadb8e6b4b5f8d04cf04392967dd). 

### Experiment 2b
After observing limited condition differences in Experiment 2a, we ran a follow-up experiment on descriptions from Experiment 3 of @boyce2024, where the communication channel thicknesses were more extreme. Here, we used a 2 $\times$ 2 $\times$ 2 within-subjects design, drawing our transcripts from the first and last rounds of thick and thin, 2- and 6- person games. In the "thick" condition, matchers could send text messages to the chat, one person was the describer for the whole game, and matchers received feedback on everyone's selections. In contrast, in the "thin" condition, matchers could only communicate by sending 4 emoji, the describer role rotated, and matchers recieved limited feedback. As the emoji did not have referential content, we did not include them in the transcripts shown to naïve matchers. 
For experiment 2b, we recruited `r expt_2_data |> select(workerid) |> n_distinct()` participants who each saw 64 trials (8 in each of the 8 conditions). Overall, participants saw `r expt_2_data |> select(gameId, round, correct_tangram) |> n_distinct()` transcripts from `r expt_2_data |> select(gameId) |> n_distinct()` games.  This experiment was pre-registered at  [this anonymized link](https://osf.io/rdp5k/?view_only=87452ac1a7894f98b51b5642346e9e3d). 


```{r}
acc_mod_1 <- read_rds(here(mod_results, "acc_1.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))


acc_1_form <- read_rds(here(mod_form, "acc_1.rds"))

acc_mod_2 <- read_rds(here(mod_results, "acc_2.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_2_form <- read_rds(here(mod_form, "acc_2.rds"))


acc_mod_1_orig_acc <- read_rds(here(mod_results, "acc_1_orig_acc.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_mod_2_orig_acc <- read_rds(here(mod_results, "acc_2_orig_acc.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_mod_1_orig_length <- read_rds(here(mod_results, "acc_1_orig_length.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_mod_2_orig_length <- read_rds(here(mod_results, "acc_2_orig_length.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_mod_mlp <- read_rds(here(mod_results, "acc_mlp_1_2_beta.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_mlp_form <- read_rds(here(mod_form, "acc_mlp_1_2_beta.rds"))

acc_mod_mlp_orig_acc <- read_rds(here(mod_results, "acc_mlp_1_2_orig_acc_beta.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_mod_mlp_orig_length <- read_rds(here(mod_results, "acc_mlp_1_2_orig_length_beta.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

#expt_1_data |> summarize(m=mean(correct)) |> mutate(m=round(m*100))

acc_mod_1_me <- read_rds(here(mod_me, "acc_1.rds")) #|> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_mod_2_me <- read_rds(here(mod_me, "acc_2.rds")) #|> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_mod_mlp_me <- read_rds(here(mod_me, "acc_mlp_1_2_beta.rds")) #|> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))


acc_mod_1_sbert <- read_rds(here(mod_results, "acc_1_sbert.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_mod_2_sbert <- read_rds(here(mod_results, "acc_2_sbert.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_mod_mlp_sbert <- read_rds(here(mod_results, "acc_mlp_1_2_sbert_beta.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))


```

## Results

### Experiment 2a
For Experiment 2a, we ran a Bayesian mixed-effects logistic model of naïve matcher accuracy in `brms` [@burkner2018].^[`r form(acc_1_form)`] For this and other models, we report the model estimate and 95\% credible interval. Overall, naïve matchers were right 62% of the time, far above the 1/12 = 8.3% expected by random chance (OR = `r stats_text(acc_mod_1, 1)`).
There were no large effects of condition (Figure \ref{expt2-condition} middle panel). Participants tended to be less accurate at descriptions from the last round (OR of last round = `r stats_text(acc_mod_1, 4)`). There was no  clear effect of original group size (OR of 6-player game = `r stats_text(acc_mod_1, 2)`), but there was an interaction between round and group size (OR = `r stats_text(acc_mod_1, 3)`). Later transcripts from larger games were easier to understand, but earlier transcripts from smaller games were easier to understand.
Much of the variation in accuracy was driven by the target image, which accounted for more variation than participant differences (standard deviation of image distribution = `r stats_text(acc_mod_1_me, 3)`; SD of participant distribution = `r stats_text(acc_mod_1_me, 7)`). Some images were much easier to identify as the target than others (Figure \ref{expt2-tangram}). 

### Experiment 2b

For Experiment 2b, we ran a similar Bayesian mixed-effects logistic model.^[`r form(acc_2_form)`] Naïve matchers were above chance (OR = `r stats_text(acc_mod_2, 1)`, Figure \ref{expt2-condition}). As in Experiment 2a, there were not substantial effects of condition. Last-round descriptions had slightly lower accuracy (OR of last round = `r stats_text(acc_mod_2, 6)`), but there was an interaction with thickness, where for thin games, last round descriptions were less opaque (OR  = `r stats_text(acc_mod_2, 8)`). Again there was strong variation based on target image (`r stats_text(acc_mod_2_me, 5)`), which exceeded by-participant variation (`r stats_text(acc_mod_2_me, 13)`).

### Additional predictors

We considered the accuracy of the in-game matchers from @boyce2024 and the length of the description as post-hoc predictors. In both experiments, in-game accuracy was predictive of naïve matcher accuracy (Expt 2a OR = `r stats_text(acc_mod_1_orig_acc, 4)`, Expt 2b OR = `r stats_text(acc_mod_2_orig_acc, 6)`). The log number of words in the description was not predictive in Experiment 2a (OR = `r stats_text(acc_mod_1_orig_length, 4)`), but longer descriptions were slightly beneficial in Experiment 2b (OR = `r stats_text(acc_mod_2_orig_length, 6)`). 

The pattern of which conditions became more opaque in later rounds resembled the pattern of which conditions produced descriptions that diverged the most in semantic space in @boyce2024. As a post-hoc test of whether opacity might be related to semantic divergence from other descriptions, we used the mean semantic similarity between an utterance and other utterances in the same condition as an additional predictor of accuracy.^[Semantic similarity was operationalized as cosine similarity between S-BERT embeddings [@reimers2019], the measure of semantic distance used in @boyce2024.] Similarity to other utterances was strongly predictive of increased accuracy in both experiments (Expt 2a: OR = `r stats_text(acc_mod_1_sbert, 3)`, Expt 2b: OR = `r stats_text(acc_mod_2_sbert, 4)`) and was more predictive for the last round descriptions (Expt 2a: OR = `r stats_text(acc_mod_1_sbert, 4)`, Expt 2b: OR = `r stats_text(acc_mod_2_sbert, 5)`). While exploratory, this analysis suggests that referring expressions that are further from shared semantic priors (i.e., more idiosyncratic) are harder for naïve matchers to understand. 

## Model results

As a computational comparison, we used the probability the CLIP-MLP model assigned to the correct target as our dependent measure and fit a Bayesian mixed-effects beta regression on the descriptions from Experiment 2.^[`r form(acc_mlp_form)`] The CLIP-MLP model was far above chance, but had lower accuracy than the human participants (OR = `r stats_text(acc_mod_mlp, 1)`). The strongest predictor of accuracy was later round (OR = `r stats_text(acc_mod_mlp, 8)`), but even this was uncertain. There was substantial by-target image variation (SD = `r stats_text(acc_mod_mlp_me, 7)`). 

In additional models, we checked the effect of in-game matcher accuracy, length of the description, and semantic divergence. CLIP-MLP had higher accuracy when in-game matcher accuracy was higher (OR = `r stats_text(acc_mod_mlp_orig_acc, 8)`), and when descriptions were shorter (OR for log words = `r stats_text(acc_mod_mlp_orig_length, 8)`). The model may perform poorly on long descriptions because they are further from the model's training distribution of image captions.
A description's semantic similarity to other descriptions was predictive of higher accuracy (OR = `r stats_text(acc_mod_mlp_sbert, 5)`), especially for last round utterances (OR = `r stats_text(acc_mod_mlp_sbert, 6)`), in line with the human results.

## Discussion

Overall, naïve human matchers were fairly accurate overall, but less accurate than matchers in the original game, consistent with prior work. The computational model was less accurate, but still far above chance. <!-- The largest source of variability in accuracy was from the target images; while there was some variability in accuracy by images for the original matchers, there was substantially more variability for naïve matchers. --> <!--The computational model showed a similar pattern of large effects of image, but had overall lower accuracy than naïve human matchers.--> 
The largest source of variability in accuracy was from target images, and whether earlier or later utterances were more opaque varied by game condition. 
<!--For 2-player medium games and both thick conditions, naïve matchers understood the earlier (pre-convention) descriptions better, suggestive of a slight increase in opacity. However, in 6-player medium games, the reverse pattern held, and there was no substantial difference in interpretability across timepoints for the thin games. This pattern is associated with how cohesive groups were and how much games diverged semantically from each other.--> The level of semantic divergence from other expressions was strongly predictive of the opacity of the expression. While this analysis was not prespecified, it still provides some suggestion that descriptions that were closer to shared semantic priors were also more interpretable.

# Experiment 3

The experience of naïve matchers in Experiment 2 differed from in-game matchers in several ways; any of these could explain differences in accuracy. In-game matchers received descriptions from a consistent group, in the order they were created, and were the intended audience of the descriptions. In Experiment 3, we focused on the role of context and group-specific interaction history to tease apart some of these differences. Our primary question of interest was how much seeing the entire the conversation history in order would increase the interpretability of later round descriptions. 

## Methods

We compared naïve matchers in yoked and shuffled conditions. In the yoked condition, naïve matchers saw all the descriptions from a single game in the order they originally occurred. In the shuffled condition, naïve matchers saw all the descriptions from a single game in a randomized order.^[For clarity, we note this is the same yoking but a different shuffling than that used in @hawkins2023a.]

Because some descriptions are already fairly comprehensible in isolation, we focused on games that showed strong group-specificity. We hand-picked 10 games from @boyce2024 on the basis of high in-game matcher accuracy, strong patterns of descriptions shortening over repetition, and the use of idiosyncratic or non-modal referring expressions. Thus, these games showed the hallmarks of strong conventionalization to terms that were more likely to be opaque to outsiders. 

We recruited 196 participants (99 in the yoked condition and 97 in shuffled) who each saw all 72 trials of one of the 10 games. This experiment was pre-registered at [this anonymized link](https://osf.io/zqwp5/?view_only=87420dc86f0a4a56a395cd464aa3a5c1). Participants read the transcripts in a modified self-paced reading procedure where they uncovered the text word-by-word (revealed words stayed visible); only after uncovering the entire transcript could participants select an image. We do not analyze the reading time data here.

```{r fig-yoked, fig.env="figure", fig.pos = "t", fig.align = "center", out.width="90%", fig.width=5, fig.height=3, fig.cap = "Accuracies for Experiment 3. Error bars are bootstrapped 95% CIs. \\label{yoked}" }
expt_4_acc_data <- expt_4_data |>
  filter(type == "selection") |>
  mutate(correct = as.numeric(correct)) |>
  select(workerid, correct, orig_repNum, round, condition, matcher_trialNum, gameId, correct_tangram, source)

expt_3_relevant_games <- expt_4_acc_data |>
  select(gameId, round) |>
  unique()
expt_3_fig_data <- expt_4_acc_data |>
  bind_rows(mlp_mod |> inner_join(expt_3_relevant_games)) |>
  bind_rows(original_results |> inner_join(expt_3_relevant_games)) |>
  mutate(condition = ifelse(!is.na(condition), condition, source)) |>
  mutate(round = str_sub(round, -1)) |>
  mutate(condition = factor(condition, levels = c("original", "yoked", "shuffled", "model"), labels = c("Original", "Yoked", "Shuffled", "CLIP-MLP")))


predicted_expt_3_fig <- read_rds(here(mod_loc, "predicted/acc_3.rds")) |>
  bind_rows(read_rds(here(mod_loc, "predicted/acc_3_mlp.rds")) |> mutate(condition = "model")) |>
  mutate(across(mean:high, inv_logit_scaled)) |>
  mutate(round = orig_repNum + 1) |>
  mutate(condition = factor(condition, levels = c("original", "yoked", "shuffled", "model"), labels = c("Original", "Yoked", "Shuffled", "CLIP-MLP")))

predicted_expt_3_fig_no_trial <- read_rds(here(mod_loc, "predicted/acc_3_no_trial.rds")) |>
  bind_rows(read_rds(here(mod_loc, "predicted/acc_3_mlp.rds")) |> mutate(condition = "model")) |>
  mutate(across(mean:high, inv_logit_scaled)) |>
  mutate(round = orig_repNum + 1) |>
  mutate(condition = factor(condition, levels = c("original", "yoked", "shuffled", "model"), labels = c("Original", "Yoked", "Shuffled", "CLIP-MLP")))

expt_3_fig_data |> ggplot(aes(x = round, color = condition, y = correct, group = condition)) +
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .6), geom = "line") +
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .6)) +
  # geom_pointrange(data = predicted_expt_3_fig, aes(y = mean, ymax = high, ymin = low), position=position_dodge(.6)) +
  #geom_line(data = predicted_expt_3_fig, aes(y = mean), position=position_dodge(.6), lty="dashed") +
  #   geom_pointrange(data = predicted_expt_3_fig_no_trial |> filter(condition!="CLIP-MLP"), aes(y = mean, ymax = high, ymin = low), position=position_dodge(.6)) +
 # geom_line(data = predicted_expt_3_fig_no_trial |> filter(condition!="CLIP-MLP"), aes(y = mean), position=position_dodge(.6), lty="dotted") +
  scale_y_continuous(lim = c(0, 1), expand = c(0, 0)) +
  geom_hline(yintercept = 1 / 12, lty = "dashed") +
  labs(x = "Round", y = "Accuracy") +
  scale_color_manual(breaks = c("Original", "Yoked", "Shuffled", "CLIP-MLP"), values = c("#7570B3", "#E7298A", "#1B9E77", "#D95F02")) +
  theme(
    legend.position = "bottom", legend.box = "vertical",
    strip.background = element_blank(),
    legend.title = element_blank(),
    legend.margin = margin(c(-10, 1, 0, 1))
  )
```


```{r}
acc_mod_4 <- read_rds(here(mod_results, "acc_4.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_mod_4_me <- read_rds(here(mod_me, "acc_4.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_4_form <- read_rds(here(mod_form, "acc_4.rds"))

acc_mlp_4 <- read_rds(here(mod_results, "acc_yoked_mlp_beta.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_mlp_4_me <- read_rds(here(mod_me, "acc_yoked_mlp_beta.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_4_mlp_form <- read_rds(here(mod_form, "acc_yoked_mlp_beta.rds"))


acc_compare <- read_rds(here(mod_results, "yoked_shuffled_original.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_compare_me <- read_rds(here(mod_me, "yoked_shuffled_original.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_4_form_compare <- read_rds(here(mod_form, "yoked_shuffled_original.rds"))
```

## Results and discussion

Our primary question of interest was how much seeing the conversation history unfold in order would help participants interpret descriptions, especially those from later rounds.

We compared accuracy across the yoked and shuffled conditions with a Bayesian mixed-effects logistic regression.^[`r form(acc_4_form)`]. The descriptions were more transparent when they were presented in a yoked order (OR = `r stats_text(acc_mod_4, 2)`, Figure \ref{yoked}). In the shuffled condition, there was no main effect of round number (OR for one round later = `r stats_text(acc_mod_4, 4)`), but there was a marginal interaction where the benefit of the yoked condition decreased for later rounds (OR for one round later = `r stats_text(acc_mod_4, 5)`). This was offset by matchers in both conditions improving at the task over time (OR for one trial later in matcher viewing order = `r stats_text(acc_mod_4, 3)`). <!--In the yoked condition round and trial number were aligned, so an improvement over time could be either from matcher practice or from descriptions being easier to understand. In the shuffled condition, matcher practice effects did not correlate with position in the original game. -->

Comparing to the performance of in-game matchers, we separated out the benefits of seeing the descriptions in order versus being a participant in the group.^[`r form(acc_4_form_compare)`] There was a benefit to seeing the items in order (OR = `r stats_text(acc_compare, 3)`) and a larger benefit to being a participant during the game (OR = `r stats_text(acc_compare, 7)`). The benefit of seeing the items in order waned in later blocks (OR = `r stats_text(acc_compare, 5)`), but the benefit of being in the game did not (OR = `r stats_text(acc_compare, 6)`). In all cases, there was a baseline improvement over trials (OR = `r stats_text(acc_compare, 2)`). As a caveat, we note that in-game matchers and naïve matchers may have varied from each other in terms of effort and time spent on the task, and thus the comparison should be interpreted cautiously.

The accuracy of the CLIP-MLP model was worse than the shuffled human results, and did not change across rounds (OR for one round later = `r stats_text(acc_mlp_4, 2)`). The larger difference between naïve human and CLIP-MLP accuracies in Experiment 3 than Experiment 2 suggests that the shuffled ordering still provides useful context that helps matchers understand the conventions. This history was not available to the CLIP-MLP model which saw every description as a one-shot task. 


# General Discussion 

Real-world conventions vary in whether they are opaque to outsiders ("rizz") or interpretable even to those who don't produce them ("roundabout"). Convention formation in the real world is difficult to study, so iterated reference games are a method for operationalizing convention formation for experimental study. In reference games, conventions are partner-specific: different groups' evolving conventions follow different paths through semantic space. Despite the number of studies on iterated reference games, few studies have examined whether the descriptions are interpretable by outsiders. 

Across multiple experiments, we found that naïve human matchers were far above chance at identifying the targets, and our computational model was also above chance. For both humans and models, more variation was explained by the target image than the round or game condition the descriptions came from, suggesting that conventionalization was not the primary driver of how difficult an expression was to interpret. Even for games selected for strong conventionalization, naïve matchers had high accuracy overall, although this accuracy was further increased if they saw the conversation history in order. Exploratory analyses also suggested that more idiosyncratic descriptions were more opaque. Our findings are consistent with a lexical uncertainty approach, where expressions that are closer to overall priors are easier to understand, and groups that have fewer people and thicker channels are more able to break away from these priors and have conventions that drift farther apart in semantic space. 
 
 <!-- We also tested a computational model built on CLIP with a multi-layer perceptron readout as a way of approximating the semantic distance between descriptions and images. The CLIP-MLP model was far above chance in its assignment of probabilities to target images and correlated with human accuracies, although its probabilities were lower than human matcher accuracies.  -->


<!-- This work suggests that conventions formed within a small group may still be fairly comprehensible to those outside the group, as the semantics of these conventionalized expressions may still be relatively close to the semantics of the image. Hence, such conventions may not be completely arbitrary, and are instead strongly related to the image and the shared general lexicon, although it is possible that greater arbitrariness and opacity may be obtained with more extended interaction. -->
<!--, who may produce different descriptions themselves. This finding raises questions around how well-calibrated describers are to their matcher's level of knowledge, and whether the process of convention formation is actually efficient. Even naïve matchers can often understand the shorthand descriptions, especially when there has not been a lot of semantic drift, but in reference games, describers choose elaborated descriptions with new matchers. In a game, norms of cooperation and conversation may lead describers to start new matchers with elaborated descriptions designed to give them a high level of confidence in target selection. Describers are also constrained by their need to come up with a description in real time. However, the high level of understanding and the lack of substantial benefit from early round descriptions does raise empirical questions about how calibrated describers are to the level of information necessary. -->

Limiting the generality of our findings, our experimental and computational results were only on a specific set of iterated reference game transcripts and images. Some images may lend themselves to more transparent descriptions because they are more iconic, with a narrower prior over different ways they could be conceptualized, or they may be further from competitors within this pool of images. Future work sampling across larger sets of images [such as @ji2022] could probe image-level factors. <!--Future work could also explore within-description sources of variation and how the structure and word choice of utterance correlates with naïve matcher accuracy. Computational models could be especially beneficial because they could be run on subsets or ablations of descriptive text. -->

Nonetheless, this work has demonstrated the utility of adopting a broader perspective on convention comprehension. In particular, the use of computational modelling enabled estimating semantics under lexical uncertainty without symbolic semantic representations. Future work could capitalize on this approach to better understand semantic dynamics underlying convention formation, as well as provide further quantitative investigations of pragmatics models like RSA and CHAI. These directions will help us to better understand the nature of reference and conventions, and how humans navigate the complex and ever-evolving landscape of communication.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
