---
title: "TODO"
bibliography: library.bib
csl: apa7.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Veronica Boyce (vboyce@stanford.edu)} \\ Department of Psychology, \\Stanford University \And {\large \bf Ben (TODO email) } Department of Psychology, \\Stanford University
    \AND {\large \bf Alvin (TODO email)} \\ TODO affiliation, Stanford University \And {\large \bf Michael C. Frank (mcfrank@stanford.edu)} \\ Department of Psychology, \\ Stanford University}

abstract: >
    TODO abstract
    
keywords: >
    TODO keywords
    
output: cogsci2024::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(
  fig.width = 3, fig.height = 3, fig.crop = F,
  fig.pos = "tb", fig.path = "figs/",
  echo = F, warning = F, cache = F,
  message = F, sanitize = T
)

library(png)
library(grid)
library(ggplot2)
library(xtable)
library(tidyverse)
library(here)
library(brms)
library(rstan)
library(rstanarm)
library(ggthemes)
library(jsonlite)
library(ggthemes)
library(scales)
library(viridis)
library(ggridges)
library(cowplot)
library(tidybayes)

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
theme_set(theme_bw())

data_loc <- "data"
mod_loc <- "analysis-code/models"
images <- "experiments/expt1/assets/images"
prediction_loc <- "model-code/model_predictions"
mod_results <- "analysis-code/models/summary"
mod_form <- "analysis-code/models/formulae"
mod_me <- "analysis-code/models/mixed_fx"


expt_1_data <- read_csv(here(data_loc, "expt1_full_data.csv")) |>
  select(-proliferate.condition) |>
  filter(!is.na(response)) |>
  filter(!is.na(correct_tangram)) |>
  select(
    workerid, button_rt, condition, correct, correct_tangram,
    gameId, selected, text, trial_index
  ) |>
  mutate(workerid = as.factor(workerid)) |>
  mutate(rt_sec = button_rt / 1000) |>
  separate(condition, c("group_size", NA, "round")) |>
  mutate(
    group_size = str_c(group_size, "_player"),
    round = str_c("round_", round),
    correct = as.numeric(correct)
  ) |>
  group_by(workerid) |>
  mutate(trial_order = row_number()) |>
  ungroup() |>
  mutate(source = "naive", expt = "Expt 2", thickness = "medium")

expt_2_data <- read_csv(here(data_loc, "expt2_full_data.csv")) |>
  select(-proliferate.condition) |>
  filter(!is.na(response)) |>
  filter(!is.na(correct_tangram)) |>
  select(
    workerid, button_rt, condition, correct, correct_tangram,
    gameId, selected, text, trial_index
  ) |>
  mutate(workerid = as.factor(workerid)) |>
  mutate(rt_sec = button_rt / 1000) |>
  separate(condition, c("group_size", "thickness", "round")) |>
  mutate(
    condition = str_c(group_size, "_", thickness),
    group_size = str_c(group_size, "_player"),
    round = str_c("round_", round),
    correct = as.numeric(correct)
  ) |>
  group_by(workerid) |>
  mutate(trial_order = row_number()) |>
  ungroup() |>
  mutate(source = "naive", expt = "Expt 2")


expt_3_data <- read_csv(here(data_loc, "tgmatchercalibration-trials.csv")) |>
  select(-proliferate.condition) |>
  filter(!is.na(response)) |>
  filter(!is.na(correct_tangram)) |>
  select(
    workerid, button_rt, correct, correct_tangram,
    gameId, selected, text, trial_index
  ) |>
  mutate(workerid = as.factor(workerid)) |>
  mutate(
    rt_sec = button_rt / 1000,
    correct = as.numeric(correct)
  ) |>
  group_by(workerid) |>
  mutate(trial_order = row_number()) |>
  ungroup() |>
  mutate(source = "naive")



expt_4_data <- read_csv(here(data_loc, "tgmatcheryoked-trials.csv")) |>
  select(-proliferate.condition) |>
  filter(!is.na(correct_tangram)) |>
  select(
    workerid, button_rt, correct, correct_tangram, condition,
    gameId, selected, text, trial_index, type, rt, orig_trialNum, orig_repNum
  ) |>
  mutate(workerid = as.factor(workerid)) |>
  mutate(
    matcher_trialNum = (trial_index - 3) %/% 3,
    matcher_repNum = matcher_trialNum %/% 12
  ) |>
  mutate(workerid = ifelse(workerid == "3157" & condition == "yoked", "3157a", workerid)) |> # somehow two participants were assigned to 3157 -- but each set looks complete?
  filter(workerid != "141") |>
  filter(workerid != "35") |> # exclude two participants who didn't finish
  mutate(source = "naive", round = str_c("round_", orig_repNum + 1))


mlp_mod <- read_csv(here(prediction_loc, "mlp_best.csv")) |>
  pivot_longer(p_A:p_L) |>
  mutate(name = str_sub(name, -1)) |>
  filter(tangram == name) |>
  mutate(
    round = str_c("round_", repNum + 1),
    correct = value,
    source = "model"
  ) |>
  select(correct_tangram = tangram, gameId, round, correct, source)


# Original study accuracies
url <- "https://raw.githubusercontent.com/vboyce/multiparty-tangrams/main/"


one_round_results <- read_rds(str_c(url, "data/study1/round_results.rds")) %>% mutate(rotate = "rotate")
two_a_round_results <- read_rds(str_c(url, "data/study2a/round_results.rds")) %>% mutate(rotate = "no_rotate")
two_b_round_results <- read_rds(str_c(url, "data/study2b/round_results.rds")) %>% mutate(rotate = "full_feedback")
two_c_round_results <- read_rds(str_c(url, "data/study2c/round_results.rds")) |> mutate(rotate = "emoji")
three_round_results <- read_rds(str_c(url, "data/study3/round_results.rds")) |> rename(`_id` = "X_id", condition = name)

one_chat <- read_csv(str_c(url, "data/study1/filtered_chat.csv")) |> mutate(rotate = str_c(as.character(numPlayers), "_rotate"))
two_a_chat <- read_csv(str_c(url, "data/study2a/filtered_chat.csv")) |> mutate(rotate = "no_rotate")
two_b_chat <- read_csv(str_c(url, "data/study2b/filtered_chat.csv")) |>
  mutate(rotate = "full_feedback") |>
  select(-`row num`)
two_c_chat <- read_csv(str_c(url, "data/study2c/filtered_chat.csv")) |>
  mutate(rotate = "emoji") |>
  select(-type)
three_chat <- read_csv(str_c(url, "data/study3/filtered_chat.csv")) |>
  inner_join(read_rds(str_c(url, "data/study3/round_results.rds")) |> select(gameId, trialNum, condition = name) |> unique()) |>
  select(-rowid, -type)

original_results_raw <- one_round_results |>
  rbind(two_a_round_results) |>
  rbind(two_b_round_results) |>
  rbind(two_c_round_results) |>
  mutate(activePlayerCount = NA) |>
  rename(condition = rotate) |>
  rbind(three_round_results) |>
  mutate(
    round = str_c("round_", repNum + 1),
    correct_tangram = tangram,
    correct = ifelse(correct, 1, 0),
    source = "original"
  )

original_results <- original_results_raw |>
  group_by(gameId, correct_tangram, round, source) |>
  summarize(correct = mean(correct)) |>
  select(gameId, correct_tangram, round, correct, source)



original_length <- one_chat |>
  rbind(two_a_chat) |>
  rbind(two_b_chat) |>
  rbind(two_c_chat) |>
  mutate(activePlayerCount = NA) |>
  rename(condition = rotate) |>
  rbind(three_chat) |>
  filter(!is.chitchat) |>
  filter(role == "speaker") |>
  mutate(correct_tangram = str_sub(target, -5, -5)) |>
  group_by(repNum, gameId, correct_tangram, condition, numPlayers) |>
  mutate(utt_length_words = str_count(spellchecked, "\\W+") + 1) %>%
  summarize(
    text = paste0(text, collapse = ", "),
    total_num_words = sum(utt_length_words, na.rm = T) %>% as.numeric(),
    log_words = log(total_num_words)
  ) |>
  mutate(round = str_c("round_", repNum + 1)) |>
  select(gameId, correct_tangram, round, total_num_words, log_words)
```

```{r, eval=F}
do_preds_tangram_2 <- function(model) {
  mod <- here(mod_loc, model) |> read_rds()
  preds <- expand_grid(
    trial_order = 1:60, group_size = c("6_player", "2_player"),
    thickness = c("thick", "thin"),
    round = c("round_1", "round_6"),
    correct_tangram = c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L")
  ) |>
    #add_linpred_draws(mod, value = "predicted", re_formula = NA) |>
    add_linpred_draws(mod, value = "predicted", re_formula = ~ (group_size *thickness * round | correct_tangram)) |>
    group_by(group_size, thickness, round, correct_tangram) |>
    # group_by(group_size, round, correct_tangram) |>
    summarize(
      mean = mean(predicted),
      low = quantile(predicted, .025),
      high = quantile(predicted, .975)
    )
  return(preds)
}

#pred_acc_2 <- do_preds_tangram_2("acc_2.rds") |> write_rds(here(mod_loc, "predicted", "acc_2.rds"))

pred_acc_2 <- do_preds_tangram_2("acc_2.rds") |> write_rds(here(mod_loc, "predicted", "acc_2_tangram.rds"))

do_preds_tangram_1 <- function(model) {
  mod <- here(mod_loc, model) |> read_rds()
  preds <- expand_grid(
    trial_order = 1:60, group_size = c("6_player", "2_player"),
    # thickness = c("thick", "thin"),
    round = c("round_1", "round_6"),
    correct_tangram = c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L")
  ) |>
     add_linpred_draws(mod, value = "predicted", re_formula = ~ (group_size * round | correct_tangram)) |>
    #add_linpred_draws(mod, value = "predicted", re_formula = NA) |>
    # group_by(group_size, thickness, round, correct_tangram) |>
    group_by(group_size, round, correct_tangram) |>
    summarize(
      mean = mean(predicted),
      low = quantile(predicted, .025),
      high = quantile(predicted, .975)
    )
  return(preds)
}

#pred_acc_1 <- do_preds_tangram_1("acc_1.rds") |> write_rds(here(mod_loc, "predicted", "acc_1.rds"))

pred_acc_1 <- do_preds_tangram_1("acc_1.rds") |> write_rds(here(mod_loc, "predicted", "acc_1_tangram.rds"))

do_preds_tangram_mlp <- function(model) {
  mod <- here(mod_loc, model) |> read_rds()
  preds <- expand_grid(
    trial_order = 1:60, group_size = c("6_player", "2_player"),
    thickness = c("thick", "thin", "medium"),
    round = c("round_1", "round_6"),
    correct_tangram = c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L")
  ) |>
    #add_linpred_draws(mod, value = "predicted", re_formula = NA) |>
    add_linpred_draws(mod, value = "predicted", re_formula = ~ (group_size *thickness * round | correct_tangram)) |>
    group_by(group_size, thickness, round, correct_tangram) |>
    # group_by(group_size, round, correct_tangram) |>
    summarize(
      mean = mean(predicted),
      low = quantile(predicted, .025),
      high = quantile(predicted, .975)
    )
  return(preds)
}

do_preds_mlp <- function(model) {
  mod <- here(mod_loc, model) |> read_rds()
  preds <- expand_grid(
    trial_order = 1:60, group_size = c("6_player", "2_player"),
    thickness = c("thick", "thin", "medium"),
    round = c("round_1", "round_6"),
    correct_tangram = c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L")
  ) |>
    add_linpred_draws(mod, value = "predicted", re_formula = NA) |>
    #add_linpred_draws(mod, value = "predicted", re_formula = ~ (group_size *thickness * round | correct_tangram)) |>
    group_by(group_size, thickness, round, correct_tangram) |>
    # group_by(group_size, round, correct_tangram) |>
    summarize(
      mean = mean(predicted),
      low = quantile(predicted, .025),
      high = quantile(predicted, .975)
    )
  return(preds)
}

 do_preds_tangram_mlp("acc_mlp_1_2_beta.rds") |> write_rds(here(mod_loc, "predicted", "acc_mlp_beta_tangram.rds"))
 do_preds_mlp("acc_mlp_1_2_beta.rds") |> write_rds(here(mod_loc, "predicted", "acc_mlp_beta.rds"))


do_preds_tangram_yoked <- function(model) {
  mod <- here(mod_loc, model) |> read_rds()
  preds <- expand_grid(
    matcher_trialNum = 1:72, 
    orig_repNum= 0:5, 
    condition=c("yoked", "shuffled"),
    correct_tangram = c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L")
  ) |>
     add_linpred_draws(mod, value = "predicted", re_formula = NA) |>
    # group_by(group_size, thickness, round, correct_tangram) |>
    group_by(orig_repNum, condition) |>
    summarize(
      mean = mean(predicted),
      low = quantile(predicted, .025),
      high = quantile(predicted, .975)
    )
  return(preds)
}
  do_preds_tangram_yoked("acc_4.rds") |> write_rds(here(mod_loc, "predicted", "acc_4.rds"))

  do_preds_tangram_yoked_mlp <- function(model) {
  mod <- here(mod_loc, model) |> read_rds()
  preds <- expand_grid(
    matcher_trialNum = 1:72, 
    orig_repNum= 0:5, 
    #condition=c("yoked", "shuffled"),
    correct_tangram = c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L")
  ) |>
     add_linpred_draws(mod, value = "predicted", re_formula =NA) |>
    # group_by(group_size, thickness, round, correct_tangram) |>
    #group_by(orig_repNum, condition, correct_tangram) |>
        group_by(orig_repNum) |>

    summarize(
      mean = mean(predicted),
      low = quantile(predicted, .025),
      high = quantile(predicted, .975)
    )
  return(preds)
}
  do_preds_tangram_yoked_mlp("acc_yoked_mlp_beta.rds") |> write_rds(here(mod_loc, "predicted", "acc_yoked_mlp_beta.rds"))



```

```{r, eval=F}
library(tidybayes)

save_summary <- function(model) {
  intervals <- gather_draws(model, `b_.*`, regex = T) %>% mean_qi()

  stats <- gather_draws(model, `b_.*`, regex = T) %>%
    mutate(above_0 = ifelse(.value > 0, 1, 0)) %>%
    group_by(.variable) %>%
    summarize(pct_above_0 = mean(above_0)) %>%
    left_join(intervals, by = ".variable") %>%
    mutate(
      lower = .lower,
      upper = .upper,
      Term = str_sub(.variable, 3, -1),
      Estimate = .value
    ) %>%
    select(Term, Estimate, lower, upper)

  stats
}

save_me <- function(model) {
  intervals <- gather_draws(model, `sd_.*`, regex = T) %>%
    mean_qi() |>
    separate(.variable, into = c("group", "Term"), sep = "__") |>
    mutate(
      lower = .lower,
      upper = .upper,
      group = str_sub(group, 4, -1),
      Estimate = .value
    ) %>%
    select(group, Term, Estimate, lower, upper)

  intervals
}

do_model <- function(path) {
  model <- read_rds(here(mod_loc, path))
  save_summary(model) |> write_rds(here(mod_loc, "summary", path))
  model$formula |> write_rds(here(mod_loc, "formulae", path))
  print(summary(model))
}

do_me <- function(path) {
  model <- read_rds(here(mod_loc, path))
  message(path)
  save_me(model) |> write_rds(here(mod_loc, "mixed_fx", path))
}

mods_me <- c(
  "acc_1.rds", "acc_2.rds", "acc_4.rds", "acc_mlp_1_2_beta.rds", "acc_yoked_mlp_beta.rds", "yoked_shuffled_original.rds"
) |> walk(~ do_me(.))


mods <- list.files(path = here(mod_loc), pattern = ".*rds") |> walk(~ do_model(.))
```

```{r}
stats <- function(model, row, decimal = 2) {
  model <- model |>
    mutate(
      Estimate = round(Estimate, digits = decimal),
      Lower = round(lower, digits = decimal),
      Upper = round(upper, digits = decimal),
      `Credible Interval` = str_c("[", Lower, ", ", Upper, "]")
    ) |>
    select(Term, Estimate, `Credible Interval`)
  str_c(model[row, 1], ": ", model[row, 2], " ", model[row, 3])
}

stats_text <- function(model, row, decimal = 2) {
  model <- model |>
    mutate(
      Estimate = round(Estimate, digits = decimal),
      Lower = round(lower, digits = decimal),
      Upper = round(upper, digits = decimal),
      `Credible Interval` = str_c("[", Lower, ", ", Upper, "]")
    ) |>
    select(Term, Estimate, `Credible Interval`)
  str_c(model[row, 2], "  ", model[row, 3])
}

form <- function(model_form) {
  dep <- as.character(model_form$formula[2])
  ind <- as.character(model_form$formula[3])

  str_c(dep, " ~ ", ind) |>
    str_replace_all(" ", "") |>
    str_replace_all("\\*", " $\\\\times$ ") |>
    str_replace_all("\\+", "&nbsp;+ ") |>
    str_replace_all("~", "$\\\\sim$ ")
}
```


# Introduction

Conversation pacts and partner specificity are often studied by looking at how they are constructed; an additional perspective comes from how opaque or interpretable they are to outsiders who weren’t part of the pact

By measuring opaqueness in different conditions related to how the pacts were formed or what the language looks like, can get another perspective on the process of pact formation

An empirical test of partner - specificity 

Prior work to cover
Summary of ref games & claims around them @hawkins2020b @clark1986 etc

The side-participant / overhearer etc literature @wilkes-gibbs1992, & lit search for more

Judy’s work Visual resemblance and interaction history jointly constrain pictorial meaning @hawkinsb

possible could also mention other times when naive comprehender has been used to better understand iteractive dialogues?

“Why use models” – can frame opaqueness as semantic distance between utterance and referent – models are an explicit test of this! 
“Naive comprehender” / “matcher” 

## ALvin gets to write computational intro here
Do we also want to motivate this from a computational angle? (i.e. trying to add pragmatics to models) TODO not me

## back to Veronica 
Key question: What properties of conversational pacts and the process of their formation make them more or less easy for an outsider to understand? 

We use both human experiments and models to assess when and why expressions are opaque or understandable to outside observers. 




# Task Setup

## Materials

We draw on the corpus of reference game transcripts and results from @boyce2024. There were all 6 round iterated reference games using the same 12 target images, but varied in how large the groups were (2-6 participants per group) and how "thick" the communication channel between group members was. For our human experiments, we sample different subsets of this corpus in different experiments. We use the entire corpus for our computational modelling component. 
TODO discussion of @boyce2024 and how this is useful 

## Experimental procedure 
We recruited participants from Prolific (TODO criteria). Participants were directed to the experiment, where it was explained that previously, other participants had described these shapes to one another. They would see a series of transcripts from the prior game, and their task was the guess what the intended target was.
On each trial participants saw the full transcript from that trial, containing all the chat messages marked by whether they were from the speaker or a listener (TODO confirm the language used), except for lines that @boyce2024 had marked as not having any referential content. Participants selected the image they thought was the target from the tableau of 12. Participants recieved feedback on whether they were right or wrong on each trial. Except when the specific viewing order was part of the experimental manipulation, we randomized the order of trials, subject to the constraint that the same target could not repeat on adjacent trials.  
The task was implemented in jsPsych. We paid participants $10 an hour plus a bonus of TODO look up per correct response. 

```{r interface, fig.env = "figure*", fig.pos = "t!", out.width="\\textwidth", fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Experimental Setup and Procedure.TODO \\label{game}", cache=FALSE}
message("placeholder.png")
```

## Computational models that Ben gets to write

Computational methods
TODO V doesn’t know how to write this
QUESTION: do we focus on mlp pre- or post- calibration?


# Calibration expt that Alvin gets to write
```{r}
# this is all because I didn't keep the trial number source recorded, so we have a fun time rejoining
ParseJSONColumn <- function(x) {
  str_replace_all(x, "'", '"') %>%
    str_replace_all('Don"t know', "Don't know") %>%
    str_replace_all('don"t', "don't") |>
    str_replace_all("None", '"NA"') |>
    str_replace_all('"SAFE"', "'SAFE'") |>
    str_replace_all('he"s', "he's") |>
    str_replace_all('doesn"t', "doesn't") |>
    str_replace_all('hasn"t', "hasn't") |>
    str_replace_all('it"s', "it's") |>
    str_replace_all('It"s', "It's") |>
    str_replace_all('X" shaped', "'X' shaped") |>
    str_replace_all('"missing', "missing") |>
    str_replace_all('"flying"', "flying") |>
    str_replace_all('"skating"', "skating") |>
    str_replace_all('"partially sitting"', "partially sitting") |>
    str_replace_all('"kneeling" and ', "kneeling and ") |>
    str_replace_all('"partially kneeling"', "partially kneeling") |>
    str_replace_all('and "bunny"', "and bunny") |>
    str_replace_all('"square"', "square") |>
    str_replace_all('heads"', "heads") |>
    str_replace_all('"italy"', "italy") |>
    str_replace_all('they"re', "they're") |>
    str_replace_all('"but why"', "'but why'") |>
    fromJSON(flatten = T)
}
labels <- read_csv(here("expt_prep_code/labelled.csv")) |>
  mutate(text = str_replace_all(text, "'", "") |> str_replace_all('"', "")) |>
  group_by(tangram, gameId, trialNum, repNum, value, grouping) |>
  summarize(text = str_c(text, collapse = " "))

ready <- expt_3_data |>
  mutate(new = map(text, ParseJSONColumn)) |>
  select(-text) |>
  unnest(new) |>
  mutate(text = text |> str_replace_all("'", ""), tangram = correct_tangram) |>
  group_by(workerid, correct, tangram, gameId, trial_order) |>
  summarize(text = str_c(text, collapse = " "))

good <- ready |> left_join(labels)



for_corr <- good |>
  group_by(text, value, tangram, grouping) |>
  summarize(human_acc = mean(correct), human_n = n()) |> ungroup() |> 
  mutate(grouping=as.factor(grouping) |> reorder(value)) |> group_by(grouping)

summ <- for_corr|>
  group_by(grouping) |>
  mutate(value = mean(value))

#cor.test(for_corr$value, for_corr$human_acc)
```

```{r fig-calibration, fig.env="figure", fig.pos = "t", fig.align = "center", out.width="70%", fig.width=3, fig.height=3, fig.cap = "Correlation between human and CLIP-MLP accuracy across deciles of CLIP-MLP accuracy. Colored points are individual descriptions, black line is the boostrapped mean and 95% CI across descriptions for each decile. \\label{calibration}" }
for_corr |> ggplot(aes(y = human_acc, x = value)) +
  geom_point(aes(color = grouping), alpha=.7) +
  #geom_line(data = summ, color = "black") +
  stat_summary(data=summ)+
    stat_summary(data=summ, fun.data="mean_cl_boot",geom="line")+
  theme(legend.position = "none") +
  scale_color_viridis(discrete=T)+
  labs(x = "CLIP-MLP accuracy", y = "Human accuracy")+
  coord_equal()
```

TODO methods

pre-reg at https://osf.io/6pv5e

modeling approach and selection 

how well can models proxy humans? are there differences? 

tg-matcher 3 (calibration)
61 participants, 64 items each, from a pool of 217 transcripts spanning the models full accuracy range

# Experiment 2

As a starting point for examining what makes referential expressions more or less opaque, we had people read the descriptions from the beginnings and ends and games.  Thus, we would be able to check whether early descriptions (before much partner- or group- specific history could accumulate, but also before a "good" description had been created) or late descriptions (after both history and practice) would be easier to understand. 

## Methods

### Experiment 2a
To test our methods and establish a baseline of how well new matchers could do from reading random transcripts out of order, we ran a 2x2 within subjects design, where we drew the target transcripts from 2 and 6 player games from Experiment 1 of @boyce2024 and from the first and last (sixth) blocks of these games. These games had medium thick communication channels, in that the matchers could send textual messages to the shared chat interface, but the describer role rotated each round, and matchers recieved feedback only about whether their selection was correct or not.
We recruited XX participants in May 2024 (check) who each saw 60 trials (15 in each condition). This experiment was pre-registered at https://osf.io/k45dr. 

### Experiment 2b
After observing small condition differences in experiment 2a, we ran a second study drawing from the more extreme conditions of Experiment 3 in @boyce2024. Here, we used a  2x2x2 within subjects design, drawing our transcripts from the "thick" and "thin", 2 and 6 person, 1st and last block utterances. The "thick" condition had a consistent describer throughout the entire game, let matchers send messages to the chat freely, and showed all participants feedback on everyone's selections, including what the correct answer was. In contrast, the "thin" condition, rotated the role of describer, gave feedback only on individual selections, and describers could not send text messages to the chat, but could only communicate via 4 emoji buttons (to indicate level of understanding). Emojis did not have referential content and so were not included in the transcripts be showed naive matchers. 
For experiment 2b, we recruited XX participants in DATE who each saw 64 trials (8 in each condition). This expeirment was pre-registered at  https://osf.io/rdp5k. 


## Results

Our primary question of interest was how accurate naive matchers would be at selecting the correct target, and how much this would vary depending on what game and what round the description came from. We did not have clear predictions, as we thought there could be countervailing pressures. On the one hand, the idea of reduction and partner-specificity would suggest that the conventionalized, later round utterances would rely on the history of the game that naive matchers were not privy to, and thus that late round utterances would be more opaque and difficult to understand. On the other hand, describers gained practice over repetitions, so later round utterances might better pick up on visually salient features and include less extraneous verbiage. 

In terms of group conditions, based on the patterns of cross-game similarity in @boyce2024, we thought that smaller and thicker games were more likely to rapidly develop ideosyncratic conventions that would be more opaque than the less ideosyncratic conventions from larger groups with thinner communication channels. 

```{r}
expt_2_relevant_games <- expt_1_data |>
  bind_rows(expt_2_data) |>
  select(gameId, group_size, thickness, round) |>
  unique()

expt_2_fig_data <- expt_1_data |>
  bind_rows(expt_2_data) |>
  bind_rows(mlp_mod |> inner_join(expt_2_relevant_games)) |>
  bind_rows(original_results |> inner_join(expt_2_relevant_games)) |>
  mutate(thickness = factor(thickness, levels = c("thin", "medium", "thick"))) |> 
  mutate(source=factor(source, levels=c("original", "naive", "model"), labels=c("Original", "Naive human", "CLIP-MLP"))) |> 
  mutate(round=str_sub(round, -1))


predicted_expt_2_fig <- read_rds(here(mod_loc, "predicted/acc_1.rds")) |>
  mutate(thickness = "medium") |>
  bind_rows(read_rds(here(mod_loc, "predicted/acc_2.rds"))) |>
  mutate(source = "naive") |> 
  bind_rows(read_rds(here(mod_loc, "predicted/acc_mlp_beta.rds")) |> mutate(source="model")) |> 
    mutate(source=factor(source, levels=c("original", "naive", "model"), labels=c("Original", "Naive human", "CLIP-MLP"))) |> 
  mutate(across(mean:high, inv_logit_scaled)) |> 
    mutate(thickness = factor(thickness, levels = c("thin", "medium", "thick"))) |> 
    mutate(round=str_sub(round, -1))


tangram_predicted_expt_2_fig <- read_rds(here(mod_loc, "predicted/acc_1_tangram.rds")) |>
  mutate(thickness = "medium") |>
  bind_rows(read_rds(here(mod_loc, "predicted/acc_2_tangram.rds"))) |>
  mutate(source = "naive") |> 
    bind_rows(read_rds(here(mod_loc, "predicted/acc_mlp_beta_tangram.rds")) |> mutate(source="model")) |> 
    mutate(source=factor(source, levels=c("original", "naive", "model"), labels=c("Original", "Naive human", "CLIP-MLP"))) |> 
  mutate(across(mean:high, inv_logit_scaled)) |>   mutate(thickness = factor(thickness, levels = c("thin", "medium", "thick"))) |> 
    mutate(round=str_sub(round, -1))


```

```{r fig-condition, fig.env="figure", fig.pos = "t", fig.align = "center", out.width="90%", fig.width=4, fig.height=3, fig.cap = "Accuracies for naive humans and the CLIP-MLP model for Experiment 2. Point estimates and 95% CrI are predictions from the fixed effects of logistic and beta regressions. Bootstrapped mean accuracy from the original matchers is included as a ceiling, and random chance as a baseline. \\label{expt2-condition}" }
expt_2_fig_data |> ggplot(aes(x = round, color = source, y = correct, group = interaction(source, group_size), shape=group_size, lty=group_size)) +
    stat_summary(data= expt_2_fig_data |> filter(source=="Original"),  geom="point", fun.data="mean_cl_boot", position=position_dodge(.4))+
    stat_summary(data= expt_2_fig_data |> filter(source=="Original"), geom="line", fun.data="mean_cl_boot", position=position_dodge(.4))+
  geom_pointrange(data = predicted_expt_2_fig, aes(y = mean, ymax = high, ymin = low), position=position_dodge(.4)) +
  geom_line(data = predicted_expt_2_fig, aes(y = mean), position=position_dodge(.4)) +
  facet_grid(. ~ thickness) +
  scale_y_continuous(lim = c(0, 1), expand = c(0, 0)) +
  geom_hline(yintercept = 1 / 12, lty = "dashed") +
  labs(x = "Round", y = "Accuracy")+
  scale_color_manual(breaks=c("Original", "Naive human", "CLIP-MLP"), values=c( "#7570B3","#1B9E77", "#D95F02"))+
  theme(legend.position="bottom", legend.box="vertical", 
        strip.background=element_blank(),
        legend.title=element_blank(),
        legend.margin=margin(c(-10,1,0,1)))
```


```{r fig-2, fig.env="figure", fig.pos = "t", fig.align = "center", out.width="90%", fig.width=4, fig.height=3, fig.cap = "Accuracies for naive humans and the CLIP-MLP model for Experiment 2, split out by target image. Point estimates and 95% CI are predictions from the fixed effects and by-tangram random effects of logistic and beta regressions, bootstrapped across conditions. Bootstrapped mean accuracy from the original matchers is included as a ceiling, and random chance as a baseline. \\label{TODO2}" }
library(ggtext)
correct_tangram <- c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L")
labels <- c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L") |> map(~ str_c("<img src=", here(images, str_c("tangram_", ., ".png")), " width='20'/>"))


acc_by_target <- expt_1_data |>
  bind_rows(expt_2_data) |>
  group_by(correct_tangram) |>
  summarize(acc = sum(correct) / n()) |>
  arrange(acc)

foo <- tibble(correct_tangram, labels) |>
  left_join(acc_by_target) |>
  arrange(acc)

acc_by_type_target <- expt_2_fig_data |>
  group_by(correct_tangram, group_size, round, thickness, source) |>
  summarize(acc = sum(correct) / n()) |>
  mutate(correct_tangram = factor(correct_tangram, levels = acc_by_target$correct_tangram))

ggplot(acc_by_type_target, aes(x = correct_tangram, y = acc, color=source, shape=group_size, lty=group_size)) +
   stat_summary(data = tangram_predicted_expt_2_fig, aes(y = mean, ymax = high, ymin = low), fun.data="mean_cl_boot", position=position_dodge(width=.6)) +
  stat_summary(data=acc_by_type_target |> filter(source=="Original"), aes(color = source), geom="point", position=position_dodge(width=.6)) +
  coord_cartesian(ylim = c(0, 1), xlim = c(.5, 12.5), expand = F) +
  geom_hline(yintercept = 1 / 12, linetype = "dashed") +
  scale_x_discrete(name = NULL, labels = foo$labels) +
  scale_color_manual(breaks=c("Original", "Naive human", "CLIP-MLP"), values=c( "#7570B3","#1B9E77", "#D95F02"))+
  theme(legend.position="bottom", legend.box="vertical", 
        strip.background=element_blank(),
        legend.title=element_blank(),
        axis.text.x = element_markdown(color = "black", size = 11),
        legend.margin=margin(c(-10,1,0,1)))


# guide_legend()# acc_by_target
```




```{r, eval=F}
acc_priors <- c(
  set_prior("normal(0,1)", class = "b"),
  set_prior("normal(0,1)", class = "sd"),
  set_prior("lkj(1)", class = "cor")
)


acc_mod_1 <- brm(
  correct ~ group_size * round + trial_order +
    (group_size * round | correct_tangram) +
    (group_size * round + trial_order | workerid),
  data = expt_1_data,
  family = bernoulli(),
  file = here(mod_loc, "acc_1"),
  prior = acc_priors,
  control = list(adapt_delta = .95)
)

# summary(acc_mod_1)
```


```{r, eval=F}
acc_mod_2 <- brm(
  correct ~ group_size * thickness * round + trial_order +
    (group_size * thickness * round | correct_tangram) +
    (group_size * thickness * round + trial_order | workerid),
  data = expt_2_data,
  family = bernoulli(),
  file = here(mod_loc, "acc_2"),
  prior = acc_priors,
  control = list(adapt_delta = .95)
)


# summary(acc_mod_2)
```

```{r}
original_acc <- original_results |>
  rename(original_correct = correct) |>
  ungroup() |>
  select(round, correct_tangram, gameId, original_correct)

expt_1_data_augment <- expt_1_data |>
  left_join(original_length) |>
  left_join(original_acc)
expt_2_data_augment <- expt_2_data |>
  left_join(original_length) |>
  left_join(original_acc)
```

```{r, eval=F}
acc_mod_1_orig_acc <- brm(
  correct ~ original_correct + group_size * round + trial_order +
    (group_size * round | correct_tangram) +
    (group_size * round + trial_order | workerid),
  data = expt_1_data_augment,
  family = bernoulli(),
  file = here(mod_loc, "acc_1_orig_acc"),
  prior = acc_priors,
  control = list(adapt_delta = .95)
)

acc_mod_1_orig_length <- brm(
  correct ~ log_words + group_size * round + trial_order +
    (group_size * round | correct_tangram) +
    (group_size * round + trial_order | workerid),
  data = expt_1_data_augment,
  family = bernoulli(),
  file = here(mod_loc, "acc_1_orig_length"),
  prior = acc_priors,
  control = list(adapt_delta = .95)
)

acc_mod_2_orig_acc <- brm(
  correct ~ original_correct + group_size * thickness * round + trial_order +
    (group_size * thickness * round | correct_tangram) +
    (group_size * thickness * round + trial_order | workerid),
  data = expt_2_data_augment,
  family = bernoulli(),
  file = here(mod_loc, "acc_2_orig_acc"),
  prior = acc_priors,
  control = list(adapt_delta = .95)
)

acc_mod_2_orig_length <- brm(
  correct ~ log_words + group_size * thickness * round + trial_order +
    (group_size * thickness * round | correct_tangram) +
    (group_size * thickness * round + trial_order | workerid),
  data = expt_2_data_augment,
  family = bernoulli(),
  file = here(mod_loc, "acc_2_orig_length"),
  prior = acc_priors,
  control = list(adapt_delta = .95)
)
```


```{r}

acc_mod_1 <- read_rds(here(mod_results, "acc_1.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))


acc_mod_1_me <- read_rds(here(mod_me, "acc_1.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
acc_1_form <- read_rds(here(mod_form, "acc_1.rds"))

acc_mod_2 <- read_rds(here(mod_results, "acc_2.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_2_form <- read_rds(here(mod_form, "acc_2.rds"))

acc_mod_2_me <- read_rds(here(mod_me, "acc_2.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_mod_1_orig_acc <- read_rds(here(mod_results, "acc_1_orig_acc.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_mod_2_orig_acc <- read_rds(here(mod_results, "acc_2_orig_acc.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_mod_1_orig_length <- read_rds(here(mod_results, "acc_1_orig_length.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_mod_2_orig_length <- read_rds(here(mod_results, "acc_2_orig_length.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_mod_mlp <- read_rds(here(mod_results, "acc_mlp_1_2.rds"))
acc_mod_mlp_me <- read_rds(here(mod_me, "acc_mlp_1_2.rds"))

acc_mod_mlp_orig_acc <- read_rds(here(mod_results, "acc_mlp_1_2_orig_acc.rds"))

acc_mod_mlp_orig_length <- read_rds(here(mod_results, "acc_mlp_1_2_orig_length.rds"))


```

For Experiment 2a, we ran a mixed effects logistic model of naive matcher accuracy: `r form(acc_1_form)`. Overall, naive matchers were above chance (OR: `r stats_text(acc_mod_1, 1)`. There were not large fixed effects (FIGURE TODO)  Last round was slightly lower accuracy (OR of last round: `r stats_text(acc_mod_1, 4)`). There was not a clear effect of larger games (OR: `r stats_text(acc_mod_1, 2)`), but round and group size interacted (OR of 6 player games in round 6: `r stats_text(acc_mod_1, 3)`).  Much of the variation in accuracy was instead driven by the target images, where the Odds Ratio of the standard deviation of the target was `r stats_text(acc_mod_1_me, 3)` [I'm not sure what the clearest way to present a stat for this is at all...] with some images recieving much more transparent descriptions than others (FIGURE). 

For Experiment 2b we ran a similar mixed effects logistic model: `r form(acc_2_form)`. Overall, naive matchers were above chance (OR: `r stats_text(acc_mod_2, 1)`. Again, there was not a clear pattern of fixed effects. Last round was slightly lower accuracy (OR of last round: `r stats_text(acc_mod_2, 6)`), but there was an interaction with thickness, where thin, last round were less opaque (OR: `r stats_text(acc_mod_2, 8)`). TODO do we want to describe some of the other predictors of this model? 

and again some of the uncertainty in estimating the fixed effects was driven by the strong variation by target image (TODO STATS). `r stats_text(acc_mod_2_me, 5)`

As additional measures, we also looked whether there was predictive value from the level of accuracy of the original matchers or the number of words used by the original describer TODO.

EXPT 1 -- there is a large effect of original accuracy `r stats_text(acc_mod_1_orig_acc, 4)`
EXPT 1 -- there is not a substnatial effect of original length `r stats_text(acc_mod_1_orig_length, 4)`

EXPT 2 -- there is a large effect of original accuracy `r stats_text(acc_mod_2_orig_acc, 6)`
EXPT 2 -- there is a slight benefit to longer descriptions `r stats_text(acc_mod_2_orig_length, 6)`


## Model results

As a computational comparison, we looked at the CLIP-MLP model's performance on the same descriptions. We used the probability the model assigned as a measure of the model's accuracy. The CLIP-MLP model was far above chance, but had lower accuracy than the human participants. 

TODO model results 

Nothing is significant basically except that thick is better `r stats_text(acc_mod_mlp, 9)`. 

There is substantial by-tangram variation `r stats_text(acc_mod_mlp_me, 7)`

There is an effect of original correct `r stats_text(acc_mod_mlp_orig_acc, 8)`. 

More words is bad for the model -- effect of log words `r stats_text(acc_mod_mlp_orig_length, 8)`

```{r, eval=F}
acc_priors_mlp <- c(
  set_prior("normal(0,.2)", class = "b"),
  set_prior("normal(0,.2)", class = "sd"),
  set_prior("lkj(1)", class = "cor")
)


acc_priors <- c(
  set_prior("normal(0,1)", class = "b"),
  set_prior("normal(0,1)", class = "sd"),
  set_prior("lkj(1)", class = "cor")
)

expt_2_relevant_games <- expt_1_data |>
  bind_rows(expt_2_data) |>
  select(gameId, group_size, thickness, round) |>
  unique()

expt_2_mlp <- mlp_mod |>
  inner_join(expt_2_relevant_games) |>
  left_join(original_length) |>
  left_join(original_acc)

acc_mod_mlp <- brm(
  correct ~ group_size * thickness * round +
    (group_size * thickness * round | correct_tangram),
  data = expt_2_mlp,
  file = here(mod_loc, "acc_mlp_1_2"),
  prior = acc_priors_mlp,
  control = list(adapt_delta = .95)
)

acc_mod_mlp <- brm(
  correct ~ group_size * thickness * round +
    (group_size * thickness * round | correct_tangram),
  data = expt_2_mlp,
  family=Beta(link="logit"),
  file = here(mod_loc, "acc_mlp_1_2_beta"),
  prior = acc_priors,
  control = list(adapt_delta = .95)
)

acc_mod_mlp_orig_acc <- brm(
  correct ~ original_correct + group_size * thickness * round +
    (group_size * thickness * round | correct_tangram),
  data = expt_2_mlp,
    family=Beta(link="logit"),
  file = here(mod_loc, "acc_mlp_1_2_orig_acc_beta"),
  prior = acc_priors,
  control = list(adapt_delta = .95)
)

acc_mod_mlp_orig_length <- brm(
  correct ~ log_words + group_size * thickness * round +
    (group_size * thickness * round | correct_tangram),
  data = expt_2_mlp,
    family=Beta(link="logit"),
  file = here(mod_loc, "acc_mlp_1_2_orig_length_beta"),
  prior = acc_priors,
  control = list(adapt_delta = .95)
)
```


# Yoked v shuffled 

TODO methods 
pre-reg at https://osf.io/zqwp5

tg-matcher 4 (SPR + yoked/unyoked)
196 participants (99 in yoked, 97 in shuffled), each saw all 72 trials from 1 of 10 games. 
games not chosen at random

## Human yoked v not yoked expt

Yoked v shuffled plot of accuracy (+ model?)
Human accuracy on yoked v shuffled presentation (expt 4) 
(?) no context model comparison on expt 4 dataset?



seeing things in the same order helps
look at item level accuracy differences? 

We will exclude individual word RTs that are greater than 2000 ms. 

Condition differences: condition refers to yoked or shuffled.

Logistic model of target selection accuracy: Accuracy ~ original_rep_num * condition + viewing_order + (1 | gameId) + (1 | tangram) + (1 | participant)


This dataset was collected using a modified self-paced reading procedure, but for present purposes, we focus only on the selection results and not on the incremental reading time patterns.



```{r fig-yoked, fig.env="figure", fig.pos = "t", fig.align = "center", out.width="100%", fig.width=5, fig.height=3, fig.cap = "TODO also TODO add model comparison ?? \\label{yoked}" }



expt_4_acc_data <- expt_4_data |>
  filter(type == "selection") |>
  mutate(correct = as.numeric(correct)) |>
  select(workerid, correct, orig_repNum, round, condition, matcher_trialNum, gameId, correct_tangram, source)

expt_3_relevant_games <- expt_4_acc_data |>
  select(gameId, round) |>
  unique()
expt_3_fig_data <- expt_4_acc_data |>
  bind_rows(mlp_mod |> inner_join(expt_3_relevant_games)) |>
  bind_rows(original_results |> inner_join(expt_3_relevant_games)) |>
  mutate(condition = ifelse(!is.na(condition), condition, source)) |> 
  mutate(round=str_sub(round, -1)) |> 
    mutate(condition=factor(condition, levels=c("original", "yoked", "shuffled", "model"), labels=c("Original", "Yoked", "Shuffled", "CLIP-MLP"))) 


predicted_expt_3_fig <- read_rds(here(mod_loc, "predicted/acc_4.rds")) |>
  bind_rows(read_rds(here(mod_loc, "predicted/acc_yoked_mlp_beta.rds")) |> mutate(condition="model")) |> mutate(across(mean:high, inv_logit_scaled))|> mutate(round= orig_repNum+1) |> 
      mutate(condition=factor(condition, levels=c("original", "yoked", "shuffled", "model"), labels=c("Original", "Yoked", "Shuffled", "CLIP-MLP"))) 

expt_3_fig_data |> ggplot(aes(x = round, color = condition, y = correct, group = condition)) +
  stat_summary( fun.data = "mean_cl_boot", position = position_dodge(width = .6), geom = "line") +
  stat_summary( fun.data = "mean_cl_boot", position = position_dodge(width = .6)) +
  #geom_pointrange(data = predicted_expt_3_fig, aes(y = mean, ymax = high, ymin = low), position=position_dodge(.6)) +
  #geom_line(data = predicted_expt_3_fig, aes(y = mean), position=position_dodge(.6)) +
  scale_y_continuous(lim = c(0, 1), expand = c(0, 0)) +
  geom_hline(yintercept = 1 / 12, lty = "dashed") +
  labs(x = "Round", y = "Accuracy")+
  scale_color_manual(breaks=c("Original", "Yoked", "Shuffled", "CLIP-MLP"), values=c( "#7570B3","#E7298A","#1B9E77", "#D95F02"))+
  theme(legend.position="bottom", legend.box="vertical", 
        strip.background=element_blank(),
        legend.title=element_blank(),
        legend.margin=margin(c(-10,1,0,1)))
```
TODO looks like the models may not have fit super well, especially given that we force block effects linear? 

```{r, eval=F}
library(ggtext)
correct_tangram <- c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L")
labels <- c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L") |> map(~ str_c("<img src=", here(images, str_c("tangram_", ., ".png")), " width='20'/>"))


acc_by_target <- expt_4_acc_data |>
  group_by(correct_tangram) |>
  summarize(acc = sum(correct) / n()) |>
  arrange(acc)

foo <- tibble(correct_tangram, labels) |>
  left_join(acc_by_target) |>
  arrange(acc)

acc_by_type_target <- expt_4_acc_data |>
  group_by(correct_tangram, orig_repNum, condition) |>
  summarize(acc = sum(correct) / n()) |>
  arrange(acc)

ggplot(acc_by_type_target, aes(x = reorder(correct_tangram, acc), y = acc, )) +
  geom_point() +
  stat_summary(color = "red") +
  coord_cartesian(ylim = c(0, 1), xlim = c(.5, 12.5), expand = F) +
  geom_hline(yintercept = 1 / 12, linetype = "dashed") +
  scale_x_discrete(name = NULL, labels = foo$labels) +
  theme(axis.text.x = element_markdown(color = "black", size = 11))


# acc_by_target
```

```{r, eval=F}
for_acc_mod_4 <- expt_4_data |>
  filter(type == "selection") |>
  mutate(correct = as.numeric(correct)) |>
  select(workerid, correct, orig_repNum, condition, matcher_trialNum, gameId, correct_tangram)

acc_priors <- c(
  set_prior("normal(0, 1)", class = "b"),
  set_prior("normal(0,1)", class = "sd")
)


acc_mod_4 <- brm(correct ~ orig_repNum * condition + matcher_trialNum + (1 | gameId) + (1 | correct_tangram) + (1 | workerid), family = bernoulli(link = "logit"), data = for_acc_mod, prior = acc_priors, file = here(mod_loc, "acc_4.rds"))
```


```{r, eval=F}
acc_priors_mlp <- c(
  set_prior("normal(0,1)", class = "b"),
  set_prior("normal(0,1)", class = "sd")
)

expt_3_relevant_games <- expt_4_acc_data |>
  select(gameId, round, orig_repNum) |>
  unique()

yoked_relevant_mlp <- mlp_mod |> inner_join(expt_3_relevant_games)


acc_mod_yoked_mlp <- brm(correct ~ orig_repNum + (1 | gameId) + (1 | correct_tangram),
  data = yoked_relevant_mlp,
  prior = acc_priors_mlp,
    family=Beta(link="logit"),
  file = here(mod_loc, "acc_yoked_mlp_beta"),
  control = list(adapt_delta = .95)
)
```

not sure how to work model in b/c different scale ... 

```{r, eval=F}
original_subset <- original_results_raw |>
  inner_join(expt_3_relevant_games) |>
  mutate(order = "yoked", setting = "original") |>
  select(workerid = playerId, correct, correct_tangram = tangram, orig_repNum = repNum, gameId, matcher_trialNum = trialNum, order, setting)

yoked_shuffled_original <- expt_4_data |>
  filter(type == "selection") |>
  mutate(correct = as.numeric(correct)) |>
  select(workerid, correct, orig_repNum, condition, matcher_trialNum, gameId, correct_tangram) |>
  mutate(order = condition, setting = "new") |>
  bind_rows(original_subset)

acc_priors <- c(
  set_prior("normal(0, 1)", class = "b"),
  set_prior("normal(0,1)", class = "sd")
)


acc_mod_compare_orig <- brm(correct ~ orig_repNum * order + orig_repNum * setting + matcher_trialNum + (1 | gameId) + (1 | correct_tangram) + (1 | workerid), family = bernoulli(link = "logit"), data = yoked_shuffled_original, prior = acc_priors, file = here(mod_loc, "yoked_shuffled_original.rds"))
```


# Discussion ? 


## Part of discussion that Alvin gets to write??

Discussion

Understanding varies much more based on item than on anything else; potentially due to priors or iconicity of image (? that might be beyond scope – how well does this match up with say diversity of descriptions)

Models do pretty well? IDK what our model take away is

Especially when there is strong or idiosyncratic reduction, context helps

role of context 

limitations, incuding out of distribution for models 

might want to address language comprehension v inference 

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
