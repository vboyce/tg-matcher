@article{bergen2016,
  title = {Pragmatic Reasoning through Semantic Inference},
  author = {Bergen, Leon and Levy, Roger and Goodman, Noah},
  year = {2016},
  month = may,
  journal = {Semantics and Pragmatics},
  volume = {9},
  pages = {20:1-91},
  issn = {1937-8912},
  doi = {10.3765/sp.9.20},
  urldate = {2025-02-01},
  abstract = {A number of recent proposals have used techniques from game theory and Bayesian cognitive science to formalize Gricean pragmatic reasoning (Frank \&amp; Goodman, 2012; Franke, 2009; Goodman \&amp; Stuhlmuller, 2013; Jager, 2012). We discuss two phenomena which pose a challenge to these accounts of pragmatics: M-implicatures (Horn, 1984) and embedded implicatures which violate Hurford's constraint (Chierchia, Fox, \&amp; Spector, 2012; Hurford, 1974). While techniques have been developed for deriving M-implicatures, Hurford-violating em- bedded implicatures pose a more fundamental challenge, because of basic limitations in the models' architecture. In order to explain these phenomena, we propose a realignment of the division between semantic content and pragmatic content. Under this proposal, the semantic content of an utterance is not fixed independent of pragmatic inference; rather, pragmatic inference partially determines an utterance's semantic content. We show how semantic inference can be realized as an extension to the Rational Speech Acts framework (Goodman \&amp; Stuhlmuller, 2013). The addition of lexical uncertainty derives both M-implicatures and the relevant embedded implicatures, and preserves the derivations of more standard implicatures. We use this principle to explain a novel class of implicature, non-convex disjunctive implicatures, which have several theoretically interesting properties. In particular, these implicatures can be preserved in downward-entailing contexts in the absence of accenting, a property which is predicted by lexical uncertainty, but which violates prior generalizations in the literature (Fox \&amp; Spector, in press; Horn, 1989). BibTeX info},
  copyright = {Copyright (c) 2016 Leon Bergen, Roger Levy, Noah Goodman},
  langid = {english},
  keywords = {Bayesian modeling,Division of pragmatic labor,Embedded implicatures,Game theory,Hurford's constraint,Pragmatics},
  file = {/home/vboyce/Zotero/storage/F7ZQRAD2/Bergen et al. - 2016 - Pragmatic reasoning through semantic inference.pdf}
}

@article{bohn2022,
  title = {Modeling {{Individual Differences}} in {{Children}}'s {{Information Integration During Pragmatic Word Learning}}},
  author = {Bohn, Manuel and Schmidt, Louisa S. and Schulze, Cornelia and Frank, Michael C. and Tessler, Michael Henry},
  year = {2022},
  month = dec,
  journal = {Open Mind},
  volume = {6},
  pages = {311--326},
  issn = {2470-2986},
  doi = {10.1162/opmi_a_00069},
  urldate = {2025-02-01},
  abstract = {Pragmatics is foundational to language use and learning. Computational cognitive models have been successfully used to predict pragmatic phenomena in adults and children -- on an aggregate level. It is unclear if they can be used to predict behavior on an individual level. We address this question in children (N = 60, 3- to 5-year-olds), taking advantage of recent work on pragmatic cue integration. In Part 1, we use data from four independent tasks to estimate child-specific sensitivity parameters to three information sources: semantic knowledge, expectations about speaker informativeness, and sensitivity to common ground. In Part 2, we use these parameters to generate participant-specific trial-by-trial predictions for a new task that jointly manipulated all three information sources. The model accurately predicted children's behavior in the majority of trials. This work advances a substantive theory of individual differences in which the primary locus of developmental variation is sensitivity to individual information sources.},
  file = {/home/vboyce/Zotero/storage/NCKWLS6B/Bohn et al. - 2022 - Modeling Individual Differences in Childrenâ€™s Information Integration During Pragmatic Word Learning.pdf;/home/vboyce/Zotero/storage/BRJYR7TP/Modeling-Individual-Differences-in-Children-s.html}
}

@misc{boyce2024,
  title = {Interaction Structure Constrains the Emergence of Conventions in Group Communication},
  author = {Boyce, Veronica and Hawkins, Robert and Goodman, Noah D. and Frank, Michael C.},
  year = {2024},
  urldate = {2024-04-03},
  copyright = {All rights reserved},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/BLRBTIJU/Boyce et al. - 2024 - Interaction structure constrains the emergence of conventions in group communication.pdf;/home/vboyce/Zotero/storage/HWTBNZZ4/a3wfy.html}
}

@article{burkner2018,
  title = {Advanced Bayesian Multilevel Modeling with the r Package Brms},
  author = {B{\"u}rkner, Paul-Christian},
  year = {2018},
  journal = {The R Journal},
  volume = {10},
  number = {1},
  pages = {395--411},
  keywords = {read}
}

@inproceedings{chen2016xgboost,
  title = {Xgboost: {{A}} Scalable Tree Boosting System},
  booktitle = {Proceedings of the 22nd Acm Sigkdd International Conference on Knowledge Discovery and Data Mining},
  author = {Chen, Tianqi and Guestrin, Carlos},
  year = {2016},
  pages = {785--794}
}

@article{clark1986,
  title = {Referring as a Collaborative Process},
  author = {Clark, Herbert H and {Wilkes-Gibbs}, Deanna},
  year = {1986},
  urldate = {2020-10-06},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/H3FGM5CL/clark-and-wilkes-gibbs-1986.pdf}
}

@article{clark1987a,
  title = {Concealing One's Meaning from Overhearers},
  author = {Clark, Herbert H and Schaefer, Edward F},
  year = {1987},
  month = apr,
  journal = {Journal of Memory and Language},
  volume = {26},
  number = {2},
  pages = {209--225},
  issn = {0749596X},
  doi = {10.1016/0749-596X(87)90124-0},
  urldate = {2025-01-13},
  langid = {english},
  file = {/home/vboyce/Zotero/storage/HNKUI249/Clark and Schaefer - 1987 - Concealing one's meaning from overhearers.pdf}
}

@article{frank2012a,
  ids = {frankPredictingPragmaticReasoning2012a},
  title = {Predicting {{Pragmatic Reasoning}} in {{Language Games}}},
  author = {Frank, M. C. and Goodman, N. D.},
  year = {2012},
  month = may,
  journal = {Science},
  volume = {336},
  number = {6084},
  pages = {998--998},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1218633},
  urldate = {2020-09-18},
  langid = {english},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/EPQPW3KN/Frank and Goodman - 2012 - Predicting Pragmatic Reasoning in Language Games.pdf;/home/vboyce/Zotero/storage/J2DJZ9ES/Frank and Goodman - 2012 - Predicting Pragmatic Reasoning in Language Games.pdf}
}

@article{frank2014,
  title = {Inferring Word Meanings by Assuming That Speakers Are Informative},
  author = {Frank, Michael C. and Goodman, Noah D.},
  year = {2014},
  month = dec,
  journal = {Cognitive Psychology},
  volume = {75},
  pages = {80--96},
  issn = {0010-0285},
  doi = {10.1016/j.cogpsych.2014.08.002},
  urldate = {2025-02-01},
  abstract = {Language comprehension is more than a process of decoding the literal meaning of a speaker's utterance. Instead, by making the assumption that speakers choose their words to be informative in context, listeners routinely make pragmatic inferences that go beyond the linguistic data. If language learners make these same assumptions, they should be able to infer word meanings in otherwise ambiguous situations. We use probabilistic tools to formalize these kinds of informativeness inferences---extending a model of pragmatic language comprehension to the acquisition setting---and present four experiments whose data suggest that preschool children can use informativeness to infer word meanings and that adult judgments track quantitatively with informativeness.},
  keywords = {Bayesian models,Language acquisition,Pragmatics,Word learning},
  file = {/home/vboyce/Zotero/storage/K2TF78HN/S0010028514000589.html}
}

@article{goodman2016,
  ids = {goodmanPragmaticLanguageInterpretation2016a},
  title = {Pragmatic {{Language Interpretation}} as {{Probabilistic Inference}}},
  author = {Goodman, Noah D. and Frank, Michael C.},
  year = {2016},
  month = nov,
  journal = {Trends in Cognitive Sciences},
  volume = {20},
  number = {11},
  pages = {818--829},
  issn = {13646613},
  doi = {10.1016/j.tics.2016.08.005},
  urldate = {2020-07-07},
  langid = {english},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/CZM5IU8S/Goodman and Frank - 2016 - Pragmatic Language Interpretation as Probabilistic.pdf;/home/vboyce/Zotero/storage/YIHG78I9/Goodman and Frank - 2016 - Pragmatic Language Interpretation as Probabilistic.pdf}
}

@article{grand2022,
  title = {Semantic Projection Recovers Rich Human Knowledge of Multiple Object Features from Word Embeddings},
  author = {Grand, Gabriel and Blank, Idan Asher and Pereira, Francisco and Fedorenko, Evelina},
  year = {2022},
  month = jul,
  journal = {Nature human behaviour},
  volume = {6},
  number = {7},
  pages = {975--987},
  issn = {2397-3374},
  doi = {10.1038/s41562-022-01316-8},
  urldate = {2025-01-13},
  abstract = {How is knowledge about word meaning represented in the mental lexicon? Current computational models infer word meanings from lexical co-occurrence patterns. They learn to represent words as vectors in a multidimensional space, wherein words that are used in more similar linguistic contexts---that is, are more semantically related---are located closer together. However, whereas inter-word proximity captures only overall relatedness, human judgements are highly context dependent. For example, dolphins and alligators are similar in size but differ in dangerousness. Here, we use a domain-general method to extract context-dependent relationships from word embeddings: `semantic projection' of word-vectors onto lines that represent features such as size (the line connecting the words `small' and `big') or danger (`safe' to `dangerous'), analogous to `mental scales'. This method recovers human judgements across various object categories and properties. Thus, the geometry of word embeddings explicitly represents a wealth of context-dependent world knowledge.},
  pmcid = {PMC10349641},
  pmid = {35422527},
  file = {/home/vboyce/Zotero/storage/4X7VV3KN/Grand et al. - 2022 - Semantic projection recovers rich human knowledge of multiple object features from word embeddings.pdf}
}

@misc{gul2024,
  title = {{{CoGen}}: {{Learning}} from {{Feedback}} with {{Coupled Comprehension}} and {{Generation}}},
  shorttitle = {{{CoGen}}},
  author = {Gul, Mustafa Omer and Artzi, Yoav},
  year = {2024},
  month = aug,
  number = {arXiv:2408.15992},
  eprint = {2408.15992},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.15992},
  urldate = {2024-11-26},
  abstract = {Systems with both language comprehension and generation capabilities can benefit from the tight connection between the two. This work studies coupling comprehension and generation with focus on continually learning from interaction with users. We propose techniques to tightly integrate the two capabilities for both learning and inference. We situate our studies in two-player reference games, and deploy various models for thousands of interactions with human users, while learning from interaction feedback signals. We show dramatic improvements in performance over time, with comprehension-generation coupling leading to performance improvements up to 26\% in absolute terms and up to 17\% higher accuracies compared to a non-coupled system. Our analysis also shows coupling has substantial qualitative impact on the system's language, making it significantly more human-like.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/vboyce/Zotero/storage/ERIB3RWD/Gul and Artzi - 2024 - CoGen Learning from Feedback with Coupled Comprehension and Generation.pdf;/home/vboyce/Zotero/storage/5KYLWGD5/2408.html}
}

@article{hawkins2020b,
  title = {Characterizing the Dynamics of Learning in Repeated Reference Games},
  author = {Hawkins, Robert D. and Frank, Michael C. and Goodman, Noah D.},
  year = {2020},
  month = apr,
  journal = {arXiv:1912.07199 [cs]},
  eprint = {1912.07199},
  primaryclass = {cs},
  urldate = {2020-07-15},
  abstract = {The language we use over the course of conversation changes as we establish common ground and learn what our partner finds meaningful. Here we draw upon recent advances in natural language processing to provide a finer-grained characterization of the dynamics of this learning process. We release an open corpus ({$>$}15,000 utterances) of extended dyadic interactions in a classic repeated reference game task where pairs of participants had to coordinate on how to refer to initially difficult-to-describe tangram stimuli. We find that different pairs discover a wide variety of idiosyncratic but efficient and stable solutions to the problem of reference. Furthermore, these conventions are shaped by the communicative context: words that are more discriminative in the initial context (i.e. that are used for one target more than others) are more likely to persist through the final repetition. Finally, we find systematic structure in how a speaker's referring expressions become more efficient over time: syntactic units drop out in clusters following positive feedback from the listener, eventually leaving short labels containing open-class parts of speech. These findings provide a higher resolution look at the quantitative dynamics of ad hoc convention formation and support further development of computational models of learning in communication.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,read},
  file = {/home/vboyce/Zotero/storage/M69FCDUW/Hawkins et al. - 2020 - Characterizing the Dynamics of Learning in Repeate.pdf;/home/vboyce/Zotero/storage/5PZ8QT5M/1912.html}
}

@article{hawkins2021,
  ids = {hawkinsRespectCodeSpeakersa},
  title = {Respect the Code: {{Speakers}} Expect Novel Conventions to Generalize within but Not across Social Group Boundaries},
  author = {Hawkins, Robert D and Liu, Irina and Goldberg, Adele E and Griffiths, Thomas G},
  year = {2021},
  journal = {CogSci},
  abstract = {Speakers use different language to communicate with partners in different communities. But how do we learn and represent which conventions to use with which partners? In this paper, we argue that solving this challenging computational problem requires speakers to supplement their lexical representations with knowledge of social group structure. We formalize this idea by extending a recent hierarchical Bayesian model of convention formation with an intermediate layer explicitly representing the latent communities each partner belongs to, and derive predictions about how conventions formed within a group ought to extend to new in-group and out-group members. We then present evidence from two behavioral experiments testing these predictions using a minimal group paradigm. Taken together, our findings provide a first step toward a formal framework for understanding the interplay between language use and social group knowledge.},
  langid = {english},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/45PXV4ZM/Hawkins et al. - Respect the code Speakers expect novel convention.pdf;/home/vboyce/Zotero/storage/NNGQE3WH/Hawkins et al. - Respect the code Speakers expect novel convention.pdf}
}

@misc{hawkins2021a,
  title = {From Partners to Populations: {{A}} Hierarchical {{Bayesian}} Account of Coordination and Convention},
  shorttitle = {From Partners to Populations},
  author = {Hawkins, Robert D. and Franke, Michael and Frank, Michael C. and Goldberg, Adele E. and Smith, Kenny and Griffiths, Thomas L. and Goodman, Noah D.},
  year = {2021},
  month = dec,
  number = {arXiv:2104.05857},
  eprint = {2104.05857},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.05857},
  urldate = {2022-11-09},
  abstract = {Languages are powerful solutions to coordination problems: they provide stable, shared expectations about how the words we say correspond to the beliefs and intentions in our heads. Yet language use in a variable and non-stationary social environment requires linguistic representations to be flexible: old words acquire new ad hoc or partner-specific meanings on the fly. In this paper, we introduce CHAI (Continual Hierarchical Adaptation through Inference), a hierarchical Bayesian theory of coordination and convention formation that aims to reconcile the long-standing tension between these two basic observations. We argue that the central computational problem of communication is not simply transmission, as in classical formulations, but continual learning and adaptation over multiple timescales. Partner-specific common ground quickly emerges from social inferences within dyadic interactions, while community-wide social conventions are stable priors that have been abstracted away from interactions with multiple partners. We present new empirical data alongside simulations showing how our model provides a computational foundation for several phenomena that have posed a challenge for previous accounts: (1) the convergence to more efficient referring expressions across repeated interaction with the same partner, (2) the gradual transfer of partner-specific common ground to strangers, and (3) the influence of communicative context on which conventions eventually form.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,read,useful},
  file = {/home/vboyce/Zotero/storage/2VC3CPM6/Hawkins et al. - 2021 - From partners to populations A hierarchical Bayes.pdf}
}

@article{hawkins2023a,
  title = {Visual Resemblance and Interaction History Jointly Constrain Pictorial Meaning},
  author = {Hawkins, Robert D. and Sano, Megumi and Goodman, Noah D. and Fan, Judith E.},
  year = {2023},
  month = apr,
  journal = {Nature Communications},
  volume = {14},
  number = {1},
  pages = {2199},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-023-37737-w},
  urldate = {2025-01-09},
  abstract = {How do drawings---ranging from detailed illustrations to schematic diagrams---reliably convey meaning? Do viewers understand drawings based on how strongly they resemble an entity (i.e., as images) or based on socially mediated conventions (i.e., as symbols)? Here we evaluate a cognitive account of pictorial meaning in which visual and social information jointly support visual communication. Pairs of participants used drawings to repeatedly communicate the identity of a target object among multiple distractor objects. We manipulated social cues across three experiments and a full replication, finding that participants developed object-specific and interaction-specific strategies for communicating more efficiently over time, beyond what task practice or a resemblance-based account alone could explain. Leveraging model-based image analyses and crowdsourced annotations, we further determined that drawings did not drift toward ``arbitrariness,'' as predicted by a pure convention-based account, but preserved visually diagnostic features. Taken together, these findings advance psychological theories of how successful graphical conventions emerge.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Human behaviour,Social behaviour},
  file = {/home/vboyce/Zotero/storage/2WYE2TPJ/Hawkins et al. - 2023 - Visual resemblance and interaction history jointly constrain pictorial meaning.pdf}
}

@inproceedings{ji2022,
  title = {Abstract {{Visual Reasoning}} with {{Tangram Shapes}}},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Ji, Anya and Kojima, Noriyuki and Rush, Noah and Suhr, Alane and Vong, Wai Keen and Hawkins, Robert and Artzi, Yoav},
  editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
  year = {2022},
  month = dec,
  pages = {582--601},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.38},
  urldate = {2024-10-01},
  abstract = {We introduce KiloGram, a resource for studying abstract visual reasoning in humans and machines. Drawing on the history of tangram puzzles as stimuli in cognitive science, we build a richly annotated dataset that, with {\textbackslash}textgreater1k distinct stimuli, is orders of magnitude larger and more diverse than prior resources. It is both visually and linguistically richer, moving beyond whole shape descriptions to include segmentation maps and part labels. We use this resource to evaluate the abstract visual reasoning capacities of recent multi-modal models. We observe that pre-trained weights demonstrate limited abstract reasoning, which dramatically improves with fine-tuning. We also observe that explicitly describing parts aids abstract reasoning for both humans and models, especially when jointly encoding the linguistic and visual inputs.},
  file = {/home/vboyce/Zotero/storage/UNBU4Y2Z/Ji et al. - 2022 - Abstract Visual Reasoning with Tangram Shapes.pdf}
}

@inproceedings{kang2020,
  title = {Incorporating {{Pragmatic Reasoning Communication}} into {{Emergent Language}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kang, Yipeng and Wang, Tonghan and {de Melo}, Gerard},
  year = {2020},
  volume = {33},
  pages = {10348--10359},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-01-13},
  abstract = {Emergentism and pragmatics are two research fields that study the dynamics of linguistic communication along quite different timescales and intelligence levels. From the perspective of multi-agent reinforcement learning, they correspond to stochastic games with reinforcement training and stage games with opponent awareness, respectively. Given that their combination has been explored in linguistics, in this work, we combine computational models of short-term mutual reasoning-based pragmatics with long-term language emergentism. We explore this for agent communication in two settings, referential games and Starcraft II, assessing the relative merits of different kinds of mutual reasoning pragmatics models both empirically and theoretically. Our results shed light on their importance for making inroads towards getting more natural, accurate, robust, fine-grained, and succinct utterances.},
  file = {/home/vboyce/Zotero/storage/DJ3XFWKM/Kang et al. - 2020 - Incorporating Pragmatic Reasoning Communication into Emergent Language.pdf}
}

@misc{le2022,
  title = {Referring {{Expressions}} with {{Rational Speech Act Framework}}: {{A Probabilistic Approach}}},
  shorttitle = {Referring {{Expressions}} with {{Rational Speech Act Framework}}},
  author = {Le, Hieu and Daryanto, Taufiq and Zhafransyah, Fabian and Wijaya, Derry and Coppock, Elizabeth and Chin, Sang},
  year = {2022},
  month = may,
  number = {arXiv:2205.07795},
  eprint = {2205.07795},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.07795},
  urldate = {2025-01-13},
  abstract = {This paper focuses on a referring expression generation (REG) task in which the aim is to pick out an object in a complex visual scene. One common theoretical approach to this problem is to model the task as a two-agent cooperative scheme in which a `speaker' agent would generate the expression that best describes a targeted area and a `listener' agent would identify the target. Several recent REG systems have used deep learning approaches to represent the speaker/listener agents. The Rational Speech Act framework (RSA), a Bayesian approach to pragmatics that can predict human linguistic behavior quite accurately, has been shown to generate high quality and explainable expressions on toy datasets involving simple visual scenes. Its application to large scale problems, however, remains largely unexplored. This paper applies a combination of the probabilistic RSA framework and deep learning approaches to larger datasets involving complex visual scenes in a multi-step process with the aim of generating better-explained expressions. We carry out experiments on the RefCOCO and RefCOCO+ datasets and compare our approach with other end-to-end deep learning approaches as well as a variation of RSA to highlight our key contribution. Experimental results show that while achieving lower accuracy than SOTA deep learning methods, our approach outperforms similar RSA approach in human comprehension and has an advantage over end-to-end deep learning under limited data scenario. Lastly, we provide a detailed analysis on the expression generation process with concrete examples, thus providing a systematic view on error types and deficiencies in the generation process and identifying possible areas for future improvements.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/vboyce/Zotero/storage/6EGZC2SY/Le et al. - 2022 - Referring Expressions with Rational Speech Act Framework A Probabilistic Approach.pdf;/home/vboyce/Zotero/storage/SPB4R5AD/2205.html}
}

@article{leeuw2023,
  title = {{{jsPsych}}: {{Enabling}} an {{Open-Source Collaborative Ecosystem}} of {{Behavioral Experiments}}},
  shorttitle = {{{jsPsych}}},
  author = {de Leeuw, Joshua R. and Gilbert, Rebecca A. and Luchterhandt, Bj{\"o}rn},
  year = {2023},
  month = may,
  journal = {Journal of Open Source Software},
  volume = {8},
  number = {85},
  pages = {5351},
  issn = {2475-9066},
  doi = {10.21105/joss.05351},
  urldate = {2025-01-14},
  abstract = {de Leeuw et al., (2023). jsPsych: Enabling an Open-Source Collaborative Ecosystem of Behavioral Experiments. Journal of Open Source Software, 8(85), 5351, https://doi.org/10.21105/joss.05351},
  langid = {english},
  file = {/home/vboyce/Zotero/storage/WSNGWEEV/Leeuw et al. - 2023 - jsPsych Enabling an Open-Source Collaborative Ecosystem of Behavioral Experiments.pdf}
}

@article{leung2024,
  title = {Parents Spontaneously Scaffold the Formation of Conversational Pacts with Their Children},
  author = {Leung, Ashley and Yurovsky, Daniel and Hawkins, Robert D.},
  year = {2024},
  journal = {Child Development},
  volume = {n/a},
  number = {n/a},
  issn = {1467-8624},
  doi = {10.1111/cdev.14186},
  urldate = {2024-12-06},
  abstract = {Adults readily coordinate on temporary pacts about how to refer to things in conversation. Young children are also capable of forming pacts with peers given appropriate experimenter intervention. Here, we investigate whether parents may spontaneously provide a similar kind of scaffolding with U.S. children in a director--matcher task (N = 201, 49\% female; ages 4, 6, 8). In Experiment 1, we show that parents initiate more clarification exchanges with younger children who, in turn, are more likely to adopt labels introduced by the parent. We then examine whether the benefit of such scaffolding acts primarily through childrens' difficulties with comprehension (Experiment 2) or production (Experiment 3). Our findings suggest that parents primarily scaffold pacts by easing children's production difficulties, modeling cooperative communication.},
  langid = {english},
  keywords = {read,useful},
  file = {/home/vboyce/Zotero/storage/J6EMYUCR/Leung et al. - Parents spontaneously scaffold the formation of conversational pacts with their children.pdf;/home/vboyce/Zotero/storage/PS6ATHZ3/cdev.html}
}

@article{misyak2016,
  title = {Instantaneous {{Conventions}}: {{The Emergence}} of {{Flexible Communicative Signals}}},
  shorttitle = {Instantaneous {{Conventions}}},
  author = {Misyak, Jennifer and Noguchi, Takao and Chater, Nick},
  year = {2016},
  month = dec,
  journal = {Psychological Science},
  volume = {27},
  number = {12},
  pages = {1550--1561},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797616661199},
  urldate = {2025-01-24},
  abstract = {Humans can communicate even with few existing conventions in common (e.g., when they lack a shared language). We explored what makes this phenomenon possible with a nonlinguistic experimental task requiring participants to coordinate toward a common goal. We observed participants creating new communicative conventions using the most minimal possible signals. These conventions, furthermore, changed on a trial-by-trial basis in response to shared environmental and task constraints. Strikingly, as a result, signals of the same form successfully conveyed contradictory messages from trial to trial. Such behavior is evidence for the involvement of what we term joint inference, in which social interactants spontaneously infer the most sensible communicative convention in light of the common ground between them. Joint inference may help to elucidate how communicative conventions emerge instantaneously and how they are modified and reshaped into the elaborate systems of conventions involved in human communication, including natural languages.},
  langid = {english},
  file = {/home/vboyce/Zotero/storage/SNQRRLXL/Misyak et al. - 2016 - Instantaneous Conventions The Emergence of Flexible Communicative Signals.pdf}
}

@article{murfitt2001,
  title = {The {{Effect}} of {{Production Variables}} in {{Monolog}} and {{Dialog}} on {{Comprehension}} by {{Novel Listeners}}},
  author = {Murfitt, Tara and McAllister, Jan},
  year = {2001},
  month = sep,
  journal = {Language and Speech},
  volume = {44},
  number = {3},
  pages = {325--350},
  issn = {0023-8309, 1756-6053},
  doi = {10.1177/00238309010440030201},
  urldate = {2025-01-13},
  abstract = {Prior research has identified a number of dimensions along which speakers modify referring expressions. The present study aimed to determine and describe the actual relationshipexisting between these productioncharacteristicsand corresponding measures of listener comprehension.Spoken descriptionswere elicited from eight speakers during monolog and dialog conditions of the tangram task. In a subsequent listening experiment, 163 new subjects listened to the referring expressions from the speech corpus, and were asked to select, from an array of tangram figures, the one that was being described in the referring expression. Results revealed that the production variables most commonly documented by previous researchers collectively contributed to listener comprehension to a surprisingly small, but consistently significant, degree. Furthermore, the communicative context in which referring expressions were produced did not substantially influence their overall comprehensibility; instead, communicative context, as well as the individual characteristics of the speaker who produced the referring expression, appeared to affect which variables listeners found most useful when inferring what the speaker meant.},
  copyright = {https://journals.sagepub.com/page/policies/text-and-data-mining-license},
  langid = {english},
  file = {/home/vboyce/Zotero/storage/UFH3J2M6/Murfitt and McAllister - 2001 - The Effect of Production Variables in Monolog and Dialog on Comprehension by Novel Listeners.pdf}
}

@article{muttenthaler2021,
  title = {{{THINGSvision}}: {{A Python Toolbox}} for {{Streamlining}} the {{Extraction}} of {{Activations From Deep Neural Networks}}},
  shorttitle = {{{THINGSvision}}},
  author = {Muttenthaler, Lukas and Hebart, Martin N.},
  year = {2021},
  month = sep,
  journal = {Frontiers in Neuroinformatics},
  volume = {15},
  publisher = {Frontiers},
  issn = {1662-5196},
  doi = {10.3389/fninf.2021.679838},
  urldate = {2025-01-13},
  abstract = {{$<$}p{$>$}Over the past decade, deep neural network (DNN) models have received a lot of attention due to their near-human object classification performance and their excellent prediction of signals recorded from biological visual systems. To better understand the function of these networks and relate them to hypotheses about brain activity and behavior, researchers need to extract the activations to images across different DNN layers. The abundance of different DNN variants, however, can often be unwieldy, and the task of extracting DNN activations from different layers may be non-trivial and error-prone for someone without a strong computational background. Thus, researchers in the fields of cognitive science and computational neuroscience would benefit from a library or package that supports a user in the extraction task. {$<$}monospace{$>$}THINGSvision{$<$}/monospace{$>$} is a new Python module that aims at closing this gap by providing a simple and unified tool for extracting layer activations for a wide range of pretrained and randomly-initialized neural network architectures, even for users with little to no programming experience. We demonstrate the general utility of {$<$}monospace{$>$}THINGsvision{$<$}/monospace{$>$} by relating extracted DNN activations to a number of functional MRI and behavioral datasets using representational similarity analysis, which can be performed as an integral part of the toolbox. Together, {$<$}monospace{$>$}THINGSvision{$<$}/monospace{$>$} enables researchers across diverse fields to extract features in a streamlined manner for their custom image dataset, thereby improving the ease of relating DNNs, brain activity, and behavior, and improving the reproducibility of findings in these research fields.{$<$}/p{$>$}},
  langid = {english},
  keywords = {artificial intelligence,computational neuroscience,Computer Vision,Deep neural network (DNN),feature extraction,Python (programming language)},
  file = {/home/vboyce/Zotero/storage/X8LCDWSG/Muttenthaler and Hebart - 2021 - THINGSvision A Python Toolbox for Streamlining the Extraction of Activations From Deep Neural Netwo.pdf}
}

@article{ohmer2022,
  title = {Mutual {{Exclusivity}} in {{Pragmatic Agents}}},
  author = {Ohmer, Xenia and Franke, Michael and K{\"o}nig, Peter},
  year = {2022},
  journal = {Cognitive Science},
  volume = {46},
  number = {1},
  pages = {e13069},
  issn = {1551-6709},
  doi = {10.1111/cogs.13069},
  urldate = {2023-02-10},
  abstract = {One of the great challenges in word learning is that words are typically uttered in a context with many potential referents. Children's tendency to associate novel words with novel referents, which is taken to reflect a mutual exclusivity (ME) bias, forms a useful disambiguation mechanism. We study semantic learning in pragmatic agents---combining the Rational Speech Act model with gradient-based learning---and explore the conditions under which such agents show an ME bias. This approach provides a framework for investigating a pragmatic account of the ME bias in humans but also for building artificial agents that display an ME bias. A series of analyses demonstrates striking parallels between our model and human word learning regarding several aspects relevant to the ME bias phenomenon: online inference, long-term learning, and developmental effects. By testing different implementations, we find that two components, pragmatic online inference and incremental collection of evidence for one-to-one correspondences between words and referents, play an important role in modeling the developmental trajectory of the ME bias. Finally, we outline an extension of our model to a deep neural network architecture that can process more naturalistic visual and linguistic input. Until now, in contrast to children, deep neural networks have needed indirect access to (supposed to be novel) test inputs during training to display an ME bias. Our model is the first one to do so without using this manipulation.},
  langid = {english},
  keywords = {Deep learning,Mutual exclusivity,Pragmatics,Rational Speech Act model,Reinforcement learning},
  file = {/home/vboyce/Zotero/storage/YCF7GS8U/Ohmer et al. - 2022 - Mutual Exclusivity in Pragmatic Agents.pdf;/home/vboyce/Zotero/storage/M4AVQK8V/cogs.html}
}

@article{pedregosa2011scikit,
  title = {Scikit-Learn: {{Machine}} Learning in Python},
  author = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  year = {2011},
  journal = {the Journal of machine Learning research},
  volume = {12},
  pages = {2825--2830},
  publisher = {JMLR. org}
}

@article{potts2015,
  title = {Embedded {{Implicatures}} as {{Pragmatic Inferences}} under {{Compositional Lexical Uncertainty}}},
  author = {Potts, Christopher and Lassiter, Daniel and Levy, Roger and Frank, Michael C.},
  year = {2015},
  month = dec,
  journal = {Journal of Semantics},
  pages = {ffv012},
  issn = {0167-5133, 1477-4593},
  doi = {10.1093/jos/ffv012},
  urldate = {2025-02-01},
  abstract = {How do comprehenders reason about pragmatically ambiguous scalar terms like some in complex syntactic contexts? In many pragmatic theories of conversational implicature, local exhaustification of such terms (`only some') is predicted to be difficult or impossible if the result does not entail the literal meaning, whereas grammatical accounts predict such construals to be robustly available. Recent experimental evidence supports the salience of these local enrichments, but the grammatical theories that have been argued to account for this evidence do not provide explicit mechanisms for weighting such construals against others. We propose a probabilistic model that combines previous work on pragmatic inference under `lexical uncertainty' with a more detailed model of compositional semantics. We show that this model makes accurate predictions about new experimental data on embedded implicatures in both non-monotonic and downward-entailing semantic contexts. In addition, the model's predictions can be improved by the incorporation of neo-Gricean hypotheses about lexical alternatives. This work thus contributes to a synthesis of grammatical and probabilistic views on pragmatic inference.},
  langid = {english},
  file = {/home/vboyce/Zotero/storage/4M4M6WZ4/Potts et al. - 2015 - Embedded Implicatures as Pragmatic Inferences under Compositional Lexical Uncertainty.pdf}
}

@article{potts2016,
  title = {Embedded {{Implicatures}} as {{Pragmatic Inferences}} under {{Compositional Lexical Uncertainty}}},
  author = {Potts, Christopher and Lassiter, Daniel and Levy, Roger and Frank, Michael C.},
  year = {2016},
  month = nov,
  journal = {Journal of Semantics},
  volume = {33},
  number = {4},
  pages = {755--802},
  issn = {0167-5133},
  doi = {10.1093/jos/ffv012},
  urldate = {2025-02-03},
  abstract = {How do comprehenders reason about pragmatically ambiguous scalar terms like some in complex syntactic contexts? In many pragmatic theories of conversational implicature, local exhaustification of such terms (`only some') is predicted to be difficult or impossible if the result does not entail the literal meaning, whereas grammatical accounts predict such construals to be robustly available. Recent experimental evidence supports the salience of these local enrichments, but the grammatical theories that have been argued to account for this evidence do not provide explicit mechanisms for weighting such construals against others. We propose a probabilistic model that combines previous work on pragmatic inference under `lexical uncertainty' with a more detailed model of compositional semantics. We show that this model makes accurate predictions about new experimental data on embedded implicatures in both non-monotonic and downward-entailing semantic contexts. In addition, the model's predictions can be improved by the incorporation of neo-Gricean hypotheses about lexical alternatives. This work thus contributes to a synthesis of grammatical and probabilistic views on pragmatic inference.},
  file = {/home/vboyce/Zotero/storage/6R6CTCPQ/2563037.html}
}

@misc{radford2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year = {2021},
  month = feb,
  number = {arXiv:2103.00020},
  eprint = {2103.00020},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.00020},
  urldate = {2023-04-20},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/vboyce/Zotero/storage/7YX6Z5L9/Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf;/home/vboyce/Zotero/storage/R7UD3HPW/2103.html}
}

@misc{reimers2019,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT-Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2019},
  month = aug,
  number = {arXiv:1908.10084},
  eprint = {1908.10084},
  primaryclass = {cs},
  institution = {arXiv},
  doi = {10.48550/arXiv.1908.10084},
  urldate = {2022-06-06},
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,read},
  file = {/home/vboyce/Zotero/storage/P7BNUJI7/Reimers and Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese B.pdf;/home/vboyce/Zotero/storage/LPUGIJGS/1908.html}
}

@article{schober1989,
  title = {Understanding by Addressees and Overhearers},
  author = {Schober, Michael F and Clark, Herbert H},
  year = {1989},
  month = apr,
  journal = {Cognitive Psychology},
  volume = {21},
  number = {2},
  pages = {211--232},
  issn = {00100285},
  doi = {10.1016/0010-0285(89)90008-X},
  urldate = {2022-02-01},
  langid = {english},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/DID3SPXG/Schober and Clark - 1989 - Understanding by addressees and overhearers.pdf}
}

@article{schuster2020,
  title = {I Know What You're Probably Going to Say: {{Listener}} Adaptation to Variable Use of Uncertainty Expressions},
  shorttitle = {I Know What You're Probably Going to Say},
  author = {Schuster, Sebastian and Degen, Judith},
  year = {2020},
  month = oct,
  journal = {Cognition},
  volume = {203},
  pages = {104285},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2020.104285},
  urldate = {2025-01-27},
  abstract = {Pragmatic theories of utterance interpretation share the assumption that listeners reason about alternative utterances that a speaker could have produced, but didn't. For such reasoning to be successful, listeners must have precise expectations about a speaker's production choices. This is at odds with the considerable variability across speakers that exists at all levels of linguistic representation. This tension can be reconciled by listeners adapting to the statistics of individual speakers. While linguistic adaptation is increasingly widely attested, semantic/pragmatic adaptation is underexplored. Moreover, what kind of representations listeners update during semantic/pragmatic adaptation -- estimates of the speaker's lexicon, or estimates of the speaker's utterance preferences -- remains poorly understood. In this work, we investigate semantic/pragmatic adaptation in the domain of uncertainty expressions like might and probably. In a series of web-based experiments, we find 1) that listeners vary in their expectations about a generic speaker's use of uncertainty expressions; 2) that listeners rapidly update their expectations about the use of uncertainty expressions after brief exposure to a speaker with a specific usage of uncertainty expressions; and 3) that listeners' interpretations of uncertainty expressions change after being exposed to a specific speaker. We present a novel computational model of semantic/pragmatic adaptation based on Bayesian belief updating and show, through a series of model comparisons, that semantic/pragmatic adaptation is best captured by listeners updating their beliefs both about the speaker's lexicon and their utterance preferences. This work has implications for both semantic theories of uncertainty expressions and psycholinguistic theories of adaptation: it highlights the need for dynamic semantic representations and suggests that listeners integrate their general linguistic knowledge with speaker-specific experiences to arrive at more precise interpretations.},
  keywords = {Adaptation,Bayesian cognitive modeling,Experimental pragmatics,Language comprehension,Uncertainty expressions},
  file = {/home/vboyce/Zotero/storage/YTS2A2GC/Schuster and Degen - 2020 - I know what you're probably going to say Listener adaptation to variable use of uncertainty express.pdf;/home/vboyce/Zotero/storage/88MZV5N4/S0010027720301049.html}
}

@article{schuster2020a,
  title = {I Know What You're Probably Going to Say: {{Listener}} Adaptation to Variable Use of Uncertainty Expressions},
  shorttitle = {I Know What You're Probably Going to Say},
  author = {Schuster, Sebastian and Degen, Judith},
  year = {2020},
  month = oct,
  journal = {Cognition},
  volume = {203},
  pages = {104285},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2020.104285},
  urldate = {2025-02-01},
  abstract = {Pragmatic theories of utterance interpretation share the assumption that listeners reason about alternative utterances that a speaker could have produced, but didn't. For such reasoning to be successful, listeners must have precise expectations about a speaker's production choices. This is at odds with the considerable variability across speakers that exists at all levels of linguistic representation. This tension can be reconciled by listeners adapting to the statistics of individual speakers. While linguistic adaptation is increasingly widely attested, semantic/pragmatic adaptation is underexplored. Moreover, what kind of representations listeners update during semantic/pragmatic adaptation -- estimates of the speaker's lexicon, or estimates of the speaker's utterance preferences -- remains poorly understood. In this work, we investigate semantic/pragmatic adaptation in the domain of uncertainty expressions like might and probably. In a series of web-based experiments, we find 1) that listeners vary in their expectations about a generic speaker's use of uncertainty expressions; 2) that listeners rapidly update their expectations about the use of uncertainty expressions after brief exposure to a speaker with a specific usage of uncertainty expressions; and 3) that listeners' interpretations of uncertainty expressions change after being exposed to a specific speaker. We present a novel computational model of semantic/pragmatic adaptation based on Bayesian belief updating and show, through a series of model comparisons, that semantic/pragmatic adaptation is best captured by listeners updating their beliefs both about the speaker's lexicon and their utterance preferences. This work has implications for both semantic theories of uncertainty expressions and psycholinguistic theories of adaptation: it highlights the need for dynamic semantic representations and suggests that listeners integrate their general linguistic knowledge with speaker-specific experiences to arrive at more precise interpretations.},
  keywords = {Adaptation,Bayesian cognitive modeling,Experimental pragmatics,Language comprehension,Uncertainty expressions},
  file = {/home/vboyce/Zotero/storage/XLIF59VH/Schuster and Degen - 2020 - I know what you're probably going to say Listener adaptation to variable use of uncertainty express.pdf;/home/vboyce/Zotero/storage/JPEJ88IG/S0010027720301049.html}
}

@article{tolins2016,
  title = {Overhearers {{Use Addressee Backchannels}} in {{Dialog Comprehension}}},
  author = {Tolins, Jackson and Fox Tree, Jean E.},
  year = {2016},
  month = aug,
  journal = {Cognitive Science},
  volume = {40},
  number = {6},
  pages = {1412--1434},
  issn = {03640213},
  doi = {10.1111/cogs.12278},
  urldate = {2022-02-01},
  abstract = {Observing others in conversation is a common format for comprehending language, yet little work has been done to understand dialogue comprehension. We tested whether overhearers use addressee backchannels as predictive cues for how to integrate information across speaker turns during comprehension of spontaneously produced collaborative narration. In Experiment 1, words that followed specific backchannels (e.g. really, oh) were recognized more slowly than words that followed either generic backchannels (e.g. uh huh, mhm) or pauses. In Experiment 2, we found that when the turn after the backchannel was a continuation of the narrative, specific backchannels prompted the fastest verification of prior information. When the turn after was an elaboration, they prompted the slowest, indicating that overhearers took specific backchannels as cues to integrate preceding talk with subsequent talk. These findings demonstrate that overhearers capitalize on the predictive relationship between backchannels and the development of speakers' talk, coordinating information across conversational roles.},
  langid = {english},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/7E8BQSBP/Tolins and Fox Tree - 2016 - Overhearers Use Addressee Backchannels in Dialog C.pdf}
}

@article{wilkes-gibbs1992,
  title = {Coordinating Beliefs in Conversation},
  author = {{Wilkes-Gibbs}, Deanna and Clark, H.},
  year = {1992},
  journal = {Journal of Memory and Language},
  pages = {183--194},
  abstract = {We show that participants in conversation develop beliefs about shared information that others do not. So-called directors talked with two partners in succession (A and B) to arrange unusual figures. Directors went from long, indefinite descriptions of the figures to short, definite references as common ground was built up with A. When B had been a silent side participant in the fast conversation, directors continued to use short references when they changed partners. References became less efftcient when B had not been a participant-even when B had heard the first conversation and seen the figures. When B had only heard the fast conversation, he or she was treated much the same as a completely naive partner. Apparently, conversation provides preferred evidence for coordinating beliefs about shared information. o 1992 ACZ\&{\textasciitilde}{\textasciitilde}C press, IN. In conversation, speakers collaborate with their partners in making references. In an earlier paper (Clark BL Wilkes-Gibbs, 1986), we proposed that when a speaker wants to refer to an object, it is not enough for her to utter a noun phrase such as the Allen wrench. She and her partner are also responsible for establishing that he has understood her as intended. What they do, therefore, is try to reach the mutual belief that he has understood her reference well enough for current purposes. They collaborate to reach this belief; she looks for reliable evidence of his understanding and he tries to provide it. In the process, he may offer alternative phrasing and ask for repeats or repairs (you mean the small metal thing shaped like an L?); she may offer more information and ask for confirmation. Predictions of the collaborative theory, as we will call it, have been confirmed for repeated references to objects (Clark \& Wilkes-Gibbs, 1986), for partners with disparate goals (Wilkes-Gibbs, 1986), and dis-Send correspondence and reprint requests to D.},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/DECDUNXC/Clark - 1992 - Coordinating beliefs in conversation.pdf}
}

@article{yoon2018,
  title = {Aim {{Low}}: {{Mechanisms}} of {{Audience Design}} in {{Multiparty Conversation}}},
  shorttitle = {Aim {{Low}}},
  author = {Yoon, Si On and {Brown-Schmidt}, Sarah},
  year = {2018},
  month = oct,
  journal = {Discourse Processes},
  volume = {55},
  number = {7},
  pages = {566--592},
  issn = {0163-853X, 1532-6950},
  doi = {10.1080/0163853X.2017.1286225},
  urldate = {2022-02-01},
  abstract = {It is well established in studies of two-party conversation that conversational partners jointly establish brief labels for repeatedly mentioned entities. When speaking to a new partner who is unfamiliar with the labels, speakers use longer expressions to facilitate understanding. How this process of audience design scales up to multiparty conversation, where individuals differ in mutual knowledge, is unknown. Here we propose, and test, three potential hypotheses regarding how speakers design referring expressions in threeparty conversation. Participants completed a referential communication task in groups of three in which one participant, the Director, gave instructions to two Matchers who differed in their knowledge of referential labels. Directors flexibly alternated between partner-specific representations of common ground (CG), producing longer descriptions for low-CG than for high-CG partners. When addressing multiple parties at once, speakers tailored descriptions for the least knowledgeable person. These findings shed light on the mechanisms that support audience design in multiparty conversation: Audience design begins with access to distinct representations of common ground held with the intended addressee or addressees. These distinct representations support an audience design process in which utterances are tailored to accommodate the least knowledgeable addressee in a group.},
  langid = {english},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/IVH2UEQF/Yoon and Brown-Schmidt - 2018 - Aim Low Mechanisms of Audience Design in Multipar.pdf}
}
