% Template for Cogsci submission with R Markdown

% Stuff changed from original Markdown PLOS Template
\documentclass[10pt, letterpaper]{article}

\usepackage{cogsci}


\title{Not so opaque after all: Conventions formed in reference games
are mostly understandable to outsiders}


\author{{\large \bf Veronica Boyce (vboyce@stanford.edu)} \\ Department of Psychology \\ Stanford University \And {\large \bf Ben Prystawski (benpry@stanford.edu)} \\Department of Psychology \\ Stanford University \AND {\large \bf Alvin Wei Ming Tan (tanawm@stanford.edu)} \\ Department of Psychology \\ Stanford University \And {\large \bf Michael C. Frank (mcfrank@stanford.edu)} \\ Department of Psychology \\ Stanford University}


\begin{document}

\maketitle

\begin{abstract}
In-groups can create conventionalized language, but this jargon may be
understandable to those outside the group. The formation of temporary
linguistic conventions between individuals is often studied in iterated
reference games, where over repeated reference to the same targets, a
describer-matcher pair establishes partner-specific shorthand names for
targets. One open question is how understandable these referring
expressions are to others who were not part of the convention formation
process. We take an outside angle on understanding convention formation,
using experiments with naive matchers and computational models to assess
the opacity of descriptions from iterated reference games. Both human
matchers and the computational model are well above chance accuracy,
with variation in performance primarily driven by the target image
rather than where or when the description came from. This additional
perspective can inform work on how conventions are formed and how
efficient the conventions actually are.

\textbf{Keywords:}
Reference games; convention formation; computational model;
\end{abstract}

\section{Introduction}\label{introduction}

Groups of people often have terms that are used within a group, such as
regional dialects, field specific jargon, or terms of art related to a
specific theoretic orientation. Those who are not part of group may use
different words for the same targets, but may be able to understand or
guess at the meanings, even as they do not produce them. While these
terms can represent stable conventions shared by sizeable communities,
temporary naming conventions can develop rapidly among small groups of
people when there is a need to refer to something without a canonical
name that distinguishes it in context.

This formation of temporary linguistic conventions between individuals
is often studied in iterated reference games. In these games, a
describer tells their partner how to sort or match a series of abstract
images (Clark \& Wilkes-Gibbs, 1986; Hawkins et al., 2020). Over
repeated rounds of referring to the same targets, pairs usually develop
conventionalized nicknames for the target images. These nicknames are
often partner-specific, in that different pairs develop different
nicknames for the same targets. When describing the shapes for people
who were not part of the group, people go back to more elaborated
descriptions, indicating an expectations that others may be unable to
understand the convention, and need a longer, or different referential
description (Hawkins et al., 2021; Wilkes-Gibbs \& Clark, 1992; Yoon \&
Brown-Schmidt, 2018). Participants treat the later-stage conventions as
more opaque to outsiders than earlier-stage descriptions, and the
differentiation of labels across groups over time is a signal of
arbitrariness in the conventions (Boyce et al., 2024; Hawkins et al.,
2020).

How arbitrary and opaque are the conventions, really? Another approach
to studying convention formation is to look at how understandable the
conventions are to others who were not part of the interaction that
originated the convention. A more arbitrary convention should be harder
to understand.

Asking naive matchers to interpret utterances produced during iterated
reference games thus can provide an additional perspective by
determining how the features of the utterances and the conditions they
were produced under affect how opaque they are, and provide empirical
tests of whether descriptions become more opaque over the course of the
reference game.

Some prior work has investigated how well naive matchers can understand
the descriptions produced in the course of an iterated reference game,
with a focus on the role of conversational shared history.

Murfitt \& McAllister (2001) recorded descriptions from 8 participants
who described shapes either solo or in a matching game with a partner
and played the descriptions to new matchers, either in order or in
reverse order. Naive listeners were more accurate when the heard
descriptions in order. In Schober \& Clark (1989) matchers in the
iterated reference game had achieved mean accuracies of of 93\% in the
first round to 100\% in the 6th round. Overhearers who listened to the
entire game in order had accuracies that rose from 81\% to 95\%.
Overhearers who listened to recordings starting in the 3rd round did
worse, with accuracies rising from 55\% to 73\%. With 16 target options,
even the late-start overhearers were far above chance.

In an iterated reference game using drawing (rather than text) as the
communication modality, Hawkins et al. (2023) compared the accuracy of
in-game matchers to naive matchers in yoked and shuffled conditions. The
yoked matchers saw all the trials from one game in order, while the
shuffled matchers saw trials sampled from 10 games but in trial order.
In-game matchers were more accurate overall (88\% on 4-way choice) than
yoked matchers (75\%) who were in turn more accurate than shuffled
matchers (69\%). Over the course of trials, both in-game and yoked
matchers showed steeper improvement in accuracy than shuffled matchers.

Leung et al. (2024) did have adults and children serve as naive
matchers, listening to descriptions from the the first and last rounds
of a parent-child iterated reference game. They found no significant
difference in how comprehensible early and late descriptions were to
naive matchers, and while naive matchers had high accuracy (88\% on
2-way choice), they were slightly less accurate than in-game matchers
overall.

Across the prior literature, while naive matchers have worse accuracy
than in-game matchers, their performance is still far above chance,
suggesting that the convention-target relationship is not purely
arbitrary. In fact, even when pairs of participants try to obfuscate
their meaning to match images with each other but not an overhearer, an
overhearing participant can still do quite well (Clark \& Schaefer,
1987).

Across these studies, recieving more context from one interaction, and
in particular, having that context be in order, is beneficial to
matchers. Except for the shuffled condition of Hawkins et al. (2023),
these studies do not address how opaque descriptions from different
points in the game are without prior context.

One way to conceptualize the opacity of a referring expression is by
considering the semantic distance between the signifier and the
referent. Under the assumption of a modality-independent global
semantics (i.e., not conditioned by partner-specific meaning),
expressions that are transparent have signifiers and referents that are
semantically close, such that any member of the sociolinguistic
community sharing the global semantics should be able to identify the
appropriate referent given the signifier. In contrast, expressions that
are opaque have signifiers and referents that are semantically distant,
such that the relations between them are more arbitrary and inaccessible
to the general community without the formation of additional conventions
(which may be partner- or group-specific).

Assessing opacity thus requires us to have a measure of
modality-independent global semantics. Such semantics are difficult to
directly obtain for humans, since we rarely have explicit semantic
formulations of the stimuli we encounter, much less formulations that
are unified across multiple modalities. However, the ability of a naïve
comprehender to understand a referring expression that is presented
without context provides a proxy measure, since the comprehender's
judgements are not conditioned on any context-specific conventions.
Thus, prior studies on naïve matchers provide one approach to estimating
the opacity of the utterances produced in reference games.

Another possible measure of modality-independent global semantics is
computational in origin. Computational methods have enabled the
projection of various stimuli (including image and text stimuli) into
embeddings in high-dimensional spaces; these embeddings have
demonstrated properties which suggest that they are reasonable
approximations of humans' semantic spaces, including similarity in
representational geometries (e.g., Grand et al., 2022; Muttenthaler \&
Hebart, 2021). Indeed, embeddings from neural network models have been
used as a form of semantics in a range of reference game scenarios
(e.g., Gul \& Artzi, 2024; Ji et al., 2022; Kang et al., 2020; Le et
al., 2022; Ohmer et al., 2022). In particular, such embeddings can be
treated as context-independent semantic representations, since they are
not updated to account for convention formation within an iterated
reference game; hence, they can serve as a computational comparison to
human performance on a naïve matching task.

In the current work, we address how the process of convention formation
shapes the levels of opacity of the referring expressions created at
different time points in an iterated reference game. Using reference
expressions created in different games from Boyce et al. (2024), we use
both human experiments and models to assess when and why expressions are
opaque or understandable to outside observers.

\begin{CodeChunk}
\begin{figure}[t!]

{\centering \includegraphics[width=1\linewidth]{matcher-diagram} 

}

\caption[Experimental Setup and Procedure]{Experimental Setup and Procedure. TODO \label{game}}\label{fig:interface}
\end{figure}
\end{CodeChunk}

\section{Task Setup}\label{task-setup}

\subsection{Materials}\label{materials}

We draw on the corpus of reference game transcripts and results from
Boyce et al. (2024). This corpus is made up of 6 round iterated
reference games using the same 12 target images. Games were played in
conditions varied in how large the describer-matcher groups were (2-6
participants) and how ``thick'' the communication channels were. For our
naive matcher experiments, we sample different subsets of this corpus.
In presenting the transcripts, we exclude utterances that were marked by
Boyce et al. (2024) as not containing any referential content
(i.e.~purely greetings, meta-commentary, or off-topic chitchat). Within
the samples, we also do not show transcripts that contained swear words
or crude or sexual language. We use the entire corpus for our
computational modelling component.

\subsection{Experimental procedure}\label{experimental-procedure}

We recruited English-speaking participants from Prolific. Participants
were directed to the experiment, where the task was explained to them.
On each trial participants saw the full transcript from that trial,
containing all the chat messages marked by whether they were from the
speaker or a listener. Participants selected the image they thought was
the target from the tableau of 12 (Figure \ref{game}). Participants
received feedback on whether they were right or wrong on each trial.
Except when the specific viewing order was part of the experimental
manipulation, we randomized the order of trials, subject to the
constraint that the same target could not repeat on adjacent trials. The
task was implemented in jsPsych (Leeuw et al., 2023). We paid
participants \$10 an hour plus a bonus of 5 cents per correct response.
All our experimental code is at TODO LINK.

\subsection{Computational models}\label{computational-models}

We used CLIP--in particular clip-vit-large-patch14--as a listener model
for our domain (Radford et al., 2021). CLIP is a natural choice for
reference games, as the model is trained to estimate the correspondence
between images and phrases in natural language. We run CLIP for a given
string of text and all 12 tangram shapes, then compute probabilities for
each tangram shape using CLIP's logits. The simplest way to do this is
simply take the softmax of the logits. However, the tangram shapes are
possibly out the distribution for the model, which leads it to favor
some images over others regardless of the content of the text. To fix
this, we train readout models that make more calibrated predictions
using CLIP's logits.

We show models the 12 tangrams images used in the experiment. For text,
we concatenate all the messages sent by the speaker in a given round of
the experiment.

\begin{table}
\label{classifier-comparison}
\caption{Cross-validated accuracies for classifiers. Standard deviations in accuracy across the 10 folds are sown in parentheses. Best performance within each model class is underlined, and best overall performance is bolded. }
\centering

    \begin{tabular}{p{1em}lr}
        \hline
        &Classifier & Accuracy \\ 
        \hline
            \multicolumn{2}{l}{Logistic Regression} & \\
                    & No penalty & \underline{0.50 (0.01)} \\ 
        & L2 penalty & 0.50  (0.01) \\ 
        \multicolumn{2}{l}{Random Forest} & \\
            & 10 estimators & 0.46 (0.02) \\
                    & 50 estimators  & 0.51 (0.02)\\ 
        & 100 estimators & 0.52 (0.02) \\ 
        & 500 estimators & \underline{0.52 (0.02)} \\ 

        \multicolumn{2}{l}{Gradient-Boosted Tree} & \\
                & 10 estimators & 0.48 (0.02) \\ 
        & 100 estimators & \underline{0.51 (0.02)} \\ 
        \multicolumn{2}{l}{Multi-layer Perceptron} & \\
    & 1 32-dim hidden layer & 0.50 (0.01) \\ 
        & 1 100-dim hidden layer  & 0.52 (0.01) \\ 
            & 1 512-dim hidden layer & 0.53 (0.02) \\ 
    & 1 1028-dim hidden layer & 0.53 (0.02) \\ 
        & 2 32-dim hidden layers  & 0.51 (0.02) \\ 
        & 2 100-dim hidden layers & \textbf{0.55 (0.02)} \\ 
    \hline
    \end{tabular}
\end{table}

We train different readout models to assign probabilities to features
using CLIP's logits as features. Models are trained to maximize task
task performance (i.e.~to assign high probability to the true tangram
given a concatenated speaker utterance). We compare four types of
models: a random forest, a logistic regression model, a Multi-layer
Perceptron, and a gradient-boosted tree. Classifiers were implemented in
the scikit-learn and XGBoost libraries Pedregosa et al. (2011) Chen \&
Guestrin (2016). Table \ref{tab:classifier-comparison} shows
cross-validated accuracy of different readout models, as well as the
performance of CLIP with no readout. The MLP with two hidden layers of
size 100 performs best, so we use its predictions in subsequent
analysis.

\section{Experiment 1}\label{experiment-1}

\begin{CodeChunk}
\begin{figure}[t]

{\centering \includegraphics[width=0.7\linewidth]{figs/fig-calibration-1} 

}

\caption[Correlation between human and CLIP-MLP probability of target across deciles of CLIP-MLP probability]{Correlation between human and CLIP-MLP probability of target across deciles of CLIP-MLP probability. Colored points are individual descriptions, black line is the boostrapped mean and 95\% CI across descriptions for each decile. \label{calibration}}\label{fig:fig-calibration}
\end{figure}
\end{CodeChunk}

Our CLIP-MLP computational model was optimized for task accuracy. To
validate whether this objective also results in human-like response
patterns, we conducted a calibration experiment to determine if, for any
given utterance, the probability that the model assigns to the target
image was aligned with the probability that a naïve human matcher would
choose the target image.

\subsection{Methods}\label{methods}

We first obtained target probabilities from our CLIP-MLP model for all
utterances from Boyce et al. (2024). We then used stratified sampling to
select 217 trials by dividing model-predicted probabilities into deciles
and choosing approximately 22 utterances per decile, spanning the 12
different possible target images.

We recruited 61 participants who each saw 64 trials randomly sampled
from the 217 tested trials. On average, each trial was seen by 18
participants. This experiment was pre-registered at
\url{https://osf.io/6pv5e}.

\subsection{Results}\label{results}

We obtained human accuracies on each trial by dividing the number of
participants who correctly selected the target image by the total number
of participants who saw the trial, as shown in Figure
\ref{fig:fig-calibration}. There was a small but significant positive
correlation between model-predicted probabilities and human accuracies
(\(r\) = 0.33 {[}0.21, 0.45{]}). This result suggests that model
predictions were calibrated to human response patterns, albeit not
perfectly. It is theoretically possible to use these calibration results
to project model predictions in order to better approximate human
responses; we leave this approach for future work. Nonetheless, the
observed positive correlation suggests that our computational model is a
reasonable approximation of human accuracies, validating its use in
subsequent experiments as a computational comparison.

\section{Experiment 2}\label{experiment-2}

As a starting point for examining what makes referential expressions
more or less opaque, we focused on referring expressions from the first
and last rounds of games. Principles of convention formation and
people's behavior when they switch to a new partner suggest that later
round utterances are more opaque and would be more difficult to
understand. One argument in the opposite direction is that later rounds
are the result of describers accumulated practice refining descriptions
to be maximally communicative and pick out the most visually salient
features. We included descriptions from games of different sizes and
communication thicknesses. Based on the patterns of cross-game
similarity in Boyce et al. (2024), we expected smaller and thicker
games, whose descriptions diverged fastest, would have more
idiosyncratic and opaque conventions than larger groups with thinner
communication channels.

\subsection{Methods}\label{methods-1}

\subsubsection{Experiment 2a}\label{experiment-2a}

To establish a baseline of how well naive matchers could understand
descriptions without context, we ran a 2x2 within subjects experiment.
We drew the target transcripts from 2- and 6-player games from
Experiment 1 of Boyce et al. (2024) and from the first and last blocks
of these games. These games had medium thick communication channels. We
recruited 60 participants who each saw 60 trials (15 in each of the 4
conditions). Overall, participants saw 774 transcripts from 40 games.
This experiment was pre-registered at \url{https://osf.io/k45dr}.

\subsubsection{Experiment 2b}\label{experiment-2b}

After observing limited condition differences in experiment 2a, we ran a
follow-up experiment on descriptions from Experiment 3 of Boyce et al.
(2024), where the communication channel thicknesses were more extreme.
Here, we used a 2x2x2 within subjects design, drawing our transcripts
from the first and last rounds of thick and thin, 2- and 6- person
games. In the thin condition, original matchers could only contribute to
the chat by sending one of 4 emojis; as the emojis did not have
referential content, we did not include them in the transcripts shown to
naive matchers. For experiment 2b, we recruited 60 participants who each
saw 64 trials (8 in each of the 8conditions). Overall, participants saw
2392 transcripts from 163 games. This experiment was pre-registered at
\url{https://osf.io/rdp5k}.

\begin{CodeChunk}
\begin{figure}[t]

{\centering \includegraphics[width=0.9\linewidth]{figs/fig-condition-1} 

}

\caption[Accuracies for naive humans and the CLIP-MLP model for Experiment 2]{Accuracies for naive humans and the CLIP-MLP model for Experiment 2. Point estimates and 95\% CrI are predictions from the fixed effects of logistic and beta regressions. Bootstrapped mean accuracy from the original matchers is included as a ceiling, and random chance as a baseline. \label{expt2-condition}}\label{fig:fig-condition}
\end{figure}
\end{CodeChunk}

\begin{CodeChunk}
\begin{figure}[t]

{\centering \includegraphics[width=0.9\linewidth]{figs/fig-2-1} 

}

\caption[Accuracies for naive humans and the CLIP-MLP model for Experiment 2, split out by target image]{Accuracies for naive humans and the CLIP-MLP model for Experiment 2, split out by target image. Point estimates and 95\% CI are predictions from the fixed effects and by-tangram random effects of logistic and beta regressions, bootstrapped across conditions. Bootstrapped mean accuracy from the original matchers is included as a ceiling, and random chance as a baseline. \label{expt2-tangram}}\label{fig:fig-2}
\end{figure}
\end{CodeChunk}

\subsection{Results}\label{results-1}

Our primary outcome was naive matcher accuracy, i.e.~how often they
selected the correct target image.

\subsubsection{Experiment 2a}\label{experiment-2a-1}

For Experiment 2a, we ran a mixed effects logistic model of naive
matcher accuracy: correct\(\sim\) group\_size \(\times\) round~+
trial\_order~+ (group\_size \(\times\) round\textbar correct\_tangram)~+
(group\_size \(\times\) round~+ trial\_order\textbar workerid). Overall,
naive matchers were right more often than not, which was far above the
1/12 expected by random chance (Odds Ratio: 1.93 {[}1.05, 3.62{]}. There
were not large effects of condition (Figure \ref{expt2-condition} middle
panel). Participants tended to be less accurate at descriptions from the
last round (OR of last round: 0.77 {[}0.53, 1.1{]}). There was not a
clear effect of original group size (OR of 6-player game: 1.15 {[}0.89,
1.47{]}), but there was an interaction between round and group size (OR:
1.49 {[}1.06, 2.1{]}). Later transcripts from larger games were easier
to understand, but later transcripts from smaller games were easier to
understand.

Much of the variation in accuracy was driven not by condition, but by
the target image (OR of standard deviation of image distribution: 2.66
{[}1.88, 4.52{]}. Some images were much easier to identify as the target
than others (Figure \ref{expt2-tangram}).

\subsubsection{Experiment 2b}\label{experiment-2b-1}

For Experiment 2b we ran a similar mixed effects logistic model to
consider the effects of group size, thickness, round, and their
interactions. Naive matchers were above 50\% accuracy (OR: 1.81 {[}1.06,
3.08{]}, Figure \ref{expt2-condition} ). Similar to experiment 2a, there
were not substantial effects of condition. Last round descriptions had
slightly lower accuracy (OR of last round: 0.64 {[}0.47, 0.85{]}), but
there was an interaction with thickness, where for thin games, last
round descriptions were less opaque (OR: 1.55 {[}1.02, 2.33{]}). Again
some of the uncertainty in estimating the fixed effects was driven by
the strong variation based on target image (OR of SD of images: 2.25
{[}1.67, 3.59{]}, Figure \ref{expt2-tangram}).

\subsubsection{Additional Predictors}\label{additional-predictors}

As additional post-hoc predictors, we examined the accuracy of the
in-game matchers from Boyce et al. (2024) and the length of the
description. In both experiments, in-game accuracy was predictive of
naive matcher accuracy (Expt 2a OR: 3.38 {[}2.46, 4.7{]}, Expt 2b OR:
2.17 {[}1.7, 2.77{]}). The log number of words in the description was
not predictive in Experiment 2a (OR: 1.05 {[}0.94, 1.17{]}), but longer
descriptions were slightly beneficial in Experiment 2b (OR: 1.1 {[}1.01,
1.2{]}).

\subsection{Model results}\label{model-results}

As a computational comparison, we looked at the CLIP-MLP model's
performance on the same descriptions. We used the probability the model
assigned to the correct target as our dependent measure and fit a beta
regression on the descriptions from Experiment 2: correct\(\sim\)
group\_size \(\times\) thickness \(\times\) round~+ (group\_size
\(\times\) thickness \(\times\) round\textbar correct\_tangram). The
CLIP-MLP model was far above chance, but had lower accuracy than the
human participants (OR: 0.6 {[}0.45, 0.82{]}).

None of the fixed effects in the model were significant, and there was
wide uncertainty for all of them. There was substantial by-target image
variation (1.58 {[}1.31, 2.15{]}) and substantial by-target variation in
the effect of later round (1.56 {[}1.29, 2.09{]}).

As additional predictors, we checked the effect of in-game matcher
accuracy and the length of the description. MLP-CLIP had higher accuracy
when in-game matcher accuracy was higher (OR: 1.5 {[}1.33, 1.69{]}), and
the model did better on shorter descriptions (OR for log words: 0.85
{[}0.82, 0.9{]}). Long descriptions may be more difficult because they
are further further from the model's training distribution of image
captions.

\subsubsection{Interim Summary}\label{interim-summary}

Overall, naive human matchers were fairly accurate overall, but less
accurate than matchers in the original game. Perhaps surprisingly, this
level of accuracy was fairly consistent across descriptions from
different times in the game and different game conditions. The largest
source of variability in accuracy was from the target images; while
there was some variabiliity in accuracy by images for the original
matchers, there was substantially more variability for naive matchers.
The computational model showed a similar pattern of large effects of
image, but had overall lower accuracy than naive human matchers.

\section{Experiment 3}\label{experiment-3}

The experiment of naive matchers in Experiment 2 differed from in-game
matchers in several ways. In-game matchers recieved descriptions from a
consistent group, recieved descriptions in the order they were created,
and were present participants during the game. In Experiment 3, we focus
on the role of context and group-specific interaction history to tease
apart some of these differences.

\subsection{Methods}\label{methods-2}

We compared naive matchers in ``yoked'' and ``shuffled'' conditions. In
the ``yoked'' condition, naive matchers saw all the descriptions from a
single game in the order they originally occurred. In the ``shuffled''
condition, naive matchers saw all the descriptions from a single game in
a randomized order. This is the same yoking but a different shuffling
than that used in Hawkins et al. (2023).

Because some descriptions are already fairly comprehensible in
isolation, we focused on the role of context for games that showed
strong group-specificity. We hand-picked 10 games from Boyce et al.
(2024) on the basis of high in-game matcher accuracy, strong patterns of
descriptions shortening over repetition, and the use of idiosyncratic or
non-modal referring expressions. Thus, these games showed the hallways
of strong conventionalization to terms that were more likely to be
opaque to outsiders.

We recruited 196 participants (99 in the yoked condition and 97 in
shuffled) who each saw all 72 trials of 1 of the 10 games. This
experiment was pre-registered at \url{https://osf.io/zqwp5}.
Participants read the transcripts in a modified self-paced reading
procedure where they uncovered the text word by word (revealed words
stayed visible); only after uncovering the entire transcript could
participants select an image. We do not analyse the reading time data
here.

\begin{CodeChunk}
\begin{figure}[t]

{\centering \includegraphics[width=0.9\linewidth]{figs/fig-yoked-1} 

}

\caption[Accuracies for Experiment 3]{Accuracies for Experiment 3. Error bars are bootstrapped 95\% CIs. TODO not using predictions because those fuzz out round to round differences. \label{yoked}}\label{fig:fig-yoked}
\end{figure}
\end{CodeChunk}

\subsection{Results}\label{results-2}

Our primary question of interest was how much having the conversation
history would help make later round descriptions more understandable to
participants in the yoked condition.

We compared accuracy across the yoked and shuffled conditions with a
logistic regression: correct\(\sim\) orig\_repNum \(\times\) condition~+
matcher\_trialNum~+ (1\textbar gameId)~+ (1\textbar correct\_tangram)~+
(1\textbar workerid). The descriptions were more transparent when they
were presented in a yoked order (OR: 2.2 {[}1.63, 3{]}, Figure
\ref{yoked}). In the shuffled condition, there was no main effect of
round number (OR for one round later: 0.99 {[}0.95, 1.02{]}), but there
was a marginal interaction where the benefit of the yoked condition
decreased for later rounds (OR for one round later: 0.94 {[}0.89, 1{]}).
This was offset by matchers in both conditions improving at the task
over time (OR for one trial later in matcher viewing order: 1.02
{[}1.02, 1.02{]}). In the yoked condition round and trial number were
aligned, so an improvement over time could be either from matcher
practice or from descriptions being easier to understand. In the
shuffled condition, matcher practice effects did not correlate with
position in the original game.

Comparing to the performance of in-game matchers, we can separate out
the benefits of seeing the descriptions in order versus being a
participant in the group: correct\(\sim\) orig\_repNum \(\times\)
order~+ orig\_repNum \(\times\) setting~+ matcher\_trialNum~+
(1\textbar gameId)~+ (1\textbar correct\_tangram)~+
(1\textbar workerid). There is a benefit to seeing the items in order
(OR: 2.24 {[}1.63, 3.04{]}) and a larger benefit to being a participant
during the game (OR: 4.35 {[}2.77, 6.89{]}). The benefit of seeing the
items in order wanes in later blocks (OR: 0.94 {[}0.89, 1{]}), but the
benefit of being in the game does not (OR: 1.06 {[}0.95, 1.18{]}). In
all cases, there is a baseline improvement over trials (OR: 1.02
{[}1.02, 1.02{]}).

The accuracy of the CLIP-MLP model is worse than the shuffled human
results, and does not show change across rounds (OR for one round later:
1.02 {[}0.97, 1.07{]}). The larger difference between naive human and
CLIP-MLP accuracies in Experiment 3 than Experiment 2 could suggest that
even the shuffled ordering still provides useful context that helps
matchers understand the conventions. This history is not available to
the CLIP-MLP model which sees every description as a one-shot task.

\section{Discussion}\label{discussion}

Conventions are formed in reference games in a partner-specific way,
where different groups follow different paths through semantic space as
they form conventions. However, the referential descriptions used over
the course of convention formation remain relatively understandable to
outsiders. Across multiple experiments with human matchers, we found
that naive human matchers were far above chance accuracy at identifying
the targets, with variation explained more by the target image than the
round or game condition the descriptions came from. Even for games
selected for strong conventionalization, naive matchers had high
accuracy that was aided if they saw the descriptions in order.

We also tested a computational model built on CLIP with a multi-layer
perceptron thingy as a way of quantifying the semantic distance
descriptions of the image options. The CLIP-MLP model was far above
chance in its assignment of probabilities to target images, and although
its probabilities were lower than human matcher accuracies, the model
mirrored the qualitative pattern of larger effects from target image
than from description source.

Our experimental and computational results were only on a specific set
of iterated reference game transcripts targeting a specific set of 12
images and so numeric results may not generalize to other reference
games with different array sizes and sets of images. There are potential
non-measured differences between in-game matchers and naive matchers
because naive matchers did not have to wait for descriptions to appear
and thus had more choice about how closely to read descriptions or how
long to spend looking at images. We also cannot know whether naive
matchers (or for that matter, in-game matchers) fully ``comprehended''
the language in the descriptions, or were making inferences about the
meaning on a more analytic level.

This work suggests that conventions formed within a small group may
still be fairly comprehensible to those outside the group, who may
produce different descriptions. This raises questions around how
well-calibrated describers are to their matchers level of knowledge, and
whether the process of convention formation is actually efficient. Even
naive matchers can often understand the shorthand descriptions, but in
reference games, describers choose elaborated descriptions with new
matchers. In a game, norms of cooperation and conversation, may lead
describers to start new matchers with elaborated descriptions designed
to give them a high level of confidence in target selection. Describers
are also constrained by their need to come of with a description in real
time. However, the high level of understanding and the lack of
substantial benefit from early round descriptions does raise empirical
questions about how calibrated describers are to the level of
information necessary.

We found large variation in how accurate the computation model and naive
matchers were on different target images. This raises questions about
what makes some images much easier to identify. They might be more
iconic, with a narrower prior over different ways they could be
conceptualized, or they may be further from competitors within this pool
of images. We are limited by the 12 images we used, but future work
sampling across larger sets of images (such as Ji et al., 2022) could
probe image-level factors. Future work could also explore
within-description sources of variation and how the structure and word
choice of utterance correlates with naive matcher accuracy.
Computational models could be especially beneficial because they could
be run on subsets or ablations of descriptive text.

\section{References}\label{references}

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}

\noindent

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-boyce2024}
Boyce, V., Hawkins, R., Goodman, N. D., \& Frank, M. C. (2024).
\emph{Interaction structure constrains the emergence of conventions in
group communication}.

\bibitem[\citeproctext]{ref-chen2016xgboost}
Chen, T., \& Guestrin, C. (2016). Xgboost: {A} scalable tree boosting
system. \emph{Proceedings of the 22nd Acm Sigkdd International
Conference on Knowledge Discovery and Data Mining}, 785--794.

\bibitem[\citeproctext]{ref-clark1987a}
Clark, H. H., \& Schaefer, E. F. (1987). Concealing one's meaning from
overhearers. \emph{Journal of Memory and Language}, \emph{26}(2),
209--225. \url{https://doi.org/10.1016/0749-596X(87)90124-0}

\bibitem[\citeproctext]{ref-clark1986}
Clark, H. H., \& Wilkes-Gibbs, D. (1986). \emph{Referring as a
collaborative process}.

\bibitem[\citeproctext]{ref-grand2022}
Grand, G., Blank, I. A., Pereira, F., \& Fedorenko, E. (2022). Semantic
projection recovers rich human knowledge of multiple object features
from word embeddings. \emph{Nature Human Behaviour}, \emph{6}(7),
975--987. \url{https://doi.org/10.1038/s41562-022-01316-8}

\bibitem[\citeproctext]{ref-gul2024}
Gul, M. O., \& Artzi, Y. (2024). \emph{{CoGen}: {Learning} from
{Feedback} with {Coupled Comprehension} and {Generation}}
(arXiv:2408.15992). arXiv.
\url{https://doi.org/10.48550/arXiv.2408.15992}

\bibitem[\citeproctext]{ref-hawkins2020b}
Hawkins, R. D., Frank, M. C., \& Goodman, N. D. (2020). Characterizing
the dynamics of learning in repeated reference games.
\emph{arXiv:1912.07199 {[}Cs{]}}. \url{https://arxiv.org/abs/1912.07199}

\bibitem[\citeproctext]{ref-hawkins2021}
Hawkins, R. D., Liu, I., Goldberg, A. E., \& Griffiths, T. G. (2021).
Respect the code: {Speakers} expect novel conventions to generalize
within but not across social group boundaries. \emph{CogSci}.

\bibitem[\citeproctext]{ref-hawkins2023a}
Hawkins, R. D., Sano, M., Goodman, N. D., \& Fan, J. E. (2023). Visual
resemblance and interaction history jointly constrain pictorial meaning.
\emph{Nature Communications}, \emph{14}(1), 2199.
\url{https://doi.org/10.1038/s41467-023-37737-w}

\bibitem[\citeproctext]{ref-ji2022}
Ji, A., Kojima, N., Rush, N., Suhr, A., Vong, W. K., Hawkins, R., \&
Artzi, Y. (2022). Abstract {Visual Reasoning} with {Tangram Shapes}. In
Y. Goldberg, Z. Kozareva, \& Y. Zhang (Eds.), \emph{Proceedings of the
2022 {Conference} on {Empirical Methods} in {Natural Language
Processing}} (pp. 582--601). Association for Computational Linguistics.
\url{https://doi.org/10.18653/v1/2022.emnlp-main.38}

\bibitem[\citeproctext]{ref-kang2020}
Kang, Y., Wang, T., \& de Melo, G. (2020). Incorporating {Pragmatic
Reasoning Communication} into {Emergent Language}. \emph{Advances in
{Neural Information Processing Systems}}, \emph{33}, 10348--10359.

\bibitem[\citeproctext]{ref-le2022}
Le, H., Daryanto, T., Zhafransyah, F., Wijaya, D., Coppock, E., \& Chin,
S. (2022). \emph{Referring {Expressions} with {Rational Speech Act
Framework}: {A Probabilistic Approach}} (arXiv:2205.07795). arXiv.
\url{https://doi.org/10.48550/arXiv.2205.07795}

\bibitem[\citeproctext]{ref-leeuw2023}
Leeuw, J. R. de, Gilbert, R. A., \& Luchterhandt, B. (2023). {jsPsych}:
{Enabling} an {Open-Source Collaborative Ecosystem} of {Behavioral
Experiments}. \emph{Journal of Open Source Software}, \emph{8}(85),
5351. \url{https://doi.org/10.21105/joss.05351}

\bibitem[\citeproctext]{ref-leung2024}
Leung, A., Yurovsky, D., \& Hawkins, R. D. (2024). Parents spontaneously
scaffold the formation of conversational pacts with their children.
\emph{Child Development}, \emph{n/a}(n/a).
\url{https://doi.org/10.1111/cdev.14186}

\bibitem[\citeproctext]{ref-murfitt2001}
Murfitt, T., \& McAllister, J. (2001). The {Effect} of {Production
Variables} in {Monolog} and {Dialog} on {Comprehension} by {Novel
Listeners}. \emph{Language and Speech}, \emph{44}(3), 325--350.
\url{https://doi.org/10.1177/00238309010440030201}

\bibitem[\citeproctext]{ref-muttenthaler2021}
Muttenthaler, L., \& Hebart, M. N. (2021). {THINGSvision}: {A Python
Toolbox} for {Streamlining} the {Extraction} of {Activations From Deep
Neural Networks}. \emph{Frontiers in Neuroinformatics}, \emph{15}.
\url{https://doi.org/10.3389/fninf.2021.679838}

\bibitem[\citeproctext]{ref-ohmer2022}
Ohmer, X., Franke, M., \& König, P. (2022). Mutual {Exclusivity} in
{Pragmatic Agents}. \emph{Cognitive Science}, \emph{46}(1), e13069.
\url{https://doi.org/10.1111/cogs.13069}

\bibitem[\citeproctext]{ref-pedregosa2011scikit}
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B.,
Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., et
al. (2011). Scikit-learn: {Machine} learning in python. \emph{The
Journal of Machine Learning Research}, \emph{12}, 2825--2830.

\bibitem[\citeproctext]{ref-radford2021}
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S.,
Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., \&
Sutskever, I. (2021). \emph{Learning {Transferable Visual Models From
Natural Language Supervision}} (arXiv:2103.00020). arXiv.
\url{https://doi.org/10.48550/arXiv.2103.00020}

\bibitem[\citeproctext]{ref-schober1989}
Schober, M. F., \& Clark, H. H. (1989). Understanding by addressees and
overhearers. \emph{Cognitive Psychology}, \emph{21}(2), 211--232.
\url{https://doi.org/10.1016/0010-0285(89)90008-X}

\bibitem[\citeproctext]{ref-wilkes-gibbs1992}
Wilkes-Gibbs, D., \& Clark, H. (1992). Coordinating beliefs in
conversation. \emph{Journal of Memory and Language}, 183--194.

\bibitem[\citeproctext]{ref-yoon2018}
Yoon, S. O., \& Brown-Schmidt, S. (2018). Aim {Low}: {Mechanisms} of
{Audience Design} in {Multiparty Conversation}. \emph{Discourse
Processes}, \emph{55}(7), 566--592.
\url{https://doi.org/10.1080/0163853X.2017.1286225}

\end{CSLReferences}

\bibliographystyle{apacite}


\end{document}
