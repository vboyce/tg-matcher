---
title: "tg-matcher 4: Yoked v shuffled"
output:
  html_document:
    toc: true
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(dev = "png", dev.args = list(type = "cairo-png"))
options(knitr.table.format = "html")
knitr::opts_chunk$set(echo = F)
library(tidyverse)
library(viridis)
library(here)
library(ggthemes)
library(knitr)
library(ggtext)
library(ggimage)
library(jsonlite)
library(brms)
library(rstan)
library(rstanarm)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
theme_set(theme_bw())

dat_loc <- "data/tgmatcheryoked-trials.csv"
pred_loc <- "analysis-code/predictors"
```


# Boring stuff
## Read in data

```{r}
raw <- read_csv(here(dat_loc)) |>
  select(-proliferate.condition)

free_response <- raw |>
  filter(is.na(correct_tangram)) |>
  select(workerid, stimulus, response, rt)

good_stuff <- raw |>
  filter(!is.na(correct_tangram)) |>
  select(
    workerid, button_rt, correct, correct_tangram, condition,
    gameId, selected, text, trial_index, type, rt, orig_trialNum, orig_repNum
  ) |>
  mutate(workerid = as.factor(workerid)) |>
  mutate(
    matcher_trialNum = (trial_index - 3) %/% 3,
    matcher_repNum = matcher_trialNum %/% 12
  ) |>
  mutate(workerid = ifelse(workerid == "3157" & condition == "yoked", "3157a", workerid)) |> # somehow two participants were assigned to 3157 -- but each set looks complete?
  filter(workerid != "141") |>
  filter(workerid != "35") # exclude two participants who didn't finish
```

TODO there's something weird where participant 3157 has data for two complete expts (yoked and shuffled). Looks from free response like it's two separate people who just both got the same participant number?  

```{r}
good_stuff |>
  select(workerid, condition) |>
  unique() |>
  group_by(condition) |>
  tally()
```

<!--##  Bonus-->
```{r, eval=F}
# worker <- read_csv(here("data/tgmatcheryoked-workerids.csv")) |> mutate(workerid = as.factor(workerid))
#
# bonuses <- good_stuff |> filter(type=="selection") |>
#   group_by(workerid) |>
#   summarize(bonus = round(sum(correct) * .05, 2)) |>
#   left_join(worker) |>
#   select(prolific_participant_id, bonus) |>
#   write_csv(here("bonus.csv"))
#
# cost <- bonuses |>
#   mutate(cost = bonus * 4 / 3) |>
#   summarize(s = sum(cost))
#
```

# Checks
## SPR trials

```{r}
spr <- good_stuff |>
  filter(type == "reading") |>
  select(-button_rt, -correct, -selected) |>
  mutate(rt = map(rt, fromJSON)) |>
  unnest(rt)

spr_sum <- spr |>
  group_by(workerid) |>
  summarize(RT = sum(rt) / 1000)

ggplot(spr |> filter(rt < 2000), aes(x = rt)) +
  geom_histogram() +
  geom_vline(aes(xintercept = 100))
```
## Selections

```{r}
selections <- good_stuff |>
  filter(type == "selection") |>
  mutate(correct = as.numeric(selected == correct_tangram))

selections |> group_by(workerid) |> summarize(RT=sum(button_rt)/1000) |> 
  left_join(spr_sum) |> group_by(workerid) |> summarize(RT=sum(RT/60)) |> ggplot(aes(x=RT))+geom_histogram()


selections |> ggplot(aes(x = orig_repNum, y = button_rt / 1000, color = condition)) +
  geom_jitter(alpha = .05) +
  stat_summary() +
  geom_hline(yintercept = 3)

selections |> ggplot(aes(x = trial_index, y = button_rt / 1000, color = condition)) +
  geom_jitter(alpha = .05) +
  geom_smooth() +
  geom_hline(yintercept = 3)


selections |>
  group_by(gameId, correct_tangram, orig_repNum, condition) |>
  summarize(pct_correct = mean(correct)) |>
  ggplot(aes(x = orig_repNum, y = pct_correct, color = condition)) +
  geom_jitter(alpha = .1, color = "black") +
  stat_summary(fun.data = "mean_cl_boot") +
  geom_smooth() +
  theme(legend.position = "bottom")

selections |>
  group_by(trial_index, condition, gameId) |>
  summarize(pct_correct = mean(correct)) |>
  ggplot(aes(x = trial_index, y = pct_correct, color = condition)) +
  geom_jitter(alpha = .1, color = "black") +
  geom_smooth() +
  coord_cartesian(ylim = c(0, 1)) +
  theme(legend.position = "bottom")
```


## Accuracy

```{r}
acc_by_participant <- good_stuff |>
  filter(type == "selection") |>
  group_by(workerid, condition) |>
  summarize(acc = sum(correct) / n())
```

Maybe 1 or 2 random guessers, not bad. 

```{r}
ggplot(acc_by_participant, aes(x = condition, y = acc)) +
  geom_jitter(height = 0, width = .1) +
  coord_cartesian(ylim = c(0, 1), xlim = c(.5, 2.5), expand = F) +
  geom_hline(yintercept = 1 / 12, linetype = "dashed", color = "blue") +
  stat_summary(data.fun = "mean_cl_boot", color = "blue") +
  labs(y = "Percent correct")
```

## Distribution across games

Small number randomness means we have 5-18 in each cell, but that's decent!

```{r}
good_stuff |>
  select(workerid, gameId, condition) |>
  unique() |>
  group_by(gameId, condition) |>
  tally() |>
  pivot_wider(names_from = condition, values_from = n)
```

# Accuracy

```{r}
good_stuff |>
  filter(type == "selection") |>
  group_by(workerid, condition, orig_repNum) |>
  summarize(acc = sum(correct) / n()) |>
  ggplot(aes(x = orig_repNum, y = acc, color = condition)) +
  geom_jitter(alpha = .1) +
  theme(legend.position = "bottom") +
  geom_smooth()



good_stuff |>
  filter(type == "selection") |>
  group_by(gameId, condition, correct_tangram) |>
  summarize(acc = sum(correct) / n()) |>
  ggplot(aes(x = reorder(correct_tangram, acc, mean), y = acc, color = condition)) +
  geom_jitter(alpha = .5) +
  theme(legend.position = "none") +
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2))

good_stuff |>
  filter(type == "selection") |>
  group_by(gameId, condition, correct_tangram, orig_repNum) |>
  summarize(acc = sum(correct) / n()) |>
  ggplot(aes(x = reorder(correct_tangram, acc, mean), y = acc, color = condition)) +
  theme(legend.position = "none") +
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2)) +
  facet_wrap(~orig_repNum)

good_stuff |>
  filter(type == "selection") |>
  group_by(gameId, condition, orig_repNum, workerid) |>
  summarize(acc = sum(correct) / n()) |>
  ggplot(aes(x = reorder(str_sub(gameId, 1, 2), acc, mean), y = acc, color = condition)) +
  theme(legend.position = "none") +
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2))
```
    
```{r}  
good_stuff |>
  filter(type == "selection") |>
  group_by(workerid, condition, orig_repNum) |>
  summarize(acc = sum(correct) / n()) |>
  ggplot(aes(x = orig_repNum, y = acc, color = condition)) +
  geom_jitter(alpha = .1, color = "black") +
  stat_summary(fun.data = "mean_cl_boot") +
  geom_smooth() +
  theme(legend.position = "bottom")

good_stuff |>
  filter(type == "selection") |>
  group_by(gameId, condition, trial_index) |>
  summarize(acc = sum(correct) / n()) |>
  ggplot(aes(x = trial_index, y = acc, color = condition)) +
  geom_jitter(alpha = .1, color = "black") +
  geom_smooth() +
  coord_cartesian(ylim = c(0, 1)) +
  theme(legend.position = "bottom")
```

```{r}  
good_stuff |>
  filter(type == "selection") |>
  group_by(gameId, condition, orig_repNum, matcher_repNum) |>
  summarize(acc = sum(correct) / n()) |>
  ggplot(aes(x = orig_repNum, y = acc, group = matcher_repNum, color = matcher_repNum)) +
  geom_jitter(alpha = .1, color = "black") +
  stat_summary(fun.data = "mean_cl_boot") +
  theme(legend.position = "bottom") +
  facet_wrap(~condition)

good_stuff |>
  filter(type == "selection") |>
  group_by(gameId, condition, trial_index) |>
  summarize(acc = sum(correct) / n()) |>
  ggplot(aes(x = trial_index, y = acc, color = condition)) +
  geom_jitter(alpha = .1, color = "black") +
  geom_smooth() +
  coord_cartesian(ylim = c(0, 1)) +
  theme(legend.position = "bottom")
```

## For HSP

```{r}

good_stuff |>  filter(type == "selection") |>
  summarize(correct = sum(correct) / n())
summary2 <- good_stuff |>
  filter(type == "selection") |>
  group_by(condition, gameId, matcher_repNum) |>
  summarize(correct = sum(correct) / n())

good_stuff |>
  filter(type == "selection") |>
  group_by(condition, workerid, matcher_repNum) |>
  ggplot(aes(x = matcher_repNum + 1, y = as.numeric(correct), color = condition)) +
  geom_jitter(data = summary2, alpha = .4, position = position_dodge(width = .2)) +
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2)) +
  stat_summary(fun.data = "mean_cl_boot", geom = "line", position = position_dodge(width = .2, )) +
  labs(y = "Accuracy", x = "Trial block for matchers") +
  theme(legend.position = "bottom") +
  scale_x_continuous(breaks = seq(1, 6, 1)) +
  coord_cartesian(ylim = c(0, 1), xlim = c(.5, 6.5), expand = F) +
  theme(
    legend.title = element_blank(),
    panel.grid = element_blank()
  )

#ggsave(here("writing/hsp2.png"), width = 5, height = 3)

summary <- good_stuff |>
  filter(type == "selection") |>
  group_by(condition, gameId, orig_repNum) |>
  summarize(correct = sum(correct) / n())


good_stuff |>
  filter(type == "selection") |>
  group_by(condition, correct_tangram, orig_repNum) |>
  # summarize(acc = sum(correct) / n()) |>
  ggplot(aes(x = orig_repNum + 1, y = as.numeric(correct), color = condition)) +
  # geom_jitter(alpha = .1) +
  geom_jitter(data = summary, alpha = .4, position = position_dodge(width = .2)) +
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2)) +
  stat_summary(fun.data = "mean_cl_boot", geom = "line", position = position_dodge(width = .2, )) +
  theme(legend.position = "bottom") +
  scale_x_continuous(breaks = seq(1, 6, 1)) +
  coord_cartesian(ylim = c(0, 1), xlim = c(.5, 6.5), expand = F) +
  labs(y = "Accuracy", x = "Block in original game") +
  theme(
    legend.title = element_blank(),
    panel.grid = element_blank(),
    legend.text = element_text(size = 14),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 14)
  )

#ggsave(here("writing/hsp1.png"), width = 5, height = 3)
```
TODO 

Logistic model of target selection accuracy: Accuracy ~ original_rep_num * condition + viewing_order + (1 | gameId) + (1 | tangram) + (1 | participant)

Time to selection:  Selection_time ~ original_rep_num * condition + viewing_order + (1 | gameId) + (1 | tangram) + (1 | participant)


# SPR

## Prep

```{r}
labeled_yoked <- read_csv(here(pred_loc, "labeled_yoked.csv")) |> rename(orig_repNum = repNum)

spr <- good_stuff |>
  filter(type == "reading") |>
  select(-button_rt, -correct, -selected) |>
  mutate(rt = map(rt, fromJSON)) |>
  left_join(labeled_yoked) |>
  mutate(words = str_split(words, "\\s+")) |>
  unnest(rt, words) |>
  group_by(workerid, correct_tangram, condition, gameId, text, type, orig_trialNum, orig_repNum, matcher_trialNum) |>
  mutate(words = lag(words)) |>
  filter(!is.na(words)) |>
  mutate(is_from_speaker = case_when(
    words == "speaker**" ~ 1,
    words == "listener**" ~ 0,
    T ~ NA
  )) |>
  fill(is_from_speaker) |>
  mutate(
    is_valid_word = case_when(
      is.na(words) ~ 0,
      words %in% c("speaker**", "listener**") ~ 0,
      T ~ 1
    ),
    word_index = cumsum(is_valid_word),
    is_valid_speaker = ifelse(is_from_speaker == 1 & is_valid_word == 1, 1, 0),
    word_speaker_index = cumsum(is_valid_speaker),
    word_speaker_index = ifelse(is_valid_speaker, word_speaker_index, NA),
    overall_index = row_number()
  ) |>
  select(-is_valid_word, -is_valid_speaker, -trial_index, -matcher_repNum)
```

TODO check units for everything 

```{r, message=T}
kl_pred <- read_csv(here(pred_loc, "kl_full.csv")) |>
  # select(-text, -partial, -condition, -trialNum) |>
  select(-trialNum) |>
  rename(word_speaker_index = partial_length, correct_tangram = tangram, orig_repNum = repNum) |>
  select(kl, correct_tangram, gameId, orig_repNum, word_speaker_index)

llm_concat_probs <- read_csv(here(pred_loc, "llm_probs_concat.csv")) |>
  group_by(full_text, word, word_idx) |>
  summarize(logprob = mean(logprob)) |> # deal with stochasticity in surprisal estimates for when the exact same phrase occurred repeatedly
  mutate(word_speaker_index = word_idx + 1) |>
  rename(words = full_text) |>
  ungroup() |>
  left_join(read_csv(here(pred_loc, "expt_4_games_concat.csv")), by = c("words")) |>
  mutate(surprisal_llm_concat = logprob * log2(exp(1)) * -1) |>
  select(correct_tangram, gameId, orig_repNum = repNum, word_speaker_index, surprisal_llm_concat)

vlm_concat_probs <- read_csv(here(pred_loc, "vlm_probs_concat.csv")) |>
  group_by(full_text, word, word_idx) |>
  summarize(logprob = mean(logprob)) |> # deal with stochasticity in surprisal estimates for when the exact same phrase occurred repeatedly
  mutate(word_speaker_index = word_idx + 1) |>
  rename(words = full_text) |>
  ungroup() |>
  left_join(read_csv(here(pred_loc, "expt_4_games_concat.csv"))) |>
  mutate(surprisal_vlm_concat = logprob * log2(exp(1)) * -1) |>
  select(correct_tangram, gameId, orig_repNum = repNum, word_speaker_index, surprisal_vlm_concat)

# llm_as_seen_probs <- read_csv(here(pred_loc, "llm_probs_as_seen.csv")) |>
#   group_by(full_text, word_idx, word) |>
#   summarize(logprob = mean(logprob)) |>
#   mutate(overall_index = word_idx + 1, words = full_text) |>
#   left_join(read_csv(here(pred_loc, "expt_4_games_as_seen.csv"))) |>
#   mutate(surprisal_llm_as_seen = logprob * log2(exp(1)) * -1) |>
#   select(correct_tangram, gameId, orig_repNum = repNum, overall_index, surprisal_llm_as_seen, word)

word_freq <- read_csv(here(pred_loc, "word_freq.csv"))

test <- read_csv(here(pred_loc, "expt_4_games_concat.csv"))
```

A very crappy first pass at seeing if there's any signal 

```{r}
# 177,185

spr_labeled <- spr |>
  left_join(kl_pred, by = c("correct_tangram", "gameId", "orig_repNum", "word_speaker_index")) |>
  left_join(word_freq) |>
  left_join(llm_concat_probs, by = c("correct_tangram", "gameId", "orig_repNum", "word_speaker_index")) |>
  left_join(vlm_concat_probs) |>
  filter(!is.na(words)) |>
  mutate(word_len = ifelse(words %in% c("speaker**", "listener**"), NA, str_length(words))) |>
  ungroup()


spr_resid_1 <- spr_labeled |>
  mutate(surprisal_llm_concat_resid = lm(surprisal_llm_concat ~ freq + word_len, data = spr_labeled, na.action = na.exclude) |> residuals())

spr_resid_2 <- spr_resid_1 |>
  mutate(surprisal_vlm_concat_resid = lm(surprisal_vlm_concat ~ freq + word_len + surprisal_llm_concat_resid, data = spr_resid_1, na.action = na.exclude) |> residuals())

spr_resid_3 <- spr_resid_2 |>
  mutate(kl_resid = lm(kl ~ freq + word_len + surprisal_llm_concat_resid, data = spr_resid_2, na.action = na.exclude) |> residuals())


spr_resid_lag <- spr_resid_3 |>
  mutate(word_pos = word_speaker_index, 
         word_pos_log = log(word_speaker_index)) |> 
  mutate(across(c("freq", "word_len", "surprisal_llm_concat_resid", "surprisal_vlm_concat_resid", "kl_resid"), ~ scale(.x)[, 1])) |>
  mutate(across(c("freq", "word_len", "surprisal_llm_concat_resid", "surprisal_vlm_concat_resid", "kl_resid"), list(lag1 = ~ lag(.x), lag2 = ~ lag(.x, 2), lag3 = ~ lag(.x, 3)))) |>
  ungroup() |>
  filter(is_from_speaker == 1) |>
  select(-text, -type) |>
  filter(rt < 2000)

write_csv(spr_resid_lag, "spr_resid_lag.csv")
```

TODO check that we have values for everything we expect to
## LMER residualized -- linear word position

```{r}
library(lme4)


everything_lag3 <- lmer(
  rt ~ word_len * freq + kl_resid + surprisal_llm_concat_resid + surprisal_vlm_concat_resid +
    word_len_lag1 * freq_lag1 + kl_resid_lag1 + surprisal_llm_concat_resid_lag1 + surprisal_vlm_concat_resid_lag1 +
    word_len_lag2 * freq_lag2 + kl_resid_lag2 + surprisal_llm_concat_resid_lag2 + surprisal_vlm_concat_resid_lag2 +
    word_len_lag3 * freq_lag3 + kl_resid_lag3 + surprisal_llm_concat_resid_lag3 + surprisal_vlm_concat_resid_lag3 +
    orig_repNum * condition + matcher_trialNum + word_pos+
    (1 | workerid) + (1 | correct_tangram) + (1 | gameId),
  data = spr_resid_lag
)


everything_lag2 <- lmer(
  rt ~ word_len * freq + kl_resid + surprisal_llm_concat_resid + surprisal_vlm_concat_resid +
    word_len_lag1 * freq_lag1 + kl_resid_lag1 + surprisal_llm_concat_resid_lag1 + surprisal_vlm_concat_resid_lag1 +
    word_len_lag2 * freq_lag2 + kl_resid_lag2 + surprisal_llm_concat_resid_lag2 + surprisal_vlm_concat_resid_lag2 +
    orig_repNum * condition + matcher_trialNum +word_pos+
    (1 | workerid) + (1 | correct_tangram) + (1 | gameId),
  data = spr_resid_lag
)

everything_lag1 <- lmer(
  rt ~ word_len * freq + kl_resid + surprisal_llm_concat_resid + surprisal_vlm_concat_resid +
    word_len_lag1 * freq_lag1 + kl_resid_lag1 + surprisal_llm_concat_resid_lag1 + surprisal_vlm_concat_resid_lag1 +
    orig_repNum * condition + matcher_trialNum +word_pos+
    (1 | workerid) + (1 | correct_tangram) + (1 | gameId),
  data = spr_resid_lag
)

everything_lag0 <- lmer(
  rt ~ word_len * freq + kl_resid + surprisal_llm_concat_resid + surprisal_vlm_concat_resid +
    orig_repNum * condition + matcher_trialNum +word_pos+
    (1 | workerid) + (1 | correct_tangram) + (1 | gameId),
  data = spr_resid_lag
)
```

```{r}
lag3 <- summary(everything_lag3)$coef |>
  as_tibble(rownames = "term") |>
  mutate(model = "lag3")
lag2 <- summary(everything_lag2)$coef |>
  as_tibble(rownames = "term") |>
  mutate(model = "lag2")
lag1 <- summary(everything_lag1)$coef |>
  as_tibble(rownames = "term") |>
  mutate(model = "lag1")
lag0 <- summary(everything_lag0)$coef |>
  as_tibble(rownames = "term") |>
  mutate(model = "lag0")

all <- lag3 |>
  bind_rows(lag2, lag1, lag0) |>
  mutate(lower = Estimate - 1.96 * `Std. Error`, upper = Estimate + 1.96 * `Std. Error`) |>
  mutate(
    type = case_when(
      str_detect(term, "llm") ~ "LLM",
      str_detect(term, "vlm") ~ "VLM",
      str_detect(term, "kl") ~ "KL",
      str_detect(term, "orig") ~ term,
      str_detect(term, ":") ~ "freq x len",
      str_detect(term, "freq") ~ "freq",
      str_detect(term, "len") ~ "len",
      T ~ term
    ),
    lag = case_when(
      str_detect(term, "lag1") ~ -1,
      str_detect(term, "lag2") ~ -2,
      str_detect(term, "lag3") ~ -3,
      type %in% c("LLM", "VLM", "KL", "freq x len", "freq", "len") ~ 0,
      T ~ NA
    )
  ) |>
  mutate(
    type = factor(type, levels = c("len", "freq", "freq x len", "LLM", "VLM", "KL", "conditionyoked", "orig_repNum", "orig_repNum:conditionyoked", "matcher_trialNum", "word_pos")),
    type = fct_rev(type)
  )


ggplot(all |> filter(term != "(Intercept)") |> filter(!is.na(lag)), aes(x = type, y = Estimate, ymin = lower, ymax = upper, col = as.factor(model))) +
  geom_hline(yintercept = 0, lty = "dotted") +
  geom_pointrange(position = position_dodge(width = .6)) +
  coord_flip() +
  facet_grid(. ~ lag) +
  theme(legend.position = "bottom")


ggplot(all |> filter(term != "(Intercept)") |> filter(is.na(lag)), aes(x = type, y = Estimate, ymin = lower, ymax = upper, col = as.factor(model))) +
  geom_hline(yintercept = 0, lty = "dotted") +
  geom_pointrange(position = position_dodge(width = .6)) +
  coord_flip() +
  theme(legend.position = "bottom")
```


## LMER residualized -- log word position

```{r}
library(lme4)


everything_lag3 <- lmer(
  rt ~ word_len * freq + kl_resid + surprisal_llm_concat_resid + surprisal_vlm_concat_resid +
    word_len_lag1 * freq_lag1 + kl_resid_lag1 + surprisal_llm_concat_resid_lag1 + surprisal_vlm_concat_resid_lag1 +
    word_len_lag2 * freq_lag2 + kl_resid_lag2 + surprisal_llm_concat_resid_lag2 + surprisal_vlm_concat_resid_lag2 +
    word_len_lag3 * freq_lag3 + kl_resid_lag3 + surprisal_llm_concat_resid_lag3 + surprisal_vlm_concat_resid_lag3 +
    orig_repNum * condition + matcher_trialNum + word_pos_log+
    (1 | workerid) + (1 | correct_tangram) + (1 | gameId),
  data = spr_resid_lag
)


everything_lag2 <- lmer(
  rt ~ word_len * freq + kl_resid + surprisal_llm_concat_resid + surprisal_vlm_concat_resid +
    word_len_lag1 * freq_lag1 + kl_resid_lag1 + surprisal_llm_concat_resid_lag1 + surprisal_vlm_concat_resid_lag1 +
    word_len_lag2 * freq_lag2 + kl_resid_lag2 + surprisal_llm_concat_resid_lag2 + surprisal_vlm_concat_resid_lag2 +
    orig_repNum * condition + matcher_trialNum +word_pos_log+
    (1 | workerid) + (1 | correct_tangram) + (1 | gameId),
  data = spr_resid_lag
)

everything_lag1 <- lmer(
  rt ~ word_len * freq + kl_resid + surprisal_llm_concat_resid + surprisal_vlm_concat_resid +
    word_len_lag1 * freq_lag1 + kl_resid_lag1 + surprisal_llm_concat_resid_lag1 + surprisal_vlm_concat_resid_lag1 +
    orig_repNum * condition + matcher_trialNum +word_pos_log+
    (1 | workerid) + (1 | correct_tangram) + (1 | gameId),
  data = spr_resid_lag
)

everything_lag0 <- lmer(
  rt ~ word_len * freq + kl_resid + surprisal_llm_concat_resid + surprisal_vlm_concat_resid +
    orig_repNum * condition + matcher_trialNum +word_pos_log+
    (1 | workerid) + (1 | correct_tangram) + (1 | gameId),
  data = spr_resid_lag
)
```

```{r}
lag3 <- summary(everything_lag3)$coef |>
  as_tibble(rownames = "term") |>
  mutate(model = "lag3")
lag2 <- summary(everything_lag2)$coef |>
  as_tibble(rownames = "term") |>
  mutate(model = "lag2")
lag1 <- summary(everything_lag1)$coef |>
  as_tibble(rownames = "term") |>
  mutate(model = "lag1")
lag0 <- summary(everything_lag0)$coef |>
  as_tibble(rownames = "term") |>
  mutate(model = "lag0")

all <- lag3 |>
  bind_rows(lag2, lag1, lag0) |>
  mutate(lower = Estimate - 1.96 * `Std. Error`, upper = Estimate + 1.96 * `Std. Error`) |>
  mutate(
    type = case_when(
      str_detect(term, "llm") ~ "LLM",
      str_detect(term, "vlm") ~ "VLM",
      str_detect(term, "kl") ~ "KL",
      str_detect(term, "orig") ~ term,
      str_detect(term, ":") ~ "freq x len",
      str_detect(term, "freq") ~ "freq",
      str_detect(term, "len") ~ "len",
      T ~ term
    ),
    lag = case_when(
      str_detect(term, "lag1") ~ -1,
      str_detect(term, "lag2") ~ -2,
      str_detect(term, "lag3") ~ -3,
      type %in% c("LLM", "VLM", "KL", "freq x len", "freq", "len") ~ 0,
      T ~ NA
    )
  ) |>
  mutate(
    type = factor(type, levels = c("len", "freq", "freq x len", "LLM", "VLM", "KL", "conditionyoked", "orig_repNum", "orig_repNum:conditionyoked", "matcher_trialNum", "word_pos_log")),
    type = fct_rev(type)
  )


ggplot(all |> filter(term != "(Intercept)") |> filter(!is.na(lag)), aes(x = type, y = Estimate, ymin = lower, ymax = upper, col = as.factor(model))) +
  geom_hline(yintercept = 0, lty = "dotted") +
  geom_pointrange(position = position_dodge(width = .6)) +
  coord_flip() +
  facet_grid(. ~ lag) +
  theme(legend.position = "bottom")


ggplot(all |> filter(term != "(Intercept)") |> filter(is.na(lag)), aes(x = type, y = Estimate, ymin = lower, ymax = upper, col = as.factor(model))) +
  geom_hline(yintercept = 0, lty = "dotted") +
  geom_pointrange(position = position_dodge(width = .6)) +
  coord_flip() +
  theme(legend.position = "bottom")
```


## Len and freq only

```{r}
library(lme4)


lf_lag3 <- lmer(
  rt ~ word_len * freq +
    word_len_lag1 * freq_lag1 + 
    word_len_lag2 * freq_lag2 +
    word_len_lag3 * freq_lag3 + 
    orig_repNum * condition + matcher_trialNum +
    (1 | workerid) + (1 | correct_tangram) + (1 | gameId),
  data = spr_resid_lag
)


lf_lag2 <- lmer(
  rt ~ word_len * freq + 
    word_len_lag1 * freq_lag1 +
    word_len_lag2 * freq_lag2 + 
    orig_repNum * condition + matcher_trialNum +
    (1 | workerid) + (1 | correct_tangram) + (1 | gameId),
  data = spr_resid_lag
)

lf_lag1 <- lmer(
  rt ~ word_len * freq + 
    word_len_lag1 * freq_lag1 +
    orig_repNum * condition + matcher_trialNum +
    (1 | workerid) + (1 | correct_tangram) + (1 | gameId),
  data = spr_resid_lag
)

lf_lag0 <- lmer(
  rt ~ word_len * freq + 
    orig_repNum * condition + matcher_trialNum +
    (1 | workerid) + (1 | correct_tangram) + (1 | gameId),
  data = spr_resid_lag
)
```

```{r}
lag3 <- summary(lf_lag3)$coef |>
  as_tibble(rownames = "term") |>
  mutate(model = "lag3")
lag2 <- summary(lf_lag2)$coef |>
  as_tibble(rownames = "term") |>
  mutate(model = "lag2")
lag1 <- summary(lf_lag1)$coef |>
  as_tibble(rownames = "term") |>
  mutate(model = "lag1")
lag0 <- summary(lf_lag0)$coef |>
  as_tibble(rownames = "term") |>
  mutate(model = "lag0")

all <- lag3 |>
  bind_rows(lag2, lag1, lag0) |>
  mutate(lower = Estimate - 1.96 * `Std. Error`, upper = Estimate + 1.96 * `Std. Error`) |>
  mutate(
    type = case_when(
      str_detect(term, "llm") ~ "LLM",
      str_detect(term, "vlm") ~ "VLM",
      str_detect(term, "kl") ~ "KL",
      str_detect(term, "orig") ~ term,
      str_detect(term, ":") ~ "freq x len",
      str_detect(term, "freq") ~ "freq",
      str_detect(term, "len") ~ "len",
      T ~ term
    ),
    lag = case_when(
      str_detect(term, "lag1") ~ -1,
      str_detect(term, "lag2") ~ -2,
      str_detect(term, "lag3") ~ -3,
      type %in% c("LLM", "VLM", "KL", "freq x len", "freq", "len") ~ 0,
      T ~ NA
    )
  ) |>
  mutate(
    type = factor(type, levels = c("len", "freq", "freq x len", "LLM", "VLM", "KL", "conditionyoked", "orig_repNum", "orig_repNum:conditionyoked", "matcher_trialNum")),
    type = fct_rev(type)
  )


ggplot(all |> filter(term != "(Intercept)") |> filter(!is.na(lag)), aes(x = type, y = Estimate, ymin = lower, ymax = upper, col = as.factor(model))) +
  geom_hline(yintercept = 0, lty = "dotted") +
  geom_pointrange(position = position_dodge(width = .6)) +
  coord_flip() +
  facet_grid(. ~ lag) +
  theme(legend.position = "bottom")


ggplot(all |> filter(term != "(Intercept)") |> filter(is.na(lag)), aes(x = type, y = Estimate, ymin = lower, ymax = upper, col = as.factor(model))) +
  geom_hline(yintercept = 0, lty = "dotted") +
  geom_pointrange(position = position_dodge(width = .6)) +
  coord_flip() +
  theme(legend.position = "bottom")
```


## No NA version

```{r}
library(lme4)

spr_resid_lag_no_na <- spr_resid_lag |> drop_na()

everything_lag3_no_na <- lmer(
  rt ~ word_len * freq + kl_resid + surprisal_llm_concat_resid + surprisal_vlm_concat_resid +
    word_len_lag1 * freq_lag1 + kl_resid_lag1 + surprisal_llm_concat_resid_lag1 + surprisal_vlm_concat_resid_lag1 +
    word_len_lag2 * freq_lag2 + kl_resid_lag2 + surprisal_llm_concat_resid_lag2 + surprisal_vlm_concat_resid_lag2 +
    word_len_lag3 * freq_lag3 + kl_resid_lag3 + surprisal_llm_concat_resid_lag3 + surprisal_vlm_concat_resid_lag3 +
    orig_repNum * condition + matcher_trialNum +
    (1 | workerid) + (1 | correct_tangram) + (1 | gameId),
  data = spr_resid_lag_no_na
)


everything_lag2_no_na <- lmer(
  rt ~ word_len * freq + kl_resid + surprisal_llm_concat_resid + surprisal_vlm_concat_resid +
    word_len_lag1 * freq_lag1 + kl_resid_lag1 + surprisal_llm_concat_resid_lag1 + surprisal_vlm_concat_resid_lag1 +
    word_len_lag2 * freq_lag2 + kl_resid_lag2 + surprisal_llm_concat_resid_lag2 + surprisal_vlm_concat_resid_lag2 +
    orig_repNum * condition + matcher_trialNum +
    (1 | workerid) + (1 | correct_tangram) + (1 | gameId),
  data = spr_resid_lag_no_na
)

everything_lag1_no_na <- lmer(
  rt ~ word_len * freq + kl_resid + surprisal_llm_concat_resid + surprisal_vlm_concat_resid +
    word_len_lag1 * freq_lag1 + kl_resid_lag1 + surprisal_llm_concat_resid_lag1 + surprisal_vlm_concat_resid_lag1 +
    orig_repNum * condition + matcher_trialNum +
    (1 | workerid) + (1 | correct_tangram) + (1 | gameId),
  data = spr_resid_lag_no_na
)

everything_lag0_no_na <- lmer(
  rt ~ word_len * freq + kl_resid + surprisal_llm_concat_resid + surprisal_vlm_concat_resid +
    orig_repNum * condition + matcher_trialNum +
    (1 | workerid) + (1 | correct_tangram) + (1 | gameId),
  data = spr_resid_lag_no_na
)
```

```{r}
lag3_no_na <- summary(everything_lag3_no_na)$coef |>
  as_tibble(rownames = "term") |>
  mutate(model = "lag3")
lag2_no_na <- summary(everything_lag2_no_na)$coef |>
  as_tibble(rownames = "term") |>
  mutate(model = "lag2")
lag1_no_na <- summary(everything_lag1_no_na)$coef |>
  as_tibble(rownames = "term") |>
  mutate(model = "lag1")
lag0_no_na <- summary(everything_lag0_no_na)$coef |>
  as_tibble(rownames = "term") |>
  mutate(model = "lag0")

all_no_na <- lag3_no_na |>
  bind_rows(lag2_no_na, lag1_no_na, lag0_no_na) |>
  mutate(lower = Estimate - 1.96 * `Std. Error`, upper = Estimate + 1.96 * `Std. Error`) |>
  mutate(
    type = case_when(
      str_detect(term, "llm") ~ "LLM",
      str_detect(term, "vlm") ~ "VLM",
      str_detect(term, "kl") ~ "KL",
      str_detect(term, "orig") ~ term,
      str_detect(term, ":") ~ "freq x len",
      str_detect(term, "freq") ~ "freq",
      str_detect(term, "len") ~ "len",
      T ~ term
    ),
    lag = case_when(
      str_detect(term, "lag1") ~ -1,
      str_detect(term, "lag2") ~ -2,
      str_detect(term, "lag3") ~ -3,
      type %in% c("LLM", "VLM", "KL", "freq x len", "freq", "len") ~ 0,
      T ~ NA
    )
  ) |>
  mutate(
    type = factor(type, levels = c("len", "freq", "freq x len", "LLM", "VLM", "KL", "conditionyoked", "orig_repNum", "orig_repNum:conditionyoked", "matcher_trialNum")),
    type = fct_rev(type)
  )


ggplot(all_no_na |> filter(term != "(Intercept)") |> filter(!is.na(lag)), aes(x = type, y = Estimate, ymin = lower, ymax = upper, col = as.factor(model))) +
  geom_hline(yintercept = 0, lty = "dotted") +
  geom_pointrange(position = position_dodge(width = .6)) +
  coord_flip() +
  facet_grid(. ~ lag) +
  theme(legend.position = "bottom")


ggplot(all_no_na |> filter(term != "(Intercept)") |> filter(is.na(lag)), aes(x = type, y = Estimate, ymin = lower, ymax = upper, col = as.factor(model))) +
  geom_hline(yintercept = 0, lty = "dotted") +
  geom_pointrange(position = position_dodge(width = .6)) +
  coord_flip() +
  theme(legend.position = "bottom")
```
## BRMS residualized

```{r}
spr_labeled <- read_csv("spr_resid_lag.csv")

spr_priors <- c(
  set_prior("normal(400,100)", class = "Intercept"),
  set_prior("normal(0, 5)", class = "b"),
  set_prior("normal(0, 5)", class = "sd")
)

foo <- brm(
  rt ~ word_len * freq + kl_resid + surprisal_llm_concat_resid + surprisal_vlm_concat_resid +
    word_len_lag1 * freq_lag1 + kl_resid_lag1 + surprisal_llm_concat_resid_lag1 + surprisal_vlm_concat_resid_lag1 +
    word_len_lag2 * freq_lag2 + kl_resid_lag2 + surprisal_llm_concat_resid_lag2 + surprisal_vlm_concat_resid_lag2 +
    word_len_lag3 * freq_lag3 + kl_resid_lag3 + surprisal_llm_concat_resid_lag3 + surprisal_vlm_concat_resid_lag3 +
    orig_repNum * condition + matcher_trialNum +
    (1 | workerid) + (1 | correct_tangram) + (1 | gameId),
  data = spr_labeled, prior = spr_priors,
  control = list(adapt_delta = 0.99),
  file = "everything_lag3_normalized.rds"
)

summary(foo)
```

```{r}
nolag <- brm(
  rt ~ word_len * freq + kl_resid + surprisal_llm_concat_resid + surprisal_vlm_concat_resid +
    orig_repNum * condition + matcher_trialNum +
    (1 | workerid) + (1 | correct_tangram) + (1 | gameId),
  data = spr_labeled, prior = spr_priors,
  control = list(adapt_delta = 0.99),
  file = "everything_nolag_normalized.rds"
)
```

```{r}
foo <- read_rds(here("analysis-code/models/everything_lag3_normalized.rds"))
blah <- fixef(foo) |>
  as_tibble(rownames = "term") |>
  mutate(
    type = case_when(
      str_detect(term, "llm") ~ "LLM surp.",
      str_detect(term, "vlm") ~ "VLM surp.",
      str_detect(term, "kl") ~ "KL",
      str_detect(term, "orig") ~ term,
      str_detect(term, ":") ~ "Freq. x Len.",
      str_detect(term, "freq") ~ "Freq.",
      str_detect(term, "len") ~ "Length",
      T ~ term
    ),
    lag = case_when(
      str_detect(term, "lag1") ~ "word n-1",
      str_detect(term, "lag2") ~ "word n-2",
      str_detect(term, "lag3") ~ "word n-3",
      type %in% c("LLM surp.", "VLM surp.", "KL", "Freq. x Len.", "Freq.", "Length") ~ "word n",
      T ~ NA
    )
  ) |>
  mutate(
    type = factor(type, levels = c("Length", "Freq.", "Freq. x Len.", "LLM surp.", "VLM surp.", "KL", "conditionyoked", "orig_repNum", "orig_repNum:conditionyoked", "matcher_trialNum")),
    type = fct_rev(type)
  )

write_rds(blah, here("analysis-code/models/everything_lag3_normalized_summary.rds"))


ggplot(blah |> filter(term != "Intercept"), aes(x = type, y = Estimate, ymin = `Q2.5`, ymax = `Q97.5`, col = as.factor(lag))) +
  geom_hline(yintercept = 0, lty = "dotted") +
  geom_pointrange(position = position_dodge(width = .6)) +
  coord_flip()


blah |>
  filter(!is.na(lag)) |>
  ggplot(aes(x = type, y = Estimate, ymin = `Q2.5`, ymax = `Q97.5`, col = as.factor(lag))) +
  geom_hline(yintercept = 0, lty = "dotted") +
  geom_pointrange(position = position_dodge2(width = .6, reverse = T)) +
  coord_flip() +
  scale_color_manual(values = c("#5DC863", "#21908C", "#3B538B", "#440154")) +
  labs(y = "Standardized coefficient estimate", color = "Predictor lag") +
  theme(
    axis.title.y = element_blank(),
    legend.position = "inside",
    legend.position.inside = c(.82, .35),
    legend.background = element_blank(),
    legend.box.background = element_rect(colour = "black"),
    axis.text = element_text(size = 14),
    axis.title = element_text(size = 14),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 14)
  )

ggsave(here("writing/word_preds.pdf"), width = 5, height = 3.5)


blah |>
  filter(is.na(lag)) |>
  filter(term != "Intercept") |>
  mutate(type = factor(type, levels = c("conditionyoked", "orig_repNum", "orig_repNum:conditionyoked", "matcher_trialNum"), labels = c("Condition (yoked)", "Original Block", "Block x condition", "Viewing order")) |> fct_rev()) |>
  ggplot(aes(x = type, y = Estimate, ymin = `Q2.5`, ymax = `Q97.5`)) +
  geom_hline(yintercept = 0, lty = "dotted") +
  geom_pointrange(position = position_dodge2(width = .6, reverse = T)) +
  coord_flip() +
  labs(y = "Estimated coefficient", color = "Predictor lag") +
  theme(
    axis.title.y = element_blank(),
    legend.position = "inside",
    legend.position.inside = c(.8, .5),
    legend.background = element_blank(),
    legend.box.background = element_rect(colour = "black"),
    axis.text = element_text(size = 14),
    axis.title = element_text(size = 14),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 14)
  )

ggsave(here("writing/other_preds.pdf"), width = 5, height = 1.5)
```

# What we said in the pre-reg

Condition differences: condition refers to yoked or shuffled.

Logistic model of target selection accuracy: Accuracy ~ original_rep_num * condition + viewing_order + (1 | gameId) + (1 | tangram) + (1 | participant)

Time to selection:  Selection_time ~ original_rep_num * condition + viewing_order + (1 | gameId) + (1 | tangram) + (1 | participant)

Per-word reading time analyses: We will compare different predictors of RTs. Because SPR notoriously has lagged effects, we will consider models that incorporate the predictors of either just the target word, or the target word as well as up to three prior words. We are unsure of exactly what predictors will work, so these should be considered exploratory analyses. 

We are unsure of how well mixed effects will fit, but we will aim for this structure. 

Base_model: RT ~ (word_len + word_freq) + original_rep_num * condition + viewing_order + (1| gameId) +  (1| participant) + (1|tangram)

KL divergence of information: From our CLIP model, we will look at the KL divergence in the model’s predicted distribution of adding that word based on the distributions for the partial description up to that word and the partial description including that word.  RT ~ (word_len + word_freq + KL_div) + original_rep_num * condition + viewing_order + (1| gameId) +  (1| participant) + (1|tangram)

LM surprisal: We will use a non-vision language model (a llama model) to calculate per-word surprisal. RT ~ (word_len + word_freq + surprisal) + original_rep_num * condition + viewing_order + (1| gameId) +  (1| participant) + (1|tangram)

VLM surprisal: We will use a vision language model (llama 3.2) to calculate per-word surprisal conditioned on either the target image or the grid of images with the target highlighted. RT ~ (word_len + word_freq + vision_surprisal) + original_rep_num * condition + viewing_order + (1| gameId) +  (1| participant) + (1|tangram)

We will also do model comparisons using models with multiple of these predictors into order to look at which sources of information (KL divergence, surprisal, vlm surprisal) lead to better fitting models and whether adding multiple of these sources provides additional fit. 

We will also look at which items and  words are best or worst fit by these models. 

Outliers and Exclusions
Describe exactly how outliers will be defined and handled, and your precise rule(s) for excluding observations.
We will exclude individual word RTs that are greater than 2000 ms. 
